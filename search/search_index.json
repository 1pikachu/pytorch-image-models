{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started Welcome Welcome to the timm documentation, a lean set of docs that covers the basics of timm . For a more comprehensive set of docs (currently under development), please visit timmdocs by Aman Arora . Install The library can be installed with pip: pip install timm I update the PyPi (pip) packages when I'm confident there are no significant model regressions from previous releases. If you want to pip install the bleeding edge from GitHub, use: pip install git+https://github.com/rwightman/pytorch-image-models.git Conda Environment All development and testing has been done in Conda Python 3 environments on Linux x86-64 systems, specifically 3.7, 3.8, 3.9, 3.10 Little to no care has been taken to be Python 2.x friendly and will not support it. If you run into any challenges running on Windows, or other OS, I'm definitely open to looking into those issues so long as it's in a reproducible (read Conda) environment. PyTorch versions 1.9, 1.10, 1.11 have been tested with the latest versions of this code. I've tried to keep the dependencies minimal, the setup is as per the PyTorch default install instructions for Conda: conda create -n torch-env conda activate torch-env conda install pytorch torchvision cudatoolkit=11.3 -c pytorch conda install pyyaml Load a Pretrained Model Pretrained models can be loaded using timm.create_model import timm m = timm . create_model ( 'mobilenetv3_large_100' , pretrained = True ) m . eval () List Models with Pretrained Weights import timm from pprint import pprint model_names = timm . list_models ( pretrained = True ) pprint ( model_names ) >>> [ 'adv_inception_v3' , 'cspdarknet53' , 'cspresnext50' , 'densenet121' , 'densenet161' , 'densenet169' , 'densenet201' , 'densenetblur121d' , 'dla34' , 'dla46_c' , ... ] List Model Architectures by Wildcard import timm from pprint import pprint model_names = timm . list_models ( '*resne*t*' ) pprint ( model_names ) >>> [ 'cspresnet50' , 'cspresnet50d' , 'cspresnet50w' , 'cspresnext50' , ... ]","title":"Getting Started"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#welcome","text":"Welcome to the timm documentation, a lean set of docs that covers the basics of timm . For a more comprehensive set of docs (currently under development), please visit timmdocs by Aman Arora .","title":"Welcome"},{"location":"#install","text":"The library can be installed with pip: pip install timm I update the PyPi (pip) packages when I'm confident there are no significant model regressions from previous releases. If you want to pip install the bleeding edge from GitHub, use: pip install git+https://github.com/rwightman/pytorch-image-models.git Conda Environment All development and testing has been done in Conda Python 3 environments on Linux x86-64 systems, specifically 3.7, 3.8, 3.9, 3.10 Little to no care has been taken to be Python 2.x friendly and will not support it. If you run into any challenges running on Windows, or other OS, I'm definitely open to looking into those issues so long as it's in a reproducible (read Conda) environment. PyTorch versions 1.9, 1.10, 1.11 have been tested with the latest versions of this code. I've tried to keep the dependencies minimal, the setup is as per the PyTorch default install instructions for Conda: conda create -n torch-env conda activate torch-env conda install pytorch torchvision cudatoolkit=11.3 -c pytorch conda install pyyaml","title":"Install"},{"location":"#load-a-pretrained-model","text":"Pretrained models can be loaded using timm.create_model import timm m = timm . create_model ( 'mobilenetv3_large_100' , pretrained = True ) m . eval ()","title":"Load a Pretrained Model"},{"location":"#list-models-with-pretrained-weights","text":"import timm from pprint import pprint model_names = timm . list_models ( pretrained = True ) pprint ( model_names ) >>> [ 'adv_inception_v3' , 'cspdarknet53' , 'cspresnext50' , 'densenet121' , 'densenet161' , 'densenet169' , 'densenet201' , 'densenetblur121d' , 'dla34' , 'dla46_c' , ... ]","title":"List Models with Pretrained Weights"},{"location":"#list-model-architectures-by-wildcard","text":"import timm from pprint import pprint model_names = timm . list_models ( '*resne*t*' ) pprint ( model_names ) >>> [ 'cspresnet50' , 'cspresnet50d' , 'cspresnet50w' , 'cspresnext50' , ... ]","title":"List Model Architectures by Wildcard"},{"location":"archived_changes/","text":"Archived Changes June 8, 2021 Add first ResMLP weights, trained in PyTorch XLA on TPU-VM w/ my XLA branch. 24 block variant, 79.2 top-1. Add ResNet51-Q model w/ pretrained weights at 82.36 top-1. NFNet inspired block layout with quad layer stem and no maxpool Same param count (35.7M) and throughput as ResNetRS-50 but +1.5 top-1 @ 224x224 and +2.5 top-1 at 288x288 May 25, 2021 Add LeViT, Visformer, Convit (PR by Aman Arora), Twins (PR by paper authors) transformer models Cleanup input_size/img_size override handling and testing for all vision transformer models Add efficientnetv2_rw_m model and weights (started training before official code). 84.8 top-1, 53M params. May 14, 2021 Add EfficientNet-V2 official model defs w/ ported weights from official Tensorflow/Keras impl. 1k trained variants: tf_efficientnetv2_s/m/l 21k trained variants: tf_efficientnetv2_s/m/l_in21k 21k pretrained -> 1k fine-tuned: tf_efficientnetv2_s/m/l_in21ft1k v2 models w/ v1 scaling: tf_efficientnetv2_b0 through b3 Rename my prev V2 guess efficientnet_v2s -> efficientnetv2_rw_s Some blank efficientnetv2_* models in-place for future native PyTorch training May 5, 2021 Add MLP-Mixer models and port pretrained weights from Google JAX impl Add CaiT models and pretrained weights from FB Add ResNet-RS models and weights from TF . Thanks Aman Arora Add CoaT models and weights. Thanks Mohammed Rizin Add new ImageNet-21k weights & finetuned weights for TResNet, MobileNet-V3, ViT models. Thanks mrT Add GhostNet models and weights. Thanks Kai Han Update ByoaNet attention modles Improve SA module inits Hack together experimental stand-alone Swin based attn module and swinnet Consistent '26t' model defs for experiments. Add improved Efficientnet-V2S (prelim model def) weights. 83.8 top-1. WandB logging support April 13, 2021 Add Swin Transformer models and weights from https://github.com/microsoft/Swin-Transformer April 12, 2021 Add ECA-NFNet-L1 (slimmed down F1 w/ SiLU, 41M params) trained with this code. 84% top-1 @ 320x320. Trained at 256x256. Add EfficientNet-V2S model (unverified model definition) weights. 83.3 top-1 @ 288x288. Only trained single res 224. Working on progressive training. Add ByoaNet model definition (Bring-your-own-attention) w/ SelfAttention block and corresponding SA/SA-like modules and model defs Lambda Networks - https://arxiv.org/abs/2102.08602 Bottleneck Transformers - https://arxiv.org/abs/2101.11605 Halo Nets - https://arxiv.org/abs/2103.12731 Adabelief optimizer contributed by Juntang Zhuang April 1, 2021 Add snazzy benchmark.py script for bulk timm model benchmarking of train and/or inference Add Pooling-based Vision Transformer (PiT) models (from https://github.com/naver-ai/pit ) Merged distilled variant into main for torchscript compatibility Some timm cleanup/style tweaks and weights have hub download support Cleanup Vision Transformer (ViT) models Merge distilled (DeiT) model into main so that torchscript can work Support updated weight init (defaults to old still) that closer matches original JAX impl (possibly better training from scratch) Separate hybrid model defs into different file and add several new model defs to fiddle with, support patch_size != 1 for hybrids Fix fine-tuning num_class changes (PiT and ViT) and pos_embed resizing (Vit) with distilled variants nn.Sequential for block stack (does not break downstream compat) TnT (Transformer-in-Transformer) models contributed by author (from https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/TNT ) Add RegNetY-160 weights from DeiT teacher model Add new NFNet-L0 w/ SE attn (rename nfnet_l0b -> nfnet_l0 ) weights 82.75 top-1 @ 288x288 Some fixes/improvements for TFDS dataset wrapper March 7, 2021 First 0.4.x PyPi release w/ NFNets (& related), ByoB (GPU-Efficient, RepVGG, etc). Change feature extraction for pre-activation nets (NFNets, ResNetV2) to return features before activation. Feb 18, 2021 Add pretrained weights and model variants for NFNet-F* models from DeepMind Haiku impl . Models are prefixed with dm_ . They require SAME padding conv, skipinit enabled, and activation gains applied in act fn. These models are big, expect to run out of GPU memory. With the GELU activiation + other options, they are roughly \u00bd the inference speed of my SiLU PyTorch optimized s variants. Original model results are based on pre-processing that is not the same as all other models so you'll see different results in the results csv (once updated). Matching the original pre-processing as closely as possible I get these results: dm_nfnet_f6 - 86.352 dm_nfnet_f5 - 86.100 dm_nfnet_f4 - 85.834 dm_nfnet_f3 - 85.676 dm_nfnet_f2 - 85.178 dm_nfnet_f1 - 84.696 dm_nfnet_f0 - 83.464 Feb 16, 2021 Add Adaptive Gradient Clipping (AGC) as per https://arxiv.org/abs/2102.06171 . Integrated w/ PyTorch gradient clipping via mode arg that defaults to prev 'norm' mode. For backward arg compat, clip-grad arg must be specified to enable when using train.py. AGC w/ default clipping factor --clip-grad .01 --clip-mode agc PyTorch global norm of 1.0 (old behaviour, always norm), --clip-grad 1.0 PyTorch value clipping of 10, --clip-grad 10. --clip-mode value AGC performance is definitely sensitive to the clipping factor. More experimentation needed to determine good values for smaller batch sizes and optimizers besides those in paper. So far I've found .001-.005 is necessary for stable RMSProp training w/ NFNet/NF-ResNet. Feb 12, 2021 Update Normalization-Free nets to include new NFNet-F ( https://arxiv.org/abs/2102.06171 ) model defs Feb 10, 2021 More model archs, incl a flexible ByobNet backbone ('Bring-your-own-blocks') GPU-Efficient-Networks ( https://github.com/idstcv/GPU-Efficient-Networks ), impl in byobnet.py RepVGG ( https://github.com/DingXiaoH/RepVGG ), impl in byobnet.py classic VGG (from torchvision, impl in vgg ) Refinements to normalizer layer arg handling and normalizer+act layer handling in some models Default AMP mode changed to native PyTorch AMP instead of APEX. Issues not being fixed with APEX. Native works with --channels-last and --torchscript model training, APEX does not. Fix a few bugs introduced since last pypi release Feb 8, 2021 Add several ResNet weights with ECA attention. 26t & 50t trained @ 256, test @ 320. 269d train @ 256, fine-tune @320, test @ 352. ecaresnet26t - 79.88 top-1 @ 320x320, 79.08 @ 256x256 ecaresnet50t - 82.35 top-1 @ 320x320, 81.52 @ 256x256 ecaresnet269d - 84.93 top-1 @ 352x352, 84.87 @ 320x320 Remove separate tiered ( t ) vs tiered_narrow ( tn ) ResNet model defs, all tn changed to t and t models removed ( seresnext26t_32x4d only model w/ weights that was removed). Support model default_cfgs with separate train vs test resolution test_input_size and remove extra _320 suffix ResNet model defs that were just for test. Jan 30, 2021 Add initial \"Normalization Free\" NF-RegNet-B* and NF-ResNet model definitions based on paper Jan 25, 2021 Add ResNetV2 Big Transfer (BiT) models w/ ImageNet-1k and 21k weights from https://github.com/google-research/big_transfer Add official R50+ViT-B/16 hybrid models + weights from https://github.com/google-research/vision_transformer ImageNet-21k ViT weights are added w/ model defs and representation layer (pre logits) support NOTE: ImageNet-21k classifier heads were zero'd in original weights, they are only useful for transfer learning Add model defs and weights for DeiT Vision Transformer models from https://github.com/facebookresearch/deit Refactor dataset classes into ImageDataset/IterableImageDataset + dataset specific parser classes Add Tensorflow-Datasets (TFDS) wrapper to allow use of TFDS image classification sets with train script Ex: train.py /data/tfds --dataset tfds/oxford_iiit_pet --val-split test --model resnet50 -b 256 --amp --num-classes 37 --opt adamw --lr 3e-4 --weight-decay .001 --pretrained -j 2 Add improved .tar dataset parser that reads images from .tar, folder of .tar files, or .tar within .tar Run validation on full ImageNet-21k directly from tar w/ BiT model: validate.py /data/fall11_whole.tar --model resnetv2_50x1_bitm_in21k --amp Models in this update should be stable w/ possible exception of ViT/BiT, possibility of some regressions with train/val scripts and dataset handling Jan 3, 2021 Add SE-ResNet-152D weights 256x256 val, 0.94 crop top-1 - 83.75 320x320 val, 1.0 crop - 84.36 Update results files Dec 18, 2020 Add ResNet-101D, ResNet-152D, and ResNet-200D weights trained @ 256x256 256x256 val, 0.94 crop (top-1) - 101D (82.33), 152D (83.08), 200D (83.25) 288x288 val, 1.0 crop - 101D (82.64), 152D (83.48), 200D (83.76) 320x320 val, 1.0 crop - 101D (83.00), 152D (83.66), 200D (84.01) Dec 7, 2020 Simplify EMA module (ModelEmaV2), compatible with fully torchscripted models Misc fixes for SiLU ONNX export, default_cfg missing from Feature extraction models, Linear layer w/ AMP + torchscript PyPi release @ 0.3.2 (needed by EfficientDet) Oct 30, 2020 Test with PyTorch 1.7 and fix a small top-n metric view vs reshape issue. Convert newly added 224x224 Vision Transformer weights from official JAX repo. 81.8 top-1 for B/16, 83.1 L/16. Support PyTorch 1.7 optimized, native SiLU (aka Swish) activation. Add mapping to 'silu' name, custom swish will eventually be deprecated. Fix regression for loading pretrained classifier via direct model entrypoint functions. Didn't impact create_model() factory usage. PyPi release @ 0.3.0 version! Oct 26, 2020 Update Vision Transformer models to be compatible with official code release at https://github.com/google-research/vision_transformer Add Vision Transformer weights (ImageNet-21k pretrain) for 384x384 base and large models converted from official jax impl ViT-B/16 - 84.2 ViT-B/32 - 81.7 ViT-L/16 - 85.2 ViT-L/32 - 81.5 Oct 21, 2020 Weights added for Vision Transformer (ViT) models. 77.86 top-1 for 'small' and 79.35 for 'base'. Thanks to Christof for training the base model w/ lots of GPUs. Oct 13, 2020 Initial impl of Vision Transformer models. Both patch and hybrid (CNN backbone) variants. Currently trying to train... Adafactor and AdaHessian (FP32 only, no AMP) optimizers EdgeTPU-M ( efficientnet_em ) model trained in PyTorch, 79.3 top-1 Pip release, doc updates pending a few more changes... Sept 18, 2020 New ResNet 'D' weights. 72.7 (top-1) ResNet-18-D, 77.1 ResNet-34-D, 80.5 ResNet-50-D Added a few untrained defs for other ResNet models (66D, 101D, 152D, 200/200D) Sept 3, 2020 New weights Wide-ResNet50 - 81.5 top-1 (vs 78.5 torchvision) SEResNeXt50-32x4d - 81.3 top-1 (vs 79.1 cadene) Support for native Torch AMP and channels_last memory format added to train/validate scripts ( --channels-last , --native-amp vs --apex-amp ) Models tested with channels_last on latest NGC 20.08 container. AdaptiveAvgPool in attn layers changed to mean((2,3)) to work around bug with NHWC kernel. Aug 12, 2020 New/updated weights from training experiments EfficientNet-B3 - 82.1 top-1 (vs 81.6 for official with AA and 81.9 for AdvProp) RegNetY-3.2GF - 82.0 top-1 (78.9 from official ver) CSPResNet50 - 79.6 top-1 (76.6 from official ver) Add CutMix integrated w/ Mixup. See pull request for some usage examples Some fixes for using pretrained weights with in_chans != 3 on several models. Aug 5, 2020 Universal feature extraction, new models, new weights, new test sets. * All models support the features_only=True argument for create_model call to return a network that extracts feature maps from the deepest layer at each stride. * New models * CSPResNet, CSPResNeXt, CSPDarkNet, DarkNet * ReXNet * (Modified Aligned) Xception41/65/71 (a proper port of TF models) * New trained weights * SEResNet50 - 80.3 top-1 * CSPDarkNet53 - 80.1 top-1 * CSPResNeXt50 - 80.0 top-1 * DPN68b - 79.2 top-1 * EfficientNet-Lite0 (non-TF ver) - 75.5 (submitted by @hal-314 ) * Add 'real' labels for ImageNet and ImageNet-Renditions test set, see results/README.md * Test set ranking/top-n diff script by @KushajveerSingh * Train script and loader/transform tweaks to punch through more aug arguments * README and documentation overhaul. See initial (WIP) documentation at https://rwightman.github.io/pytorch-image-models/ * adamp and sgdp optimizers added by @hellbell June 11, 2020 Bunch of changes: * DenseNet models updated with memory efficient addition from torchvision (fixed a bug), blur pooling and deep stem additions * VoVNet V1 and V2 models added, 39 V2 variant (ese_vovnet_39b) trained to 79.3 top-1 * Activation factory added along with new activations: * select act at model creation time for more flexibility in using activations compatible with scripting or tracing (ONNX export) * hard_mish (experimental) added with memory-efficient grad, along with ME hard_swish * context mgr for setting exportable/scriptable/no_jit states * Norm + Activation combo layers added with initial trial support in DenseNet and VoVNet along with impl of EvoNorm and InplaceAbn wrapper that fit the interface * Torchscript works for all but two of the model types as long as using Pytorch 1.5+, tests added for this * Some import cleanup and classifier reset changes, all models will have classifier reset to nn.Identity on reset_classifer(0) call * Prep for 0.1.28 pip release May 12, 2020 Add ResNeSt models (code adapted from https://github.com/zhanghang1989/ResNeSt , paper https://arxiv.org/abs/2004.08955 )) May 3, 2020 Pruned EfficientNet B1, B2, and B3 ( https://arxiv.org/abs/2002.08258 ) contributed by Yonathan Aflalo May 1, 2020 Merged a number of execellent contributions in the ResNet model family over the past month BlurPool2D and resnetblur models initiated by Chris Ha , I trained resnetblur50 to 79.3. TResNet models and SpaceToDepth, AntiAliasDownsampleLayer layers by mrT23 ecaresnet (50d, 101d, light) models and two pruned variants using pruning as per ( https://arxiv.org/abs/2002.08258 ) by Yonathan Aflalo 200 pretrained models in total now with updated results csv in results folder April 5, 2020 Add some newly trained MobileNet-V2 models trained with latest h-params, rand augment. They compare quite favourably to EfficientNet-Lite 3.5M param MobileNet-V2 100 @ 73% 4.5M param MobileNet-V2 110d @ 75% 6.1M param MobileNet-V2 140 @ 76.5% 5.8M param MobileNet-V2 120d @ 77.3% March 18, 2020 Add EfficientNet-Lite models w/ weights ported from Tensorflow TPU Add RandAugment trained ResNeXt-50 32x4d weights with 79.8 top-1. Trained by Andrew Lavin (see Training section for hparams) April 5, 2020 Add some newly trained MobileNet-V2 models trained with latest h-params, rand augment. They compare quite favourably to EfficientNet-Lite 3.5M param MobileNet-V2 100 @ 73% 4.5M param MobileNet-V2 110d @ 75% 6.1M param MobileNet-V2 140 @ 76.5% 5.8M param MobileNet-V2 120d @ 77.3% March 18, 2020 Add EfficientNet-Lite models w/ weights ported from Tensorflow TPU Add RandAugment trained ResNeXt-50 32x4d weights with 79.8 top-1. Trained by Andrew Lavin (see Training section for hparams) Feb 29, 2020 New MobileNet-V3 Large weights trained from stratch with this code to 75.77% top-1 IMPORTANT CHANGE - default weight init changed for all MobilenetV3 / EfficientNet / related models overall results similar to a bit better training from scratch on a few smaller models tried performance early in training seems consistently improved but less difference by end set fix_group_fanout=False in _init_weight_goog fn if you need to reproducte past behaviour Experimental LR noise feature added applies a random perturbation to LR each epoch in specified range of training Feb 18, 2020 Big refactor of model layers and addition of several attention mechanisms. Several additions motivated by 'Compounding the Performance Improvements...' ( https://arxiv.org/abs/2001.06268 ): Move layer/module impl into layers subfolder/module of models and organize in a more granular fashion ResNet downsample paths now properly support dilation (output stride != 32) for avg_pool ('D' variant) and 3x3 (SENets) networks Add Selective Kernel Nets on top of ResNet base, pretrained weights skresnet18 - 73% top-1 skresnet34 - 76.9% top-1 skresnext50_32x4d (equiv to SKNet50) - 80.2% top-1 ECA and CECA (circular padding) attention layer contributed by Chris Ha CBAM attention experiment (not the best results so far, may remove) Attention factory to allow dynamically selecting one of SE, ECA, CBAM in the .se position for all ResNets Add DropBlock and DropPath (formerly DropConnect for EfficientNet/MobileNetv3) support to all ResNet variants Full dataset results updated that incl NoisyStudent weights and 2 of the 3 SK weights Feb 12, 2020 Add EfficientNet-L2 and B0-B7 NoisyStudent weights ported from Tensorflow TPU Feb 6, 2020 Add RandAugment trained EfficientNet-ES (EdgeTPU-Small) weights with 78.1 top-1. Trained by Andrew Lavin (see Training section for hparams) Feb \u00bd, 2020 Port new EfficientNet-B8 (RandAugment) weights, these are different than the B8 AdvProp, different input normalization. Update results csv files on all models for ImageNet validation and three other test sets Push PyPi package update Jan 31, 2020 Update ResNet50 weights with a new 79.038 result from further JSD / AugMix experiments. Full command line for reproduction in training section below. Jan 11/12, 2020 Master may be a bit unstable wrt to training, these changes have been tested but not all combos Implementations of AugMix added to existing RA and AA. Including numerous supporting pieces like JSD loss (Jensen-Shannon divergence + CE), and AugMixDataset SplitBatchNorm adaptation layer added for implementing Auxiliary BN as per AdvProp paper ResNet-50 AugMix trained model w/ 79% top-1 added seresnext26tn_32x4d - 77.99 top-1, 93.75 top-5 added to tiered experiment, higher img/s than 't' and 'd' Jan 3, 2020 Add RandAugment trained EfficientNet-B0 weight with 77.7 top-1. Trained by Michael Klachko with this code and recent hparams (see Training section) Add avg_checkpoints.py script for post training weight averaging and update all scripts with header docstrings and shebangs. Dec 30, 2019 Merge Dushyant Mehta's PR for SelecSLS (Selective Short and Long Range Skip Connections) networks. Good GPU memory consumption and throughput. Original: https://github.com/mehtadushy/SelecSLS-Pytorch Dec 28, 2019 Add new model weights and training hparams (see Training Hparams section) efficientnet_b3 - 81.5 top-1, 95.7 top-5 at default res/crop, 81.9, 95.8 at 320x320 1.0 crop-pct trained with RandAugment, ended up with an interesting but less than perfect result (see training section) seresnext26d_32x4d - 77.6 top-1, 93.6 top-5 deep stem (32, 32, 64), avgpool downsample stem/dowsample from bag-of-tricks paper seresnext26t_32x4d - 78.0 top-1, 93.7 top-5 deep tiered stem (24, 48, 64), avgpool downsample (a modified 'D' variant) stem sizing mods from Jeremy Howard and fastai devs discussing ResNet architecture experiments Dec 23, 2019 Add RandAugment trained MixNet-XL weights with 80.48 top-1. --dist-bn argument added to train.py, will distribute BN stats between nodes after each train epoch, before eval Dec 4, 2019 Added weights from the first training from scratch of an EfficientNet (B2) with my new RandAugment implementation. Much better than my previous B2 and very close to the official AdvProp ones (80.4 top-1, 95.08 top-5). Nov 29, 2019 Brought EfficientNet and MobileNetV3 up to date with my https://github.com/rwightman/gen-efficientnet-pytorch code. Torchscript and ONNX export compat excluded. AdvProp weights added Official TF MobileNetv3 weights added EfficientNet and MobileNetV3 hook based 'feature extraction' classes added. Will serve as basis for using models as backbones in obj detection/segmentation tasks. Lots more to be done here... HRNet classification models and weights added from https://github.com/HRNet/HRNet-Image-Classification Consistency in global pooling, reset_classifer , and forward_features across models forward_features always returns unpooled feature maps now Reasonable chance I broke something... let me know Nov 22, 2019 Add ImageNet training RandAugment implementation alongside AutoAugment. PyTorch Transform compatible format, using PIL. Currently training two EfficientNet models from scratch with promising results... will update. drop-connect cmd line arg finally added to train.py , no need to hack model fns. Works for efficientnet/mobilenetv3 based models, ignored otherwise.","title":"Archived Changes"},{"location":"archived_changes/#archived-changes","text":"","title":"Archived Changes"},{"location":"archived_changes/#june-8-2021","text":"Add first ResMLP weights, trained in PyTorch XLA on TPU-VM w/ my XLA branch. 24 block variant, 79.2 top-1. Add ResNet51-Q model w/ pretrained weights at 82.36 top-1. NFNet inspired block layout with quad layer stem and no maxpool Same param count (35.7M) and throughput as ResNetRS-50 but +1.5 top-1 @ 224x224 and +2.5 top-1 at 288x288","title":"June 8, 2021"},{"location":"archived_changes/#may-25-2021","text":"Add LeViT, Visformer, Convit (PR by Aman Arora), Twins (PR by paper authors) transformer models Cleanup input_size/img_size override handling and testing for all vision transformer models Add efficientnetv2_rw_m model and weights (started training before official code). 84.8 top-1, 53M params.","title":"May 25, 2021"},{"location":"archived_changes/#may-14-2021","text":"Add EfficientNet-V2 official model defs w/ ported weights from official Tensorflow/Keras impl. 1k trained variants: tf_efficientnetv2_s/m/l 21k trained variants: tf_efficientnetv2_s/m/l_in21k 21k pretrained -> 1k fine-tuned: tf_efficientnetv2_s/m/l_in21ft1k v2 models w/ v1 scaling: tf_efficientnetv2_b0 through b3 Rename my prev V2 guess efficientnet_v2s -> efficientnetv2_rw_s Some blank efficientnetv2_* models in-place for future native PyTorch training","title":"May 14, 2021"},{"location":"archived_changes/#may-5-2021","text":"Add MLP-Mixer models and port pretrained weights from Google JAX impl Add CaiT models and pretrained weights from FB Add ResNet-RS models and weights from TF . Thanks Aman Arora Add CoaT models and weights. Thanks Mohammed Rizin Add new ImageNet-21k weights & finetuned weights for TResNet, MobileNet-V3, ViT models. Thanks mrT Add GhostNet models and weights. Thanks Kai Han Update ByoaNet attention modles Improve SA module inits Hack together experimental stand-alone Swin based attn module and swinnet Consistent '26t' model defs for experiments. Add improved Efficientnet-V2S (prelim model def) weights. 83.8 top-1. WandB logging support","title":"May 5, 2021"},{"location":"archived_changes/#april-13-2021","text":"Add Swin Transformer models and weights from https://github.com/microsoft/Swin-Transformer","title":"April 13, 2021"},{"location":"archived_changes/#april-12-2021","text":"Add ECA-NFNet-L1 (slimmed down F1 w/ SiLU, 41M params) trained with this code. 84% top-1 @ 320x320. Trained at 256x256. Add EfficientNet-V2S model (unverified model definition) weights. 83.3 top-1 @ 288x288. Only trained single res 224. Working on progressive training. Add ByoaNet model definition (Bring-your-own-attention) w/ SelfAttention block and corresponding SA/SA-like modules and model defs Lambda Networks - https://arxiv.org/abs/2102.08602 Bottleneck Transformers - https://arxiv.org/abs/2101.11605 Halo Nets - https://arxiv.org/abs/2103.12731 Adabelief optimizer contributed by Juntang Zhuang","title":"April 12, 2021"},{"location":"archived_changes/#april-1-2021","text":"Add snazzy benchmark.py script for bulk timm model benchmarking of train and/or inference Add Pooling-based Vision Transformer (PiT) models (from https://github.com/naver-ai/pit ) Merged distilled variant into main for torchscript compatibility Some timm cleanup/style tweaks and weights have hub download support Cleanup Vision Transformer (ViT) models Merge distilled (DeiT) model into main so that torchscript can work Support updated weight init (defaults to old still) that closer matches original JAX impl (possibly better training from scratch) Separate hybrid model defs into different file and add several new model defs to fiddle with, support patch_size != 1 for hybrids Fix fine-tuning num_class changes (PiT and ViT) and pos_embed resizing (Vit) with distilled variants nn.Sequential for block stack (does not break downstream compat) TnT (Transformer-in-Transformer) models contributed by author (from https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/TNT ) Add RegNetY-160 weights from DeiT teacher model Add new NFNet-L0 w/ SE attn (rename nfnet_l0b -> nfnet_l0 ) weights 82.75 top-1 @ 288x288 Some fixes/improvements for TFDS dataset wrapper","title":"April 1, 2021"},{"location":"archived_changes/#march-7-2021","text":"First 0.4.x PyPi release w/ NFNets (& related), ByoB (GPU-Efficient, RepVGG, etc). Change feature extraction for pre-activation nets (NFNets, ResNetV2) to return features before activation.","title":"March 7, 2021"},{"location":"archived_changes/#feb-18-2021","text":"Add pretrained weights and model variants for NFNet-F* models from DeepMind Haiku impl . Models are prefixed with dm_ . They require SAME padding conv, skipinit enabled, and activation gains applied in act fn. These models are big, expect to run out of GPU memory. With the GELU activiation + other options, they are roughly \u00bd the inference speed of my SiLU PyTorch optimized s variants. Original model results are based on pre-processing that is not the same as all other models so you'll see different results in the results csv (once updated). Matching the original pre-processing as closely as possible I get these results: dm_nfnet_f6 - 86.352 dm_nfnet_f5 - 86.100 dm_nfnet_f4 - 85.834 dm_nfnet_f3 - 85.676 dm_nfnet_f2 - 85.178 dm_nfnet_f1 - 84.696 dm_nfnet_f0 - 83.464","title":"Feb 18, 2021"},{"location":"archived_changes/#feb-16-2021","text":"Add Adaptive Gradient Clipping (AGC) as per https://arxiv.org/abs/2102.06171 . Integrated w/ PyTorch gradient clipping via mode arg that defaults to prev 'norm' mode. For backward arg compat, clip-grad arg must be specified to enable when using train.py. AGC w/ default clipping factor --clip-grad .01 --clip-mode agc PyTorch global norm of 1.0 (old behaviour, always norm), --clip-grad 1.0 PyTorch value clipping of 10, --clip-grad 10. --clip-mode value AGC performance is definitely sensitive to the clipping factor. More experimentation needed to determine good values for smaller batch sizes and optimizers besides those in paper. So far I've found .001-.005 is necessary for stable RMSProp training w/ NFNet/NF-ResNet.","title":"Feb 16, 2021"},{"location":"archived_changes/#feb-12-2021","text":"Update Normalization-Free nets to include new NFNet-F ( https://arxiv.org/abs/2102.06171 ) model defs","title":"Feb 12, 2021"},{"location":"archived_changes/#feb-10-2021","text":"More model archs, incl a flexible ByobNet backbone ('Bring-your-own-blocks') GPU-Efficient-Networks ( https://github.com/idstcv/GPU-Efficient-Networks ), impl in byobnet.py RepVGG ( https://github.com/DingXiaoH/RepVGG ), impl in byobnet.py classic VGG (from torchvision, impl in vgg ) Refinements to normalizer layer arg handling and normalizer+act layer handling in some models Default AMP mode changed to native PyTorch AMP instead of APEX. Issues not being fixed with APEX. Native works with --channels-last and --torchscript model training, APEX does not. Fix a few bugs introduced since last pypi release","title":"Feb 10, 2021"},{"location":"archived_changes/#feb-8-2021","text":"Add several ResNet weights with ECA attention. 26t & 50t trained @ 256, test @ 320. 269d train @ 256, fine-tune @320, test @ 352. ecaresnet26t - 79.88 top-1 @ 320x320, 79.08 @ 256x256 ecaresnet50t - 82.35 top-1 @ 320x320, 81.52 @ 256x256 ecaresnet269d - 84.93 top-1 @ 352x352, 84.87 @ 320x320 Remove separate tiered ( t ) vs tiered_narrow ( tn ) ResNet model defs, all tn changed to t and t models removed ( seresnext26t_32x4d only model w/ weights that was removed). Support model default_cfgs with separate train vs test resolution test_input_size and remove extra _320 suffix ResNet model defs that were just for test.","title":"Feb 8, 2021"},{"location":"archived_changes/#jan-30-2021","text":"Add initial \"Normalization Free\" NF-RegNet-B* and NF-ResNet model definitions based on paper","title":"Jan 30, 2021"},{"location":"archived_changes/#jan-25-2021","text":"Add ResNetV2 Big Transfer (BiT) models w/ ImageNet-1k and 21k weights from https://github.com/google-research/big_transfer Add official R50+ViT-B/16 hybrid models + weights from https://github.com/google-research/vision_transformer ImageNet-21k ViT weights are added w/ model defs and representation layer (pre logits) support NOTE: ImageNet-21k classifier heads were zero'd in original weights, they are only useful for transfer learning Add model defs and weights for DeiT Vision Transformer models from https://github.com/facebookresearch/deit Refactor dataset classes into ImageDataset/IterableImageDataset + dataset specific parser classes Add Tensorflow-Datasets (TFDS) wrapper to allow use of TFDS image classification sets with train script Ex: train.py /data/tfds --dataset tfds/oxford_iiit_pet --val-split test --model resnet50 -b 256 --amp --num-classes 37 --opt adamw --lr 3e-4 --weight-decay .001 --pretrained -j 2 Add improved .tar dataset parser that reads images from .tar, folder of .tar files, or .tar within .tar Run validation on full ImageNet-21k directly from tar w/ BiT model: validate.py /data/fall11_whole.tar --model resnetv2_50x1_bitm_in21k --amp Models in this update should be stable w/ possible exception of ViT/BiT, possibility of some regressions with train/val scripts and dataset handling","title":"Jan 25, 2021"},{"location":"archived_changes/#jan-3-2021","text":"Add SE-ResNet-152D weights 256x256 val, 0.94 crop top-1 - 83.75 320x320 val, 1.0 crop - 84.36 Update results files","title":"Jan 3, 2021"},{"location":"archived_changes/#dec-18-2020","text":"Add ResNet-101D, ResNet-152D, and ResNet-200D weights trained @ 256x256 256x256 val, 0.94 crop (top-1) - 101D (82.33), 152D (83.08), 200D (83.25) 288x288 val, 1.0 crop - 101D (82.64), 152D (83.48), 200D (83.76) 320x320 val, 1.0 crop - 101D (83.00), 152D (83.66), 200D (84.01)","title":"Dec 18, 2020"},{"location":"archived_changes/#dec-7-2020","text":"Simplify EMA module (ModelEmaV2), compatible with fully torchscripted models Misc fixes for SiLU ONNX export, default_cfg missing from Feature extraction models, Linear layer w/ AMP + torchscript PyPi release @ 0.3.2 (needed by EfficientDet)","title":"Dec 7, 2020"},{"location":"archived_changes/#oct-30-2020","text":"Test with PyTorch 1.7 and fix a small top-n metric view vs reshape issue. Convert newly added 224x224 Vision Transformer weights from official JAX repo. 81.8 top-1 for B/16, 83.1 L/16. Support PyTorch 1.7 optimized, native SiLU (aka Swish) activation. Add mapping to 'silu' name, custom swish will eventually be deprecated. Fix regression for loading pretrained classifier via direct model entrypoint functions. Didn't impact create_model() factory usage. PyPi release @ 0.3.0 version!","title":"Oct 30, 2020"},{"location":"archived_changes/#oct-26-2020","text":"Update Vision Transformer models to be compatible with official code release at https://github.com/google-research/vision_transformer Add Vision Transformer weights (ImageNet-21k pretrain) for 384x384 base and large models converted from official jax impl ViT-B/16 - 84.2 ViT-B/32 - 81.7 ViT-L/16 - 85.2 ViT-L/32 - 81.5","title":"Oct 26, 2020"},{"location":"archived_changes/#oct-21-2020","text":"Weights added for Vision Transformer (ViT) models. 77.86 top-1 for 'small' and 79.35 for 'base'. Thanks to Christof for training the base model w/ lots of GPUs.","title":"Oct 21, 2020"},{"location":"archived_changes/#oct-13-2020","text":"Initial impl of Vision Transformer models. Both patch and hybrid (CNN backbone) variants. Currently trying to train... Adafactor and AdaHessian (FP32 only, no AMP) optimizers EdgeTPU-M ( efficientnet_em ) model trained in PyTorch, 79.3 top-1 Pip release, doc updates pending a few more changes...","title":"Oct 13, 2020"},{"location":"archived_changes/#sept-18-2020","text":"New ResNet 'D' weights. 72.7 (top-1) ResNet-18-D, 77.1 ResNet-34-D, 80.5 ResNet-50-D Added a few untrained defs for other ResNet models (66D, 101D, 152D, 200/200D)","title":"Sept 18, 2020"},{"location":"archived_changes/#sept-3-2020","text":"New weights Wide-ResNet50 - 81.5 top-1 (vs 78.5 torchvision) SEResNeXt50-32x4d - 81.3 top-1 (vs 79.1 cadene) Support for native Torch AMP and channels_last memory format added to train/validate scripts ( --channels-last , --native-amp vs --apex-amp ) Models tested with channels_last on latest NGC 20.08 container. AdaptiveAvgPool in attn layers changed to mean((2,3)) to work around bug with NHWC kernel.","title":"Sept 3, 2020"},{"location":"archived_changes/#aug-12-2020","text":"New/updated weights from training experiments EfficientNet-B3 - 82.1 top-1 (vs 81.6 for official with AA and 81.9 for AdvProp) RegNetY-3.2GF - 82.0 top-1 (78.9 from official ver) CSPResNet50 - 79.6 top-1 (76.6 from official ver) Add CutMix integrated w/ Mixup. See pull request for some usage examples Some fixes for using pretrained weights with in_chans != 3 on several models.","title":"Aug 12, 2020"},{"location":"archived_changes/#aug-5-2020","text":"Universal feature extraction, new models, new weights, new test sets. * All models support the features_only=True argument for create_model call to return a network that extracts feature maps from the deepest layer at each stride. * New models * CSPResNet, CSPResNeXt, CSPDarkNet, DarkNet * ReXNet * (Modified Aligned) Xception41/65/71 (a proper port of TF models) * New trained weights * SEResNet50 - 80.3 top-1 * CSPDarkNet53 - 80.1 top-1 * CSPResNeXt50 - 80.0 top-1 * DPN68b - 79.2 top-1 * EfficientNet-Lite0 (non-TF ver) - 75.5 (submitted by @hal-314 ) * Add 'real' labels for ImageNet and ImageNet-Renditions test set, see results/README.md * Test set ranking/top-n diff script by @KushajveerSingh * Train script and loader/transform tweaks to punch through more aug arguments * README and documentation overhaul. See initial (WIP) documentation at https://rwightman.github.io/pytorch-image-models/ * adamp and sgdp optimizers added by @hellbell","title":"Aug 5, 2020"},{"location":"archived_changes/#june-11-2020","text":"Bunch of changes: * DenseNet models updated with memory efficient addition from torchvision (fixed a bug), blur pooling and deep stem additions * VoVNet V1 and V2 models added, 39 V2 variant (ese_vovnet_39b) trained to 79.3 top-1 * Activation factory added along with new activations: * select act at model creation time for more flexibility in using activations compatible with scripting or tracing (ONNX export) * hard_mish (experimental) added with memory-efficient grad, along with ME hard_swish * context mgr for setting exportable/scriptable/no_jit states * Norm + Activation combo layers added with initial trial support in DenseNet and VoVNet along with impl of EvoNorm and InplaceAbn wrapper that fit the interface * Torchscript works for all but two of the model types as long as using Pytorch 1.5+, tests added for this * Some import cleanup and classifier reset changes, all models will have classifier reset to nn.Identity on reset_classifer(0) call * Prep for 0.1.28 pip release","title":"June 11, 2020"},{"location":"archived_changes/#may-12-2020","text":"Add ResNeSt models (code adapted from https://github.com/zhanghang1989/ResNeSt , paper https://arxiv.org/abs/2004.08955 ))","title":"May 12, 2020"},{"location":"archived_changes/#may-3-2020","text":"Pruned EfficientNet B1, B2, and B3 ( https://arxiv.org/abs/2002.08258 ) contributed by Yonathan Aflalo","title":"May 3, 2020"},{"location":"archived_changes/#may-1-2020","text":"Merged a number of execellent contributions in the ResNet model family over the past month BlurPool2D and resnetblur models initiated by Chris Ha , I trained resnetblur50 to 79.3. TResNet models and SpaceToDepth, AntiAliasDownsampleLayer layers by mrT23 ecaresnet (50d, 101d, light) models and two pruned variants using pruning as per ( https://arxiv.org/abs/2002.08258 ) by Yonathan Aflalo 200 pretrained models in total now with updated results csv in results folder","title":"May 1, 2020"},{"location":"archived_changes/#april-5-2020","text":"Add some newly trained MobileNet-V2 models trained with latest h-params, rand augment. They compare quite favourably to EfficientNet-Lite 3.5M param MobileNet-V2 100 @ 73% 4.5M param MobileNet-V2 110d @ 75% 6.1M param MobileNet-V2 140 @ 76.5% 5.8M param MobileNet-V2 120d @ 77.3%","title":"April 5, 2020"},{"location":"archived_changes/#march-18-2020","text":"Add EfficientNet-Lite models w/ weights ported from Tensorflow TPU Add RandAugment trained ResNeXt-50 32x4d weights with 79.8 top-1. Trained by Andrew Lavin (see Training section for hparams)","title":"March 18, 2020"},{"location":"archived_changes/#april-5-2020_1","text":"Add some newly trained MobileNet-V2 models trained with latest h-params, rand augment. They compare quite favourably to EfficientNet-Lite 3.5M param MobileNet-V2 100 @ 73% 4.5M param MobileNet-V2 110d @ 75% 6.1M param MobileNet-V2 140 @ 76.5% 5.8M param MobileNet-V2 120d @ 77.3%","title":"April 5, 2020"},{"location":"archived_changes/#march-18-2020_1","text":"Add EfficientNet-Lite models w/ weights ported from Tensorflow TPU Add RandAugment trained ResNeXt-50 32x4d weights with 79.8 top-1. Trained by Andrew Lavin (see Training section for hparams)","title":"March 18, 2020"},{"location":"archived_changes/#feb-29-2020","text":"New MobileNet-V3 Large weights trained from stratch with this code to 75.77% top-1 IMPORTANT CHANGE - default weight init changed for all MobilenetV3 / EfficientNet / related models overall results similar to a bit better training from scratch on a few smaller models tried performance early in training seems consistently improved but less difference by end set fix_group_fanout=False in _init_weight_goog fn if you need to reproducte past behaviour Experimental LR noise feature added applies a random perturbation to LR each epoch in specified range of training","title":"Feb 29, 2020"},{"location":"archived_changes/#feb-18-2020","text":"Big refactor of model layers and addition of several attention mechanisms. Several additions motivated by 'Compounding the Performance Improvements...' ( https://arxiv.org/abs/2001.06268 ): Move layer/module impl into layers subfolder/module of models and organize in a more granular fashion ResNet downsample paths now properly support dilation (output stride != 32) for avg_pool ('D' variant) and 3x3 (SENets) networks Add Selective Kernel Nets on top of ResNet base, pretrained weights skresnet18 - 73% top-1 skresnet34 - 76.9% top-1 skresnext50_32x4d (equiv to SKNet50) - 80.2% top-1 ECA and CECA (circular padding) attention layer contributed by Chris Ha CBAM attention experiment (not the best results so far, may remove) Attention factory to allow dynamically selecting one of SE, ECA, CBAM in the .se position for all ResNets Add DropBlock and DropPath (formerly DropConnect for EfficientNet/MobileNetv3) support to all ResNet variants Full dataset results updated that incl NoisyStudent weights and 2 of the 3 SK weights","title":"Feb 18, 2020"},{"location":"archived_changes/#feb-12-2020","text":"Add EfficientNet-L2 and B0-B7 NoisyStudent weights ported from Tensorflow TPU","title":"Feb 12, 2020"},{"location":"archived_changes/#feb-6-2020","text":"Add RandAugment trained EfficientNet-ES (EdgeTPU-Small) weights with 78.1 top-1. Trained by Andrew Lavin (see Training section for hparams)","title":"Feb 6, 2020"},{"location":"archived_changes/#feb-12-2020_1","text":"Port new EfficientNet-B8 (RandAugment) weights, these are different than the B8 AdvProp, different input normalization. Update results csv files on all models for ImageNet validation and three other test sets Push PyPi package update","title":"Feb 1/2, 2020"},{"location":"archived_changes/#jan-31-2020","text":"Update ResNet50 weights with a new 79.038 result from further JSD / AugMix experiments. Full command line for reproduction in training section below.","title":"Jan 31, 2020"},{"location":"archived_changes/#jan-1112-2020","text":"Master may be a bit unstable wrt to training, these changes have been tested but not all combos Implementations of AugMix added to existing RA and AA. Including numerous supporting pieces like JSD loss (Jensen-Shannon divergence + CE), and AugMixDataset SplitBatchNorm adaptation layer added for implementing Auxiliary BN as per AdvProp paper ResNet-50 AugMix trained model w/ 79% top-1 added seresnext26tn_32x4d - 77.99 top-1, 93.75 top-5 added to tiered experiment, higher img/s than 't' and 'd'","title":"Jan 11/12, 2020"},{"location":"archived_changes/#jan-3-2020","text":"Add RandAugment trained EfficientNet-B0 weight with 77.7 top-1. Trained by Michael Klachko with this code and recent hparams (see Training section) Add avg_checkpoints.py script for post training weight averaging and update all scripts with header docstrings and shebangs.","title":"Jan 3, 2020"},{"location":"archived_changes/#dec-30-2019","text":"Merge Dushyant Mehta's PR for SelecSLS (Selective Short and Long Range Skip Connections) networks. Good GPU memory consumption and throughput. Original: https://github.com/mehtadushy/SelecSLS-Pytorch","title":"Dec 30, 2019"},{"location":"archived_changes/#dec-28-2019","text":"Add new model weights and training hparams (see Training Hparams section) efficientnet_b3 - 81.5 top-1, 95.7 top-5 at default res/crop, 81.9, 95.8 at 320x320 1.0 crop-pct trained with RandAugment, ended up with an interesting but less than perfect result (see training section) seresnext26d_32x4d - 77.6 top-1, 93.6 top-5 deep stem (32, 32, 64), avgpool downsample stem/dowsample from bag-of-tricks paper seresnext26t_32x4d - 78.0 top-1, 93.7 top-5 deep tiered stem (24, 48, 64), avgpool downsample (a modified 'D' variant) stem sizing mods from Jeremy Howard and fastai devs discussing ResNet architecture experiments","title":"Dec 28, 2019"},{"location":"archived_changes/#dec-23-2019","text":"Add RandAugment trained MixNet-XL weights with 80.48 top-1. --dist-bn argument added to train.py, will distribute BN stats between nodes after each train epoch, before eval","title":"Dec 23, 2019"},{"location":"archived_changes/#dec-4-2019","text":"Added weights from the first training from scratch of an EfficientNet (B2) with my new RandAugment implementation. Much better than my previous B2 and very close to the official AdvProp ones (80.4 top-1, 95.08 top-5).","title":"Dec 4, 2019"},{"location":"archived_changes/#nov-29-2019","text":"Brought EfficientNet and MobileNetV3 up to date with my https://github.com/rwightman/gen-efficientnet-pytorch code. Torchscript and ONNX export compat excluded. AdvProp weights added Official TF MobileNetv3 weights added EfficientNet and MobileNetV3 hook based 'feature extraction' classes added. Will serve as basis for using models as backbones in obj detection/segmentation tasks. Lots more to be done here... HRNet classification models and weights added from https://github.com/HRNet/HRNet-Image-Classification Consistency in global pooling, reset_classifer , and forward_features across models forward_features always returns unpooled feature maps now Reasonable chance I broke something... let me know","title":"Nov 29, 2019"},{"location":"archived_changes/#nov-22-2019","text":"Add ImageNet training RandAugment implementation alongside AutoAugment. PyTorch Transform compatible format, using PIL. Currently training two EfficientNet models from scratch with promising results... will update. drop-connect cmd line arg finally added to train.py , no need to hack model fns. Works for efficientnet/mobilenetv3 based models, ignored otherwise.","title":"Nov 22, 2019"},{"location":"changes/","text":"Recent Changes March 23, 2022 Add ParallelBlock and LayerScale option to base vit models to support model configs in Three things everyone should know about ViT convnext_tiny_hnf (head norm first) weights trained with (close to) A2 recipe, 82.2% top-1, could do better with more epochs. March 21, 2022 Merge norm_norm_norm . IMPORTANT this update for a coming 0.6.x release will likely de-stabilize the master branch for a while. Branch 0.5.x or a previous 0.5.x release can be used if stability is required. Significant weights update (all TPU trained) as described in this release regnety_040 - 82.3 @ 224, 82.96 @ 288 regnety_064 - 83.0 @ 224, 83.65 @ 288 regnety_080 - 83.17 @ 224, 83.86 @ 288 regnetv_040 - 82.44 @ 224, 83.18 @ 288 (timm pre-act) regnetv_064 - 83.1 @ 224, 83.71 @ 288 (timm pre-act) regnetz_040 - 83.67 @ 256, 84.25 @ 320 regnetz_040h - 83.77 @ 256, 84.5 @ 320 (w/ extra fc in head) resnetv2_50d_gn - 80.8 @ 224, 81.96 @ 288 (pre-act GroupNorm) resnetv2_50d_evos 80.77 @ 224, 82.04 @ 288 (pre-act EvoNormS) regnetz_c16_evos - 81.9 @ 256, 82.64 @ 320 (EvoNormS) regnetz_d8_evos - 83.42 @ 256, 84.04 @ 320 (EvoNormS) xception41p - 82 @ 299 (timm pre-act) xception65 - 83.17 @ 299 xception65p - 83.14 @ 299 (timm pre-act) resnext101_64x4d - 82.46 @ 224, 83.16 @ 288 seresnext101_32x8d - 83.57 @ 224, 84.270 @ 288 resnetrs200 - 83.85 @ 256, 84.44 @ 320 HuggingFace hub support fixed w/ initial groundwork for allowing alternative 'config sources' for pretrained model definitions and weights (generic local file / remote url support soon) SwinTransformer-V2 implementation added. Submitted by Christoph Reich . Training experiments and model changes by myself are ongoing so expect compat breaks. Swin-S3 (AutoFormerV2) models / weights added from https://github.com/microsoft/Cream/tree/main/AutoFormerV2 MobileViT models w/ weights adapted from https://github.com/apple/ml-cvnets PoolFormer models w/ weights adapted from https://github.com/sail-sg/poolformer VOLO models w/ weights adapted from https://github.com/sail-sg/volo Significant work experimenting with non-BatchNorm norm layers such as EvoNorm, FilterResponseNorm, GroupNorm, etc Enhance support for alternate norm + act ('NormAct') layers added to a number of models, esp EfficientNet/MobileNetV3, RegNet, and aligned Xception Grouped conv support added to EfficientNet family Add 'group matching' API to all models to allow grouping model parameters for application of 'layer-wise' LR decay, lr scale added to LR scheduler Gradient checkpointing support added to many models forward_head(x, pre_logits=False) fn added to all models to allow separate calls of forward_features + forward_head All vision transformer and vision MLP models update to return non-pooled / non-token selected features from foward_features , for consistency with CNN models, token selection or pooling now applied in forward_head Feb 2, 2022 Chris Hughes posted an exhaustive run through of timm on his blog yesterday. Well worth a read. Getting Started with PyTorch Image Models (timm): A Practitioner\u2019s Guide I'm currently prepping to merge the norm_norm_norm branch back to master (ver 0.6.x) in next week or so. The changes are more extensive than usual and may destabilize and break some model API use (aiming for full backwards compat). So, beware pip install git+https://github.com/rwightman/pytorch-image-models installs! 0.5.x releases and a 0.5.x branch will remain stable with a cherry pick or two until dust clears. Recommend sticking to pypi install for a bit if you want stable. Jan 14, 2022 Version 0.5.4 w/ release to be pushed to pypi. It's been a while since last pypi update and riskier changes will be merged to main branch soon.... Add ConvNeXT models /w weights from official impl ( https://github.com/facebookresearch/ConvNeXt ), a few perf tweaks, compatible with timm features Tried training a few small (~1.8-3M param) / mobile optimized models, a few are good so far, more on the way... mnasnet_small - 65.6 top-1 mobilenetv2_050 - 65.9 lcnet_100/075/050 - 72.1 / 68.8 / 63.1 semnasnet_075 - 73 fbnetv3_b/d/g - 79.1 / 79.7 / 82.0 TinyNet models added by rsomani95 LCNet added via MobileNetV3 architecture Nov 22, 2021 A number of updated weights anew new model defs eca_halonext26ts - 79.5 @ 256 resnet50_gn (new) - 80.1 @ 224, 81.3 @ 288 resnet50 - 80.7 @ 224, 80.9 @ 288 (trained at 176, not replacing current a1 weights as default since these don't scale as well to higher res, weights ) resnext50_32x4d - 81.1 @ 224, 82.0 @ 288 sebotnet33ts_256 (new) - 81.2 @ 224 lamhalobotnet50ts_256 - 81.5 @ 256 halonet50ts - 81.7 @ 256 halo2botnet50ts_256 - 82.0 @ 256 resnet101 - 82.0 @ 224, 82.8 @ 288 resnetv2_101 (new) - 82.1 @ 224, 83.0 @ 288 resnet152 - 82.8 @ 224, 83.5 @ 288 regnetz_d8 (new) - 83.5 @ 256, 84.0 @ 320 regnetz_e8 (new) - 84.5 @ 256, 85.0 @ 320 vit_base_patch8_224 (85.8 top-1) & in21k variant weights added thanks Martins Bruveris Groundwork in for FX feature extraction thanks to Alexander Soare models updated for tracing compatibility (almost full support with some distlled transformer exceptions) Oct 19, 2021 ResNet strikes back ( https://arxiv.org/abs/2110.00476 ) weights added, plus any extra training components used. Model weights and some more details here ( https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-rsb-weights ) BCE loss and Repeated Augmentation support for RSB paper 4 series of ResNet based attention model experiments being added (implemented across byobnet.py/byoanet.py). These include all sorts of attention, from channel attn like SE, ECA to 2D QKV self-attention layers such as Halo, Bottlneck, Lambda. Details here ( https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights ) Working implementations of the following 2D self-attention modules (likely to be differences from paper or eventual official impl): Halo ( https://arxiv.org/abs/2103.12731 ) Bottleneck Transformer ( https://arxiv.org/abs/2101.11605 ) LambdaNetworks ( https://arxiv.org/abs/2102.08602 ) A RegNetZ series of models with some attention experiments (being added to). These do not follow the paper ( https://arxiv.org/abs/2103.06877 ) in any way other than block architecture, details of official models are not available. See more here ( https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights ) ConvMixer ( https://openreview.net/forum?id=TVHS5Y4dNvM ), CrossVit ( https://arxiv.org/abs/2103.14899 ), and BeiT ( https://arxiv.org/abs/2106.08254 ) architectures + weights added freeze/unfreeze helpers by Alexander Soare Aug 18, 2021 Optimizer bonanza! Add LAMB and LARS optimizers, incl trust ratio clipping options. Tweaked to work properly in PyTorch XLA (tested on TPUs w/ timm bits branch ) Add MADGRAD from FB research w/ a few tweaks (decoupled decay option, step handling that works with PyTorch XLA) Some cleanup on all optimizers and factory. No more .data , a bit more consistency, unit tests for all! SGDP and AdamP still won't work with PyTorch XLA but others should (have yet to test Adabelief, Adafactor, Adahessian myself). EfficientNet-V2 XL TF ported weights added, but they don't validate well in PyTorch (L is better). The pre-processing for the V2 TF training is a bit diff and the fine-tuned 21k -> 1k weights are very sensitive and less robust than the 1k weights. Added PyTorch trained EfficientNet-V2 'Tiny' w/ GlobalContext attn weights. Only .1-.2 top-1 better than the SE so more of a curiosity for those interested. July 12, 2021 Add XCiT models from official facebook impl . Contributed by Alexander Soare July 5-9, 2021 Add efficientnetv2_rw_t weights, a custom 'tiny' 13.6M param variant that is a bit better than (non NoisyStudent) B3 models. Both faster and better accuracy (at same or lower res) top-1 82.34 @ 288x288 and 82.54 @ 320x320 Add SAM pretrained in1k weight for ViT B/16 ( vit_base_patch16_sam_224 ) and B/32 ( vit_base_patch32_sam_224 ) models. Add 'Aggregating Nested Transformer' (NesT) w/ weights converted from official Flax impl . Contributed by Alexander Soare . jx_nest_base - 83.534, jx_nest_small - 83.120, jx_nest_tiny - 81.426 June 23, 2021 Reproduce gMLP model training, gmlp_s16_224 trained to 79.6 top-1, matching paper . Hparams for this and other recent MLP training here June 20, 2021 Release Vision Transformer 'AugReg' weights from How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers .npz weight loading support added, can load any of the 50K+ weights from the AugReg series See example notebook from official impl for navigating the augreg weights Replaced all default weights w/ best AugReg variant (if possible). All AugReg 21k classifiers work. Highlights: vit_large_patch16_384 (87.1 top-1), vit_large_r50_s32_384 (86.2 top-1), vit_base_patch16_384 (86.0 top-1) vit_deit_* renamed to just deit_* Remove my old small model, replace with DeiT compatible small w/ AugReg weights Add 1 st training of my gmixer_24_224 MLP /w GLU, 78.1 top-1 w/ 25M params. Add weights from official ResMLP release ( https://github.com/facebookresearch/deit ) Add eca_nfnet_l2 weights from my 'lightweight' series. 84.7 top-1 at 384x384. Add distilled BiT 50x1 student and 152x2 Teacher weights from Knowledge distillation: A good teacher is patient and consistent NFNets and ResNetV2-BiT models work w/ Pytorch XLA now weight standardization uses F.batch_norm instead of std_mean (std_mean wasn't lowered) eps values adjusted, will be slight differences but should be quite close Improve test coverage and classifier interface of non-conv (vision transformer and mlp) models Cleanup a few classifier / flatten details for models w/ conv classifiers or early global pool Please report any regressions, this PR touched quite a few models.","title":"Recent Changes"},{"location":"changes/#recent-changes","text":"","title":"Recent Changes"},{"location":"changes/#march-23-2022","text":"Add ParallelBlock and LayerScale option to base vit models to support model configs in Three things everyone should know about ViT convnext_tiny_hnf (head norm first) weights trained with (close to) A2 recipe, 82.2% top-1, could do better with more epochs.","title":"March 23, 2022"},{"location":"changes/#march-21-2022","text":"Merge norm_norm_norm . IMPORTANT this update for a coming 0.6.x release will likely de-stabilize the master branch for a while. Branch 0.5.x or a previous 0.5.x release can be used if stability is required. Significant weights update (all TPU trained) as described in this release regnety_040 - 82.3 @ 224, 82.96 @ 288 regnety_064 - 83.0 @ 224, 83.65 @ 288 regnety_080 - 83.17 @ 224, 83.86 @ 288 regnetv_040 - 82.44 @ 224, 83.18 @ 288 (timm pre-act) regnetv_064 - 83.1 @ 224, 83.71 @ 288 (timm pre-act) regnetz_040 - 83.67 @ 256, 84.25 @ 320 regnetz_040h - 83.77 @ 256, 84.5 @ 320 (w/ extra fc in head) resnetv2_50d_gn - 80.8 @ 224, 81.96 @ 288 (pre-act GroupNorm) resnetv2_50d_evos 80.77 @ 224, 82.04 @ 288 (pre-act EvoNormS) regnetz_c16_evos - 81.9 @ 256, 82.64 @ 320 (EvoNormS) regnetz_d8_evos - 83.42 @ 256, 84.04 @ 320 (EvoNormS) xception41p - 82 @ 299 (timm pre-act) xception65 - 83.17 @ 299 xception65p - 83.14 @ 299 (timm pre-act) resnext101_64x4d - 82.46 @ 224, 83.16 @ 288 seresnext101_32x8d - 83.57 @ 224, 84.270 @ 288 resnetrs200 - 83.85 @ 256, 84.44 @ 320 HuggingFace hub support fixed w/ initial groundwork for allowing alternative 'config sources' for pretrained model definitions and weights (generic local file / remote url support soon) SwinTransformer-V2 implementation added. Submitted by Christoph Reich . Training experiments and model changes by myself are ongoing so expect compat breaks. Swin-S3 (AutoFormerV2) models / weights added from https://github.com/microsoft/Cream/tree/main/AutoFormerV2 MobileViT models w/ weights adapted from https://github.com/apple/ml-cvnets PoolFormer models w/ weights adapted from https://github.com/sail-sg/poolformer VOLO models w/ weights adapted from https://github.com/sail-sg/volo Significant work experimenting with non-BatchNorm norm layers such as EvoNorm, FilterResponseNorm, GroupNorm, etc Enhance support for alternate norm + act ('NormAct') layers added to a number of models, esp EfficientNet/MobileNetV3, RegNet, and aligned Xception Grouped conv support added to EfficientNet family Add 'group matching' API to all models to allow grouping model parameters for application of 'layer-wise' LR decay, lr scale added to LR scheduler Gradient checkpointing support added to many models forward_head(x, pre_logits=False) fn added to all models to allow separate calls of forward_features + forward_head All vision transformer and vision MLP models update to return non-pooled / non-token selected features from foward_features , for consistency with CNN models, token selection or pooling now applied in forward_head","title":"March 21, 2022"},{"location":"changes/#feb-2-2022","text":"Chris Hughes posted an exhaustive run through of timm on his blog yesterday. Well worth a read. Getting Started with PyTorch Image Models (timm): A Practitioner\u2019s Guide I'm currently prepping to merge the norm_norm_norm branch back to master (ver 0.6.x) in next week or so. The changes are more extensive than usual and may destabilize and break some model API use (aiming for full backwards compat). So, beware pip install git+https://github.com/rwightman/pytorch-image-models installs! 0.5.x releases and a 0.5.x branch will remain stable with a cherry pick or two until dust clears. Recommend sticking to pypi install for a bit if you want stable.","title":"Feb 2, 2022"},{"location":"changes/#jan-14-2022","text":"Version 0.5.4 w/ release to be pushed to pypi. It's been a while since last pypi update and riskier changes will be merged to main branch soon.... Add ConvNeXT models /w weights from official impl ( https://github.com/facebookresearch/ConvNeXt ), a few perf tweaks, compatible with timm features Tried training a few small (~1.8-3M param) / mobile optimized models, a few are good so far, more on the way... mnasnet_small - 65.6 top-1 mobilenetv2_050 - 65.9 lcnet_100/075/050 - 72.1 / 68.8 / 63.1 semnasnet_075 - 73 fbnetv3_b/d/g - 79.1 / 79.7 / 82.0 TinyNet models added by rsomani95 LCNet added via MobileNetV3 architecture","title":"Jan 14, 2022"},{"location":"changes/#nov-22-2021","text":"A number of updated weights anew new model defs eca_halonext26ts - 79.5 @ 256 resnet50_gn (new) - 80.1 @ 224, 81.3 @ 288 resnet50 - 80.7 @ 224, 80.9 @ 288 (trained at 176, not replacing current a1 weights as default since these don't scale as well to higher res, weights ) resnext50_32x4d - 81.1 @ 224, 82.0 @ 288 sebotnet33ts_256 (new) - 81.2 @ 224 lamhalobotnet50ts_256 - 81.5 @ 256 halonet50ts - 81.7 @ 256 halo2botnet50ts_256 - 82.0 @ 256 resnet101 - 82.0 @ 224, 82.8 @ 288 resnetv2_101 (new) - 82.1 @ 224, 83.0 @ 288 resnet152 - 82.8 @ 224, 83.5 @ 288 regnetz_d8 (new) - 83.5 @ 256, 84.0 @ 320 regnetz_e8 (new) - 84.5 @ 256, 85.0 @ 320 vit_base_patch8_224 (85.8 top-1) & in21k variant weights added thanks Martins Bruveris Groundwork in for FX feature extraction thanks to Alexander Soare models updated for tracing compatibility (almost full support with some distlled transformer exceptions)","title":"Nov 22, 2021"},{"location":"changes/#oct-19-2021","text":"ResNet strikes back ( https://arxiv.org/abs/2110.00476 ) weights added, plus any extra training components used. Model weights and some more details here ( https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-rsb-weights ) BCE loss and Repeated Augmentation support for RSB paper 4 series of ResNet based attention model experiments being added (implemented across byobnet.py/byoanet.py). These include all sorts of attention, from channel attn like SE, ECA to 2D QKV self-attention layers such as Halo, Bottlneck, Lambda. Details here ( https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights ) Working implementations of the following 2D self-attention modules (likely to be differences from paper or eventual official impl): Halo ( https://arxiv.org/abs/2103.12731 ) Bottleneck Transformer ( https://arxiv.org/abs/2101.11605 ) LambdaNetworks ( https://arxiv.org/abs/2102.08602 ) A RegNetZ series of models with some attention experiments (being added to). These do not follow the paper ( https://arxiv.org/abs/2103.06877 ) in any way other than block architecture, details of official models are not available. See more here ( https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights ) ConvMixer ( https://openreview.net/forum?id=TVHS5Y4dNvM ), CrossVit ( https://arxiv.org/abs/2103.14899 ), and BeiT ( https://arxiv.org/abs/2106.08254 ) architectures + weights added freeze/unfreeze helpers by Alexander Soare","title":"Oct 19, 2021"},{"location":"changes/#aug-18-2021","text":"Optimizer bonanza! Add LAMB and LARS optimizers, incl trust ratio clipping options. Tweaked to work properly in PyTorch XLA (tested on TPUs w/ timm bits branch ) Add MADGRAD from FB research w/ a few tweaks (decoupled decay option, step handling that works with PyTorch XLA) Some cleanup on all optimizers and factory. No more .data , a bit more consistency, unit tests for all! SGDP and AdamP still won't work with PyTorch XLA but others should (have yet to test Adabelief, Adafactor, Adahessian myself). EfficientNet-V2 XL TF ported weights added, but they don't validate well in PyTorch (L is better). The pre-processing for the V2 TF training is a bit diff and the fine-tuned 21k -> 1k weights are very sensitive and less robust than the 1k weights. Added PyTorch trained EfficientNet-V2 'Tiny' w/ GlobalContext attn weights. Only .1-.2 top-1 better than the SE so more of a curiosity for those interested.","title":"Aug 18, 2021"},{"location":"changes/#july-12-2021","text":"Add XCiT models from official facebook impl . Contributed by Alexander Soare","title":"July 12, 2021"},{"location":"changes/#july-5-9-2021","text":"Add efficientnetv2_rw_t weights, a custom 'tiny' 13.6M param variant that is a bit better than (non NoisyStudent) B3 models. Both faster and better accuracy (at same or lower res) top-1 82.34 @ 288x288 and 82.54 @ 320x320 Add SAM pretrained in1k weight for ViT B/16 ( vit_base_patch16_sam_224 ) and B/32 ( vit_base_patch32_sam_224 ) models. Add 'Aggregating Nested Transformer' (NesT) w/ weights converted from official Flax impl . Contributed by Alexander Soare . jx_nest_base - 83.534, jx_nest_small - 83.120, jx_nest_tiny - 81.426","title":"July 5-9, 2021"},{"location":"changes/#june-23-2021","text":"Reproduce gMLP model training, gmlp_s16_224 trained to 79.6 top-1, matching paper . Hparams for this and other recent MLP training here","title":"June 23, 2021"},{"location":"changes/#june-20-2021","text":"Release Vision Transformer 'AugReg' weights from How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers .npz weight loading support added, can load any of the 50K+ weights from the AugReg series See example notebook from official impl for navigating the augreg weights Replaced all default weights w/ best AugReg variant (if possible). All AugReg 21k classifiers work. Highlights: vit_large_patch16_384 (87.1 top-1), vit_large_r50_s32_384 (86.2 top-1), vit_base_patch16_384 (86.0 top-1) vit_deit_* renamed to just deit_* Remove my old small model, replace with DeiT compatible small w/ AugReg weights Add 1 st training of my gmixer_24_224 MLP /w GLU, 78.1 top-1 w/ 25M params. Add weights from official ResMLP release ( https://github.com/facebookresearch/deit ) Add eca_nfnet_l2 weights from my 'lightweight' series. 84.7 top-1 at 384x384. Add distilled BiT 50x1 student and 152x2 Teacher weights from Knowledge distillation: A good teacher is patient and consistent NFNets and ResNetV2-BiT models work w/ Pytorch XLA now weight standardization uses F.batch_norm instead of std_mean (std_mean wasn't lowered) eps values adjusted, will be slight differences but should be quite close Improve test coverage and classifier interface of non-conv (vision transformer and mlp) models Cleanup a few classifier / flatten details for models w/ conv classifiers or early global pool Please report any regressions, this PR touched quite a few models.","title":"June 20, 2021"},{"location":"feature_extraction/","text":"Feature Extraction All of the models in timm have consistent mechanisms for obtaining various types of features from the model for tasks besides classification. Penultimate Layer Features (Pre-Classifier Features) The features from the penultimate model layer can be obtained in several ways without requiring model surgery (although feel free to do surgery). One must first decide if they want pooled or un-pooled features. Unpooled There are three ways to obtain unpooled features. Without modifying the network, one can call model.forward_features(input) on any model instead of the usual model(input) . This will bypass the head classifier and global pooling for networks. If one wants to explicitly modify the network to return unpooled features, they can either create the model without a classifier and pooling, or remove it later. Both paths remove the parameters associated with the classifier from the network. forward_features() import torch import timm m = timm . create_model ( 'xception41' , pretrained = True ) o = m ( torch . randn ( 2 , 3 , 299 , 299 )) print ( f 'Original shape: { o . shape } ' ) o = m . forward_features ( torch . randn ( 2 , 3 , 299 , 299 )) print ( f 'Unpooled shape: { o . shape } ' ) Output: Original shape: torch.Size([2, 1000]) Unpooled shape: torch.Size([2, 2048, 10, 10]) Create with no classifier and pooling import torch import timm m = timm . create_model ( 'resnet50' , pretrained = True , num_classes = 0 , global_pool = '' ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Unpooled shape: { o . shape } ' ) Output: Unpooled shape: torch.Size([2, 2048, 7, 7]) Remove it later import torch import timm m = timm . create_model ( 'densenet121' , pretrained = True ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Original shape: { o . shape } ' ) m . reset_classifier ( 0 , '' ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Unpooled shape: { o . shape } ' ) Output: Original shape: torch.Size([2, 1000]) Unpooled shape: torch.Size([2, 1024, 7, 7]) Pooled To modify the network to return pooled features, one can use forward_features() and pool/flatten the result themselves, or modify the network like above but keep pooling intact. Create with no classifier import torch import timm m = timm . create_model ( 'resnet50' , pretrained = True , num_classes = 0 ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Pooled shape: { o . shape } ' ) Output: Pooled shape: torch.Size([2, 2048]) Remove it later import torch import timm m = timm . create_model ( 'ese_vovnet19b_dw' , pretrained = True ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Original shape: { o . shape } ' ) m . reset_classifier ( 0 ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Pooled shape: { o . shape } ' ) Output: Pooled shape: torch.Size([2, 1024]) Multi-scale Feature Maps (Feature Pyramid) Object detection, segmentation, keypoint, and a variety of dense pixel tasks require access to feature maps from the backbone network at multiple scales. This is often done by modifying the original classification network. Since each network varies quite a bit in structure, it's not uncommon to see only a few backbones supported in any given obj detection or segmentation library. timm allows a consistent interface for creating any of the included models as feature backbones that output feature maps for selected levels. A feature backbone can be created by adding the argument features_only=True to any create_model call. By default 5 strides will be output from most models (not all have that many), with the first starting at 2 (some start at 1 or 4). Create a feature map extraction model import torch import timm m = timm . create_model ( 'resnest26d' , features_only = True , pretrained = True ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) for x in o : print ( x . shape ) Output: torch.Size([2, 64, 112, 112]) torch.Size([2, 256, 56, 56]) torch.Size([2, 512, 28, 28]) torch.Size([2, 1024, 14, 14]) torch.Size([2, 2048, 7, 7]) Query the feature information After a feature backbone has been created, it can be queried to provide channel or resolution reduction information to the downstream heads without requiring static config or hardcoded constants. The .feature_info attribute is a class encapsulating the information about the feature extraction points. import torch import timm m = timm . create_model ( 'regnety_032' , features_only = True , pretrained = True ) print ( f 'Feature channels: { m . feature_info . channels () } ' ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) for x in o : print ( x . shape ) Output: Feature channels: [32, 72, 216, 576, 1512] torch.Size([2, 32, 112, 112]) torch.Size([2, 72, 56, 56]) torch.Size([2, 216, 28, 28]) torch.Size([2, 576, 14, 14]) torch.Size([2, 1512, 7, 7]) Select specific feature levels or limit the stride There are two additional creation arguments impacting the output features. out_indices selects which indices to output output_stride limits the feature output stride of the network (also works in classification mode BTW) out_indices is supported by all models, but not all models have the same index to feature stride mapping. Look at the code or check feature_info to compare. The out indices generally correspond to the C(i+1)th feature level (a 2^(i+1) reduction). For most models, index 0 is the stride 2 features, and index 4 is stride 32. output_stride is achieved by converting layers to use dilated convolutions. Doing so is not always straightforward, some networks only support output_stride=32 . import torch import timm m = timm . create_model ( 'ecaresnet101d' , features_only = True , output_stride = 8 , out_indices = ( 2 , 4 ), pretrained = True ) print ( f 'Feature channels: { m . feature_info . channels () } ' ) print ( f 'Feature reduction: { m . feature_info . reduction () } ' ) o = m ( torch . randn ( 2 , 3 , 320 , 320 )) for x in o : print ( x . shape ) Output: Feature channels: [512, 2048] Feature reduction: [8, 8] torch.Size([2, 512, 40, 40]) torch.Size([2, 2048, 40, 40])","title":"Feature Extraction"},{"location":"feature_extraction/#feature-extraction","text":"All of the models in timm have consistent mechanisms for obtaining various types of features from the model for tasks besides classification.","title":"Feature Extraction"},{"location":"feature_extraction/#penultimate-layer-features-pre-classifier-features","text":"The features from the penultimate model layer can be obtained in several ways without requiring model surgery (although feel free to do surgery). One must first decide if they want pooled or un-pooled features.","title":"Penultimate Layer Features (Pre-Classifier Features)"},{"location":"feature_extraction/#unpooled","text":"There are three ways to obtain unpooled features. Without modifying the network, one can call model.forward_features(input) on any model instead of the usual model(input) . This will bypass the head classifier and global pooling for networks. If one wants to explicitly modify the network to return unpooled features, they can either create the model without a classifier and pooling, or remove it later. Both paths remove the parameters associated with the classifier from the network.","title":"Unpooled"},{"location":"feature_extraction/#forward_features","text":"import torch import timm m = timm . create_model ( 'xception41' , pretrained = True ) o = m ( torch . randn ( 2 , 3 , 299 , 299 )) print ( f 'Original shape: { o . shape } ' ) o = m . forward_features ( torch . randn ( 2 , 3 , 299 , 299 )) print ( f 'Unpooled shape: { o . shape } ' ) Output: Original shape: torch.Size([2, 1000]) Unpooled shape: torch.Size([2, 2048, 10, 10])","title":"forward_features()"},{"location":"feature_extraction/#create-with-no-classifier-and-pooling","text":"import torch import timm m = timm . create_model ( 'resnet50' , pretrained = True , num_classes = 0 , global_pool = '' ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Unpooled shape: { o . shape } ' ) Output: Unpooled shape: torch.Size([2, 2048, 7, 7])","title":"Create with no classifier and pooling"},{"location":"feature_extraction/#remove-it-later","text":"import torch import timm m = timm . create_model ( 'densenet121' , pretrained = True ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Original shape: { o . shape } ' ) m . reset_classifier ( 0 , '' ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Unpooled shape: { o . shape } ' ) Output: Original shape: torch.Size([2, 1000]) Unpooled shape: torch.Size([2, 1024, 7, 7])","title":"Remove it later"},{"location":"feature_extraction/#pooled","text":"To modify the network to return pooled features, one can use forward_features() and pool/flatten the result themselves, or modify the network like above but keep pooling intact.","title":"Pooled"},{"location":"feature_extraction/#create-with-no-classifier","text":"import torch import timm m = timm . create_model ( 'resnet50' , pretrained = True , num_classes = 0 ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Pooled shape: { o . shape } ' ) Output: Pooled shape: torch.Size([2, 2048])","title":"Create with no classifier"},{"location":"feature_extraction/#remove-it-later_1","text":"import torch import timm m = timm . create_model ( 'ese_vovnet19b_dw' , pretrained = True ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Original shape: { o . shape } ' ) m . reset_classifier ( 0 ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Pooled shape: { o . shape } ' ) Output: Pooled shape: torch.Size([2, 1024])","title":"Remove it later"},{"location":"feature_extraction/#multi-scale-feature-maps-feature-pyramid","text":"Object detection, segmentation, keypoint, and a variety of dense pixel tasks require access to feature maps from the backbone network at multiple scales. This is often done by modifying the original classification network. Since each network varies quite a bit in structure, it's not uncommon to see only a few backbones supported in any given obj detection or segmentation library. timm allows a consistent interface for creating any of the included models as feature backbones that output feature maps for selected levels. A feature backbone can be created by adding the argument features_only=True to any create_model call. By default 5 strides will be output from most models (not all have that many), with the first starting at 2 (some start at 1 or 4).","title":"Multi-scale Feature Maps (Feature Pyramid)"},{"location":"feature_extraction/#create-a-feature-map-extraction-model","text":"import torch import timm m = timm . create_model ( 'resnest26d' , features_only = True , pretrained = True ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) for x in o : print ( x . shape ) Output: torch.Size([2, 64, 112, 112]) torch.Size([2, 256, 56, 56]) torch.Size([2, 512, 28, 28]) torch.Size([2, 1024, 14, 14]) torch.Size([2, 2048, 7, 7])","title":"Create a feature map extraction model"},{"location":"feature_extraction/#query-the-feature-information","text":"After a feature backbone has been created, it can be queried to provide channel or resolution reduction information to the downstream heads without requiring static config or hardcoded constants. The .feature_info attribute is a class encapsulating the information about the feature extraction points. import torch import timm m = timm . create_model ( 'regnety_032' , features_only = True , pretrained = True ) print ( f 'Feature channels: { m . feature_info . channels () } ' ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) for x in o : print ( x . shape ) Output: Feature channels: [32, 72, 216, 576, 1512] torch.Size([2, 32, 112, 112]) torch.Size([2, 72, 56, 56]) torch.Size([2, 216, 28, 28]) torch.Size([2, 576, 14, 14]) torch.Size([2, 1512, 7, 7])","title":"Query the feature information"},{"location":"feature_extraction/#select-specific-feature-levels-or-limit-the-stride","text":"There are two additional creation arguments impacting the output features. out_indices selects which indices to output output_stride limits the feature output stride of the network (also works in classification mode BTW) out_indices is supported by all models, but not all models have the same index to feature stride mapping. Look at the code or check feature_info to compare. The out indices generally correspond to the C(i+1)th feature level (a 2^(i+1) reduction). For most models, index 0 is the stride 2 features, and index 4 is stride 32. output_stride is achieved by converting layers to use dilated convolutions. Doing so is not always straightforward, some networks only support output_stride=32 . import torch import timm m = timm . create_model ( 'ecaresnet101d' , features_only = True , output_stride = 8 , out_indices = ( 2 , 4 ), pretrained = True ) print ( f 'Feature channels: { m . feature_info . channels () } ' ) print ( f 'Feature reduction: { m . feature_info . reduction () } ' ) o = m ( torch . randn ( 2 , 3 , 320 , 320 )) for x in o : print ( x . shape ) Output: Feature channels: [512, 2048] Feature reduction: [8, 8] torch.Size([2, 512, 40, 40]) torch.Size([2, 2048, 40, 40])","title":"Select specific feature levels or limit the stride"},{"location":"models/","text":"Model Summaries The model architectures included come from a wide variety of sources. Sources, including papers, original impl (\"reference code\") that I rewrote / adapted, and PyTorch impl that I leveraged directly (\"code\") are listed below. Most included models have pretrained weights. The weights are either: from their original sources ported by myself from their original impl in a different framework (e.g. Tensorflow models) trained from scratch using the included training script The validation results for the pretrained weights are here A more exciting view (with pretty pictures) of the models within timm can be found at paperswithcode . Big Transfer ResNetV2 (BiT) [ resnetv2.py ] Paper: Big Transfer (BiT): General Visual Representation Learning - https://arxiv.org/abs/1912.11370 Reference code: https://github.com/google-research/big_transfer Cross-Stage Partial Networks [ cspnet.py ] Paper: CSPNet: A New Backbone that can Enhance Learning Capability of CNN - https://arxiv.org/abs/1911.11929 Reference impl: https://github.com/WongKinYiu/CrossStagePartialNetworks DenseNet [ densenet.py ] Paper: Densely Connected Convolutional Networks - https://arxiv.org/abs/1608.06993 Code: https://github.com/pytorch/vision/tree/master/torchvision/models DLA [ dla.py ] Paper: https://arxiv.org/abs/1707.06484 Code: https://github.com/ucbdrive/dla Dual-Path Networks [ dpn.py ] Paper: Dual Path Networks - https://arxiv.org/abs/1707.01629 My PyTorch code: https://github.com/rwightman/pytorch-dpn-pretrained Reference code: https://github.com/cypw/DPNs GPU-Efficient Networks [ byobnet.py ] Paper: Neural Architecture Design for GPU-Efficient Networks - https://arxiv.org/abs/2006.14090 Reference code: https://github.com/idstcv/GPU-Efficient-Networks HRNet [ hrnet.py ] Paper: Deep High-Resolution Representation Learning for Visual Recognition - https://arxiv.org/abs/1908.07919 Code: https://github.com/HRNet/HRNet-Image-Classification Inception-V3 [ inception_v3.py ] Paper: Rethinking the Inception Architecture for Computer Vision - https://arxiv.org/abs/1512.00567 Code: https://github.com/pytorch/vision/tree/master/torchvision/models Inception-V4 [ inception_v4.py ] Paper: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning - https://arxiv.org/abs/1602.07261 Code: https://github.com/Cadene/pretrained-models.pytorch Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets Inception-ResNet-V2 [ inception_resnet_v2.py ] Paper: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning - https://arxiv.org/abs/1602.07261 Code: https://github.com/Cadene/pretrained-models.pytorch Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets NASNet-A [ nasnet.py ] Papers: Learning Transferable Architectures for Scalable Image Recognition - https://arxiv.org/abs/1707.07012 Code: https://github.com/Cadene/pretrained-models.pytorch Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet PNasNet-5 [ pnasnet.py ] Papers: Progressive Neural Architecture Search - https://arxiv.org/abs/1712.00559 Code: https://github.com/Cadene/pretrained-models.pytorch Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet EfficientNet [ efficientnet.py ] Papers: EfficientNet NoisyStudent (B0-B7, L2) - https://arxiv.org/abs/1911.04252 EfficientNet AdvProp (B0-B8) - https://arxiv.org/abs/1911.09665 EfficientNet (B0-B7) - https://arxiv.org/abs/1905.11946 EfficientNet-EdgeTPU (S, M, L) - https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html MixNet - https://arxiv.org/abs/1907.09595 MNASNet B1, A1 (Squeeze-Excite), and Small - https://arxiv.org/abs/1807.11626 MobileNet-V2 - https://arxiv.org/abs/1801.04381 FBNet-C - https://arxiv.org/abs/1812.03443 Single-Path NAS - https://arxiv.org/abs/1904.02877 My PyTorch code: https://github.com/rwightman/gen-efficientnet-pytorch Reference code: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet MobileNet-V3 [ mobilenetv3.py ] Paper: Searching for MobileNetV3 - https://arxiv.org/abs/1905.02244 Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet RegNet [ regnet.py ] Paper: Designing Network Design Spaces - https://arxiv.org/abs/2003.13678 Reference code: https://github.com/facebookresearch/pycls/blob/master/pycls/models/regnet.py RepVGG [ byobnet.py ] Paper: Making VGG-style ConvNets Great Again - https://arxiv.org/abs/2101.03697 Reference code: https://github.com/DingXiaoH/RepVGG ResNet, ResNeXt [ resnet.py ] ResNet (V1B) Paper: Deep Residual Learning for Image Recognition - https://arxiv.org/abs/1512.03385 Code: https://github.com/pytorch/vision/tree/master/torchvision/models ResNeXt Paper: Aggregated Residual Transformations for Deep Neural Networks - https://arxiv.org/abs/1611.05431 Code: https://github.com/pytorch/vision/tree/master/torchvision/models 'Bag of Tricks' / Gluon C, D, E, S ResNet variants Paper: Bag of Tricks for Image Classification with CNNs - https://arxiv.org/abs/1812.01187 Code: https://github.com/dmlc/gluon-cv/blob/master/gluoncv/model_zoo/resnetv1b.py Instagram pretrained / ImageNet tuned ResNeXt101 Paper: Exploring the Limits of Weakly Supervised Pretraining - https://arxiv.org/abs/1805.00932 Weights: https://pytorch.org/hub/facebookresearch_WSL-Images_resnext (NOTE: CC BY-NC 4.0 License, NOT commercial friendly) Semi-supervised (SSL) / Semi-weakly Supervised (SWSL) ResNet and ResNeXts Paper: Billion-scale semi-supervised learning for image classification - https://arxiv.org/abs/1905.00546 Weights: https://github.com/facebookresearch/semi-supervised-ImageNet1K-models (NOTE: CC BY-NC 4.0 License, NOT commercial friendly) Squeeze-and-Excitation Networks Paper: Squeeze-and-Excitation Networks - https://arxiv.org/abs/1709.01507 Code: Added to ResNet base, this is current version going forward, old senet.py is being deprecated ECAResNet (ECA-Net) Paper: ECA-Net: Efficient Channel Attention for Deep CNN - https://arxiv.org/abs/1910.03151v4 Code: Added to ResNet base, ECA module contributed by @VRandme, reference https://github.com/BangguWu/ECANet Res2Net [ res2net.py ] Paper: Res2Net: A New Multi-scale Backbone Architecture - https://arxiv.org/abs/1904.01169 Code: https://github.com/gasvn/Res2Net ResNeSt [ resnest.py ] Paper: ResNeSt: Split-Attention Networks - https://arxiv.org/abs/2004.08955 Code: https://github.com/zhanghang1989/ResNeSt ReXNet [ rexnet.py ] Paper: ReXNet: Diminishing Representational Bottleneck on CNN - https://arxiv.org/abs/2007.00992 Code: https://github.com/clovaai/rexnet Selective-Kernel Networks [ sknet.py ] Paper: Selective-Kernel Networks - https://arxiv.org/abs/1903.06586 Code: https://github.com/implus/SKNet , https://github.com/clovaai/assembled-cnn SelecSLS [ selecsls.py ] Paper: XNect: Real-time Multi-Person 3D Motion Capture with a Single RGB Camera - https://arxiv.org/abs/1907.00837 Code: https://github.com/mehtadushy/SelecSLS-Pytorch Squeeze-and-Excitation Networks [ senet.py ] NOTE: I am deprecating this version of the networks, the new ones are part of resnet.py Paper: Squeeze-and-Excitation Networks - https://arxiv.org/abs/1709.01507 Code: https://github.com/Cadene/pretrained-models.pytorch TResNet [ tresnet.py ] Paper: TResNet: High Performance GPU-Dedicated Architecture - https://arxiv.org/abs/2003.13630 Code: https://github.com/mrT23/TResNet VGG [ vgg.py ] Paper: Very Deep Convolutional Networks For Large-Scale Image Recognition - https://arxiv.org/pdf/1409.1556.pdf Reference code: https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py Vision Transformer [ vision_transformer.py ] Paper: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - https://arxiv.org/abs/2010.11929 Reference code and pretrained weights: https://github.com/google-research/vision_transformer VovNet V2 and V1 [ vovnet.py ] Paper: CenterMask : Real-Time Anchor-Free Instance Segmentation - https://arxiv.org/abs/1911.06667 Reference code: https://github.com/youngwanLEE/vovnet-detectron2 Xception [ xception.py ] Paper: Xception: Deep Learning with Depthwise Separable Convolutions - https://arxiv.org/abs/1610.02357 Code: https://github.com/Cadene/pretrained-models.pytorch Xception (Modified Aligned, Gluon) [ gluon_xception.py ] Paper: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation - https://arxiv.org/abs/1802.02611 Reference code: https://github.com/dmlc/gluon-cv/tree/master/gluoncv/model_zoo , https://github.com/jfzhang95/pytorch-deeplab-xception/ Xception (Modified Aligned, TF) [ aligned_xception.py ] Paper: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation - https://arxiv.org/abs/1802.02611 Reference code: https://github.com/tensorflow/models/tree/master/research/deeplab","title":"Model Summaries"},{"location":"models/#model-summaries","text":"The model architectures included come from a wide variety of sources. Sources, including papers, original impl (\"reference code\") that I rewrote / adapted, and PyTorch impl that I leveraged directly (\"code\") are listed below. Most included models have pretrained weights. The weights are either: from their original sources ported by myself from their original impl in a different framework (e.g. Tensorflow models) trained from scratch using the included training script The validation results for the pretrained weights are here A more exciting view (with pretty pictures) of the models within timm can be found at paperswithcode .","title":"Model Summaries"},{"location":"models/#big-transfer-resnetv2-bit-resnetv2py","text":"Paper: Big Transfer (BiT): General Visual Representation Learning - https://arxiv.org/abs/1912.11370 Reference code: https://github.com/google-research/big_transfer","title":"Big Transfer ResNetV2 (BiT) [resnetv2.py]"},{"location":"models/#cross-stage-partial-networks-cspnetpy","text":"Paper: CSPNet: A New Backbone that can Enhance Learning Capability of CNN - https://arxiv.org/abs/1911.11929 Reference impl: https://github.com/WongKinYiu/CrossStagePartialNetworks","title":"Cross-Stage Partial Networks [cspnet.py]"},{"location":"models/#densenet-densenetpy","text":"Paper: Densely Connected Convolutional Networks - https://arxiv.org/abs/1608.06993 Code: https://github.com/pytorch/vision/tree/master/torchvision/models","title":"DenseNet [densenet.py]"},{"location":"models/#dla-dlapy","text":"Paper: https://arxiv.org/abs/1707.06484 Code: https://github.com/ucbdrive/dla","title":"DLA [dla.py]"},{"location":"models/#dual-path-networks-dpnpy","text":"Paper: Dual Path Networks - https://arxiv.org/abs/1707.01629 My PyTorch code: https://github.com/rwightman/pytorch-dpn-pretrained Reference code: https://github.com/cypw/DPNs","title":"Dual-Path Networks [dpn.py]"},{"location":"models/#gpu-efficient-networks-byobnetpy","text":"Paper: Neural Architecture Design for GPU-Efficient Networks - https://arxiv.org/abs/2006.14090 Reference code: https://github.com/idstcv/GPU-Efficient-Networks","title":"GPU-Efficient Networks [byobnet.py]"},{"location":"models/#hrnet-hrnetpy","text":"Paper: Deep High-Resolution Representation Learning for Visual Recognition - https://arxiv.org/abs/1908.07919 Code: https://github.com/HRNet/HRNet-Image-Classification","title":"HRNet [hrnet.py]"},{"location":"models/#inception-v3-inception_v3py","text":"Paper: Rethinking the Inception Architecture for Computer Vision - https://arxiv.org/abs/1512.00567 Code: https://github.com/pytorch/vision/tree/master/torchvision/models","title":"Inception-V3 [inception_v3.py]"},{"location":"models/#inception-v4-inception_v4py","text":"Paper: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning - https://arxiv.org/abs/1602.07261 Code: https://github.com/Cadene/pretrained-models.pytorch Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets","title":"Inception-V4 [inception_v4.py]"},{"location":"models/#inception-resnet-v2-inception_resnet_v2py","text":"Paper: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning - https://arxiv.org/abs/1602.07261 Code: https://github.com/Cadene/pretrained-models.pytorch Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets","title":"Inception-ResNet-V2 [inception_resnet_v2.py]"},{"location":"models/#nasnet-a-nasnetpy","text":"Papers: Learning Transferable Architectures for Scalable Image Recognition - https://arxiv.org/abs/1707.07012 Code: https://github.com/Cadene/pretrained-models.pytorch Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet","title":"NASNet-A [nasnet.py]"},{"location":"models/#pnasnet-5-pnasnetpy","text":"Papers: Progressive Neural Architecture Search - https://arxiv.org/abs/1712.00559 Code: https://github.com/Cadene/pretrained-models.pytorch Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet","title":"PNasNet-5 [pnasnet.py]"},{"location":"models/#efficientnet-efficientnetpy","text":"Papers: EfficientNet NoisyStudent (B0-B7, L2) - https://arxiv.org/abs/1911.04252 EfficientNet AdvProp (B0-B8) - https://arxiv.org/abs/1911.09665 EfficientNet (B0-B7) - https://arxiv.org/abs/1905.11946 EfficientNet-EdgeTPU (S, M, L) - https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html MixNet - https://arxiv.org/abs/1907.09595 MNASNet B1, A1 (Squeeze-Excite), and Small - https://arxiv.org/abs/1807.11626 MobileNet-V2 - https://arxiv.org/abs/1801.04381 FBNet-C - https://arxiv.org/abs/1812.03443 Single-Path NAS - https://arxiv.org/abs/1904.02877 My PyTorch code: https://github.com/rwightman/gen-efficientnet-pytorch Reference code: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet","title":"EfficientNet [efficientnet.py]"},{"location":"models/#mobilenet-v3-mobilenetv3py","text":"Paper: Searching for MobileNetV3 - https://arxiv.org/abs/1905.02244 Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet","title":"MobileNet-V3 [mobilenetv3.py]"},{"location":"models/#regnet-regnetpy","text":"Paper: Designing Network Design Spaces - https://arxiv.org/abs/2003.13678 Reference code: https://github.com/facebookresearch/pycls/blob/master/pycls/models/regnet.py","title":"RegNet [regnet.py]"},{"location":"models/#repvgg-byobnetpy","text":"Paper: Making VGG-style ConvNets Great Again - https://arxiv.org/abs/2101.03697 Reference code: https://github.com/DingXiaoH/RepVGG","title":"RepVGG [byobnet.py]"},{"location":"models/#resnet-resnext-resnetpy","text":"ResNet (V1B) Paper: Deep Residual Learning for Image Recognition - https://arxiv.org/abs/1512.03385 Code: https://github.com/pytorch/vision/tree/master/torchvision/models ResNeXt Paper: Aggregated Residual Transformations for Deep Neural Networks - https://arxiv.org/abs/1611.05431 Code: https://github.com/pytorch/vision/tree/master/torchvision/models 'Bag of Tricks' / Gluon C, D, E, S ResNet variants Paper: Bag of Tricks for Image Classification with CNNs - https://arxiv.org/abs/1812.01187 Code: https://github.com/dmlc/gluon-cv/blob/master/gluoncv/model_zoo/resnetv1b.py Instagram pretrained / ImageNet tuned ResNeXt101 Paper: Exploring the Limits of Weakly Supervised Pretraining - https://arxiv.org/abs/1805.00932 Weights: https://pytorch.org/hub/facebookresearch_WSL-Images_resnext (NOTE: CC BY-NC 4.0 License, NOT commercial friendly) Semi-supervised (SSL) / Semi-weakly Supervised (SWSL) ResNet and ResNeXts Paper: Billion-scale semi-supervised learning for image classification - https://arxiv.org/abs/1905.00546 Weights: https://github.com/facebookresearch/semi-supervised-ImageNet1K-models (NOTE: CC BY-NC 4.0 License, NOT commercial friendly) Squeeze-and-Excitation Networks Paper: Squeeze-and-Excitation Networks - https://arxiv.org/abs/1709.01507 Code: Added to ResNet base, this is current version going forward, old senet.py is being deprecated ECAResNet (ECA-Net) Paper: ECA-Net: Efficient Channel Attention for Deep CNN - https://arxiv.org/abs/1910.03151v4 Code: Added to ResNet base, ECA module contributed by @VRandme, reference https://github.com/BangguWu/ECANet","title":"ResNet, ResNeXt [resnet.py]"},{"location":"models/#res2net-res2netpy","text":"Paper: Res2Net: A New Multi-scale Backbone Architecture - https://arxiv.org/abs/1904.01169 Code: https://github.com/gasvn/Res2Net","title":"Res2Net [res2net.py]"},{"location":"models/#resnest-resnestpy","text":"Paper: ResNeSt: Split-Attention Networks - https://arxiv.org/abs/2004.08955 Code: https://github.com/zhanghang1989/ResNeSt","title":"ResNeSt [resnest.py]"},{"location":"models/#rexnet-rexnetpy","text":"Paper: ReXNet: Diminishing Representational Bottleneck on CNN - https://arxiv.org/abs/2007.00992 Code: https://github.com/clovaai/rexnet","title":"ReXNet [rexnet.py]"},{"location":"models/#selective-kernel-networks-sknetpy","text":"Paper: Selective-Kernel Networks - https://arxiv.org/abs/1903.06586 Code: https://github.com/implus/SKNet , https://github.com/clovaai/assembled-cnn","title":"Selective-Kernel Networks [sknet.py]"},{"location":"models/#selecsls-selecslspy","text":"Paper: XNect: Real-time Multi-Person 3D Motion Capture with a Single RGB Camera - https://arxiv.org/abs/1907.00837 Code: https://github.com/mehtadushy/SelecSLS-Pytorch","title":"SelecSLS [selecsls.py]"},{"location":"models/#squeeze-and-excitation-networks-senetpy","text":"NOTE: I am deprecating this version of the networks, the new ones are part of resnet.py Paper: Squeeze-and-Excitation Networks - https://arxiv.org/abs/1709.01507 Code: https://github.com/Cadene/pretrained-models.pytorch","title":"Squeeze-and-Excitation Networks [senet.py]"},{"location":"models/#tresnet-tresnetpy","text":"Paper: TResNet: High Performance GPU-Dedicated Architecture - https://arxiv.org/abs/2003.13630 Code: https://github.com/mrT23/TResNet","title":"TResNet [tresnet.py]"},{"location":"models/#vgg-vggpy","text":"Paper: Very Deep Convolutional Networks For Large-Scale Image Recognition - https://arxiv.org/pdf/1409.1556.pdf Reference code: https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py","title":"VGG [vgg.py]"},{"location":"models/#vision-transformer-vision_transformerpy","text":"Paper: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - https://arxiv.org/abs/2010.11929 Reference code and pretrained weights: https://github.com/google-research/vision_transformer","title":"Vision Transformer [vision_transformer.py]"},{"location":"models/#vovnet-v2-and-v1-vovnetpy","text":"Paper: CenterMask : Real-Time Anchor-Free Instance Segmentation - https://arxiv.org/abs/1911.06667 Reference code: https://github.com/youngwanLEE/vovnet-detectron2","title":"VovNet V2 and V1 [vovnet.py]"},{"location":"models/#xception-xceptionpy","text":"Paper: Xception: Deep Learning with Depthwise Separable Convolutions - https://arxiv.org/abs/1610.02357 Code: https://github.com/Cadene/pretrained-models.pytorch","title":"Xception [xception.py]"},{"location":"models/#xception-modified-aligned-gluon-gluon_xceptionpy","text":"Paper: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation - https://arxiv.org/abs/1802.02611 Reference code: https://github.com/dmlc/gluon-cv/tree/master/gluoncv/model_zoo , https://github.com/jfzhang95/pytorch-deeplab-xception/","title":"Xception (Modified Aligned, Gluon) [gluon_xception.py]"},{"location":"models/#xception-modified-aligned-tf-aligned_xceptionpy","text":"Paper: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation - https://arxiv.org/abs/1802.02611 Reference code: https://github.com/tensorflow/models/tree/master/research/deeplab","title":"Xception (Modified Aligned, TF) [aligned_xception.py]"},{"location":"results/","text":"Results CSV files containing an ImageNet-1K and out-of-distribution (OOD) test set validation results for all models with pretrained weights is located in the repository results folder . Self-trained Weights The table below includes ImageNet-1k validation results of model weights that I've trained myself. It is not updated as frequently as the csv results outputs linked above. Model Acc@1 (Err) Acc@5 (Err) Param # (M) Interpolation Image Size efficientnet_b3a 82.242 (17.758) 96.114 (3.886) 12.23 bicubic 320 (1.0 crop) efficientnet_b3 82.076 (17.924) 96.020 (3.980) 12.23 bicubic 300 regnet_32 82.002 (17.998) 95.906 (4.094) 19.44 bicubic 224 skresnext50d_32x4d 81.278 (18.722) 95.366 (4.634) 27.5 bicubic 288 (1.0 crop) seresnext50d_32x4d 81.266 (18.734) 95.620 (4.380) 27.6 bicubic 224 efficientnet_b2a 80.608 (19.392) 95.310 (4.690) 9.11 bicubic 288 (1.0 crop) resnet50d 80.530 (19.470) 95.160 (4.840) 25.6 bicubic 224 mixnet_xl 80.478 (19.522) 94.932 (5.068) 11.90 bicubic 224 efficientnet_b2 80.402 (19.598) 95.076 (4.924) 9.11 bicubic 260 seresnet50 80.274 (19.726) 95.070 (4.930) 28.1 bicubic 224 skresnext50d_32x4d 80.156 (19.844) 94.642 (5.358) 27.5 bicubic 224 cspdarknet53 80.058 (19.942) 95.084 (4.916) 27.6 bicubic 256 cspresnext50 80.040 (19.960) 94.944 (5.056) 20.6 bicubic 224 resnext50_32x4d 79.762 (20.238) 94.600 (5.400) 25 bicubic 224 resnext50d_32x4d 79.674 (20.326) 94.868 (5.132) 25.1 bicubic 224 cspresnet50 79.574 (20.426) 94.712 (5.288) 21.6 bicubic 256 ese_vovnet39b 79.320 (20.680) 94.710 (5.290) 24.6 bicubic 224 resnetblur50 79.290 (20.710) 94.632 (5.368) 25.6 bicubic 224 dpn68b 79.216 (20.784) 94.414 (5.586) 12.6 bicubic 224 resnet50 79.038 (20.962) 94.390 (5.610) 25.6 bicubic 224 mixnet_l 78.976 (21.024 94.184 (5.816) 7.33 bicubic 224 efficientnet_b1 78.692 (21.308) 94.086 (5.914) 7.79 bicubic 240 efficientnet_es 78.066 (21.934) 93.926 (6.074) 5.44 bicubic 224 seresnext26t_32x4d 77.998 (22.002) 93.708 (6.292) 16.8 bicubic 224 seresnext26tn_32x4d 77.986 (22.014) 93.746 (6.254) 16.8 bicubic 224 efficientnet_b0 77.698 (22.302) 93.532 (6.468) 5.29 bicubic 224 seresnext26d_32x4d 77.602 (22.398) 93.608 (6.392) 16.8 bicubic 224 mobilenetv2_120d 77.294 (22.706 93.502 (6.498) 5.8 bicubic 224 mixnet_m 77.256 (22.744) 93.418 (6.582) 5.01 bicubic 224 resnet34d 77.116 (22.884) 93.382 (6.618) 21.8 bicubic 224 seresnext26_32x4d 77.104 (22.896) 93.316 (6.684) 16.8 bicubic 224 skresnet34 76.912 (23.088) 93.322 (6.678) 22.2 bicubic 224 ese_vovnet19b_dw 76.798 (23.202) 93.268 (6.732) 6.5 bicubic 224 resnet26d 76.68 (23.32) 93.166 (6.834) 16 bicubic 224 densenetblur121d 76.576 (23.424) 93.190 (6.810) 8.0 bicubic 224 mobilenetv2_140 76.524 (23.476) 92.990 (7.010) 6.1 bicubic 224 mixnet_s 75.988 (24.012) 92.794 (7.206) 4.13 bicubic 224 mobilenetv3_large_100 75.766 (24.234) 92.542 (7.458) 5.5 bicubic 224 mobilenetv3_rw 75.634 (24.366) 92.708 (7.292) 5.5 bicubic 224 mnasnet_a1 75.448 (24.552) 92.604 (7.396) 3.89 bicubic 224 resnet26 75.292 (24.708) 92.57 (7.43) 16 bicubic 224 fbnetc_100 75.124 (24.876) 92.386 (7.614) 5.6 bilinear 224 resnet34 75.110 (24.890) 92.284 (7.716) 22 bilinear 224 mobilenetv2_110d 75.052 (24.948) 92.180 (7.820) 4.5 bicubic 224 seresnet34 74.808 (25.192) 92.124 (7.876) 22 bilinear 224 mnasnet_b1 74.658 (25.342) 92.114 (7.886) 4.38 bicubic 224 spnasnet_100 74.084 (25.916) 91.818 (8.182) 4.42 bilinear 224 skresnet18 73.038 (26.962) 91.168 (8.832) 11.9 bicubic 224 mobilenetv2_100 72.978 (27.022) 91.016 (8.984) 3.5 bicubic 224 resnet18d 72.260 (27.740) 90.696 (9.304) 11.7 bicubic 224 seresnet18 71.742 (28.258) 90.334 (9.666) 11.8 bicubic 224 Ported and Other Weights For weights ported from other deep learning frameworks (Tensorflow, MXNet GluonCV) or copied from other PyTorch sources, please see the full results tables for ImageNet and various OOD test sets at in the results tables . Model code .py files contain links to original sources of models and weights.","title":"Results"},{"location":"results/#results","text":"CSV files containing an ImageNet-1K and out-of-distribution (OOD) test set validation results for all models with pretrained weights is located in the repository results folder .","title":"Results"},{"location":"results/#self-trained-weights","text":"The table below includes ImageNet-1k validation results of model weights that I've trained myself. It is not updated as frequently as the csv results outputs linked above. Model Acc@1 (Err) Acc@5 (Err) Param # (M) Interpolation Image Size efficientnet_b3a 82.242 (17.758) 96.114 (3.886) 12.23 bicubic 320 (1.0 crop) efficientnet_b3 82.076 (17.924) 96.020 (3.980) 12.23 bicubic 300 regnet_32 82.002 (17.998) 95.906 (4.094) 19.44 bicubic 224 skresnext50d_32x4d 81.278 (18.722) 95.366 (4.634) 27.5 bicubic 288 (1.0 crop) seresnext50d_32x4d 81.266 (18.734) 95.620 (4.380) 27.6 bicubic 224 efficientnet_b2a 80.608 (19.392) 95.310 (4.690) 9.11 bicubic 288 (1.0 crop) resnet50d 80.530 (19.470) 95.160 (4.840) 25.6 bicubic 224 mixnet_xl 80.478 (19.522) 94.932 (5.068) 11.90 bicubic 224 efficientnet_b2 80.402 (19.598) 95.076 (4.924) 9.11 bicubic 260 seresnet50 80.274 (19.726) 95.070 (4.930) 28.1 bicubic 224 skresnext50d_32x4d 80.156 (19.844) 94.642 (5.358) 27.5 bicubic 224 cspdarknet53 80.058 (19.942) 95.084 (4.916) 27.6 bicubic 256 cspresnext50 80.040 (19.960) 94.944 (5.056) 20.6 bicubic 224 resnext50_32x4d 79.762 (20.238) 94.600 (5.400) 25 bicubic 224 resnext50d_32x4d 79.674 (20.326) 94.868 (5.132) 25.1 bicubic 224 cspresnet50 79.574 (20.426) 94.712 (5.288) 21.6 bicubic 256 ese_vovnet39b 79.320 (20.680) 94.710 (5.290) 24.6 bicubic 224 resnetblur50 79.290 (20.710) 94.632 (5.368) 25.6 bicubic 224 dpn68b 79.216 (20.784) 94.414 (5.586) 12.6 bicubic 224 resnet50 79.038 (20.962) 94.390 (5.610) 25.6 bicubic 224 mixnet_l 78.976 (21.024 94.184 (5.816) 7.33 bicubic 224 efficientnet_b1 78.692 (21.308) 94.086 (5.914) 7.79 bicubic 240 efficientnet_es 78.066 (21.934) 93.926 (6.074) 5.44 bicubic 224 seresnext26t_32x4d 77.998 (22.002) 93.708 (6.292) 16.8 bicubic 224 seresnext26tn_32x4d 77.986 (22.014) 93.746 (6.254) 16.8 bicubic 224 efficientnet_b0 77.698 (22.302) 93.532 (6.468) 5.29 bicubic 224 seresnext26d_32x4d 77.602 (22.398) 93.608 (6.392) 16.8 bicubic 224 mobilenetv2_120d 77.294 (22.706 93.502 (6.498) 5.8 bicubic 224 mixnet_m 77.256 (22.744) 93.418 (6.582) 5.01 bicubic 224 resnet34d 77.116 (22.884) 93.382 (6.618) 21.8 bicubic 224 seresnext26_32x4d 77.104 (22.896) 93.316 (6.684) 16.8 bicubic 224 skresnet34 76.912 (23.088) 93.322 (6.678) 22.2 bicubic 224 ese_vovnet19b_dw 76.798 (23.202) 93.268 (6.732) 6.5 bicubic 224 resnet26d 76.68 (23.32) 93.166 (6.834) 16 bicubic 224 densenetblur121d 76.576 (23.424) 93.190 (6.810) 8.0 bicubic 224 mobilenetv2_140 76.524 (23.476) 92.990 (7.010) 6.1 bicubic 224 mixnet_s 75.988 (24.012) 92.794 (7.206) 4.13 bicubic 224 mobilenetv3_large_100 75.766 (24.234) 92.542 (7.458) 5.5 bicubic 224 mobilenetv3_rw 75.634 (24.366) 92.708 (7.292) 5.5 bicubic 224 mnasnet_a1 75.448 (24.552) 92.604 (7.396) 3.89 bicubic 224 resnet26 75.292 (24.708) 92.57 (7.43) 16 bicubic 224 fbnetc_100 75.124 (24.876) 92.386 (7.614) 5.6 bilinear 224 resnet34 75.110 (24.890) 92.284 (7.716) 22 bilinear 224 mobilenetv2_110d 75.052 (24.948) 92.180 (7.820) 4.5 bicubic 224 seresnet34 74.808 (25.192) 92.124 (7.876) 22 bilinear 224 mnasnet_b1 74.658 (25.342) 92.114 (7.886) 4.38 bicubic 224 spnasnet_100 74.084 (25.916) 91.818 (8.182) 4.42 bilinear 224 skresnet18 73.038 (26.962) 91.168 (8.832) 11.9 bicubic 224 mobilenetv2_100 72.978 (27.022) 91.016 (8.984) 3.5 bicubic 224 resnet18d 72.260 (27.740) 90.696 (9.304) 11.7 bicubic 224 seresnet18 71.742 (28.258) 90.334 (9.666) 11.8 bicubic 224","title":"Self-trained Weights"},{"location":"results/#ported-and-other-weights","text":"For weights ported from other deep learning frameworks (Tensorflow, MXNet GluonCV) or copied from other PyTorch sources, please see the full results tables for ImageNet and various OOD test sets at in the results tables . Model code .py files contain links to original sources of models and weights.","title":"Ported and Other Weights"},{"location":"scripts/","text":"Scripts A train, validation, inference, and checkpoint cleaning script included in the github root folder. Scripts are not currently packaged in the pip release. The training and validation scripts evolved from early versions of the PyTorch Imagenet Examples . I have added significant functionality over time, including CUDA specific performance enhancements based on NVIDIA's APEX Examples . Training Script The variety of training args is large and not all combinations of options (or even options) have been fully tested. For the training dataset folder, specify the folder to the base that contains a train and validation folder. To train an SE-ResNet34 on ImageNet, locally distributed, 4 GPUs, one process per GPU w/ cosine schedule, random-erasing prob of 50% and per-pixel random value: ./distributed_train.sh 4 /data/imagenet --model seresnet34 --sched cosine --epochs 150 --warmup-epochs 5 --lr 0.4 --reprob 0.5 --remode pixel --batch-size 256 --amp -j 4 NOTE: It is recommended to use PyTorch 1.9+ w/ PyTorch native AMP and DDP instead of APEX AMP. --amp defaults to native AMP as of timm ver 0.4.3. --apex-amp will force use of APEX components if they are installed. Validation / Inference Scripts Validation and inference scripts are similar in usage. One outputs metrics on a validation set and the other outputs topk class ids in a csv. Specify the folder containing validation images, not the base as in training script. To validate with the model's pretrained weights (if they exist): python validate.py /imagenet/validation/ --model seresnext26_32x4d --pretrained To run inference from a checkpoint: python inference.py /imagenet/validation/ --model mobilenetv3_large_100 --checkpoint ./output/train/model_best.pth.tar","title":"Scripts"},{"location":"scripts/#scripts","text":"A train, validation, inference, and checkpoint cleaning script included in the github root folder. Scripts are not currently packaged in the pip release. The training and validation scripts evolved from early versions of the PyTorch Imagenet Examples . I have added significant functionality over time, including CUDA specific performance enhancements based on NVIDIA's APEX Examples .","title":"Scripts"},{"location":"scripts/#training-script","text":"The variety of training args is large and not all combinations of options (or even options) have been fully tested. For the training dataset folder, specify the folder to the base that contains a train and validation folder. To train an SE-ResNet34 on ImageNet, locally distributed, 4 GPUs, one process per GPU w/ cosine schedule, random-erasing prob of 50% and per-pixel random value: ./distributed_train.sh 4 /data/imagenet --model seresnet34 --sched cosine --epochs 150 --warmup-epochs 5 --lr 0.4 --reprob 0.5 --remode pixel --batch-size 256 --amp -j 4 NOTE: It is recommended to use PyTorch 1.9+ w/ PyTorch native AMP and DDP instead of APEX AMP. --amp defaults to native AMP as of timm ver 0.4.3. --apex-amp will force use of APEX components if they are installed.","title":"Training Script"},{"location":"scripts/#validation-inference-scripts","text":"Validation and inference scripts are similar in usage. One outputs metrics on a validation set and the other outputs topk class ids in a csv. Specify the folder containing validation images, not the base as in training script. To validate with the model's pretrained weights (if they exist): python validate.py /imagenet/validation/ --model seresnext26_32x4d --pretrained To run inference from a checkpoint: python inference.py /imagenet/validation/ --model mobilenetv3_large_100 --checkpoint ./output/train/model_best.pth.tar","title":"Validation / Inference Scripts"},{"location":"training_hparam_examples/","text":"Training Examples EfficientNet-B2 with RandAugment - 80.4 top-1, 95.1 top-5 These params are for dual Titan RTX cards with NVIDIA Apex installed: ./distributed_train.sh 2 /imagenet/ --model efficientnet_b2 -b 128 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .97 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.3 --drop-path 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .016 MixNet-XL with RandAugment - 80.5 top-1, 94.9 top-5 This params are for dual Titan RTX cards with NVIDIA Apex installed: ./distributed_train.sh 2 /imagenet/ --model mixnet_xl -b 128 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .969 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.3 --drop-path 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.3 --amp --lr .016 --dist-bn reduce SE-ResNeXt-26-D and SE-ResNeXt-26-T These hparams (or similar) work well for a wide range of ResNet architecture, generally a good idea to increase the epoch # as the model size increases... ie approx 180-200 for ResNe(X)t50, and 220+ for larger. Increase batch size and LR proportionally for better GPUs or with AMP enabled. These params were for 2 1080Ti cards: ./distributed_train.sh 2 /imagenet/ --model seresnext26t_32x4d --lr 0.1 --warmup-epochs 5 --epochs 160 --weight-decay 1e-4 --sched cosine --reprob 0.4 --remode pixel -b 112 EfficientNet-B3 with RandAugment - 81.5 top-1, 95.7 top-5 The training of this model started with the same command line as EfficientNet-B2 w/ RA above. After almost three weeks of training the process crashed. The results weren't looking amazing so I resumed the training several times with tweaks to a few params (increase RE prob, decrease rand-aug, increase ema-decay). Nothing looked great. I ended up averaging the best checkpoints from all restarts. The result is mediocre at default res/crop but oddly performs much better with a full image test crop of 1.0. EfficientNet-B0 with RandAugment - 77.7 top-1, 95.3 top-5 Michael Klachko achieved these results with the command line for B2 adapted for larger batch size, with the recommended B0 dropout rate of 0.2. ./distributed_train.sh 2 /imagenet/ --model efficientnet_b0 -b 384 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .97 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-path 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .048 ResNet50 with JSD loss and RandAugment (clean + 2x RA augs) - 79.04 top-1, 94.39 top-5 Trained on two older 1080Ti cards, this took a while. Only slightly, non statistically better ImageNet validation result than my first good AugMix training of 78.99. However, these weights are more robust on tests with ImageNetV2, ImageNet-Sketch, etc. Unlike my first AugMix runs, I've enabled SplitBatchNorm, disabled random erasing on the clean split, and cranked up random erasing prob on the 2 augmented paths. ./distributed_train.sh 2 /imagenet -b 64 --model resnet50 --sched cosine --epochs 200 --lr 0.05 --amp --remode pixel --reprob 0.6 --aug-splits 3 --aa rand-m9-mstd0.5-inc1 --resplit --split-bn --jsd --dist-bn reduce EfficientNet-ES (EdgeTPU-Small) with RandAugment - 78.066 top-1, 93.926 top-5 Trained by Andrew Lavin with 8 V100 cards. Model EMA was not used, final checkpoint is the average of 8 best checkpoints during training. ./distributed_train.sh 8 /imagenet --model efficientnet_es -b 128 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .97 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-path 0.2 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .064 MobileNetV3-Large-100 - 75.766 top-1, 92,542 top-5 ./distributed_train.sh 2 /imagenet/ --model mobilenetv3_large_100 -b 512 --sched step --epochs 600 --decay-epochs 2.4 --decay-rate .973 --opt rmsproptf --opt-eps .001 -j 7 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-path 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .064 --lr-noise 0.42 0.9 ResNeXt-50 32x4d w/ RandAugment - 79.762 top-1, 94.60 top-5 These params will also work well for SE-ResNeXt-50 and SK-ResNeXt-50 and likely 101. I used them for the SK-ResNeXt-50 32x4d that I trained with 2 GPU using a slightly higher LR per effective batch size (lr=0.18, b=192 per GPU). The cmd line below are tuned for 8 GPU training. ./distributed_train.sh 8 /imagenet --model resnext50_32x4d --lr 0.6 --warmup-epochs 5 --epochs 240 --weight-decay 1e-4 --sched cosine --reprob 0.4 --recount 3 --remode pixel --aa rand-m7-mstd0.5-inc1 -b 192 -j 6 --amp --dist-bn reduce","title":"Training Examples"},{"location":"training_hparam_examples/#training-examples","text":"","title":"Training Examples"},{"location":"training_hparam_examples/#efficientnet-b2-with-randaugment-804-top-1-951-top-5","text":"These params are for dual Titan RTX cards with NVIDIA Apex installed: ./distributed_train.sh 2 /imagenet/ --model efficientnet_b2 -b 128 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .97 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.3 --drop-path 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .016","title":"EfficientNet-B2 with RandAugment - 80.4 top-1, 95.1 top-5"},{"location":"training_hparam_examples/#mixnet-xl-with-randaugment-805-top-1-949-top-5","text":"This params are for dual Titan RTX cards with NVIDIA Apex installed: ./distributed_train.sh 2 /imagenet/ --model mixnet_xl -b 128 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .969 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.3 --drop-path 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.3 --amp --lr .016 --dist-bn reduce","title":"MixNet-XL with RandAugment - 80.5 top-1, 94.9 top-5"},{"location":"training_hparam_examples/#se-resnext-26-d-and-se-resnext-26-t","text":"These hparams (or similar) work well for a wide range of ResNet architecture, generally a good idea to increase the epoch # as the model size increases... ie approx 180-200 for ResNe(X)t50, and 220+ for larger. Increase batch size and LR proportionally for better GPUs or with AMP enabled. These params were for 2 1080Ti cards: ./distributed_train.sh 2 /imagenet/ --model seresnext26t_32x4d --lr 0.1 --warmup-epochs 5 --epochs 160 --weight-decay 1e-4 --sched cosine --reprob 0.4 --remode pixel -b 112","title":"SE-ResNeXt-26-D and SE-ResNeXt-26-T"},{"location":"training_hparam_examples/#efficientnet-b3-with-randaugment-815-top-1-957-top-5","text":"The training of this model started with the same command line as EfficientNet-B2 w/ RA above. After almost three weeks of training the process crashed. The results weren't looking amazing so I resumed the training several times with tweaks to a few params (increase RE prob, decrease rand-aug, increase ema-decay). Nothing looked great. I ended up averaging the best checkpoints from all restarts. The result is mediocre at default res/crop but oddly performs much better with a full image test crop of 1.0.","title":"EfficientNet-B3 with RandAugment - 81.5 top-1, 95.7 top-5"},{"location":"training_hparam_examples/#efficientnet-b0-with-randaugment-777-top-1-953-top-5","text":"Michael Klachko achieved these results with the command line for B2 adapted for larger batch size, with the recommended B0 dropout rate of 0.2. ./distributed_train.sh 2 /imagenet/ --model efficientnet_b0 -b 384 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .97 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-path 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .048","title":"EfficientNet-B0 with RandAugment - 77.7 top-1, 95.3 top-5"},{"location":"training_hparam_examples/#resnet50-with-jsd-loss-and-randaugment-clean-2x-ra-augs-7904-top-1-9439-top-5","text":"Trained on two older 1080Ti cards, this took a while. Only slightly, non statistically better ImageNet validation result than my first good AugMix training of 78.99. However, these weights are more robust on tests with ImageNetV2, ImageNet-Sketch, etc. Unlike my first AugMix runs, I've enabled SplitBatchNorm, disabled random erasing on the clean split, and cranked up random erasing prob on the 2 augmented paths. ./distributed_train.sh 2 /imagenet -b 64 --model resnet50 --sched cosine --epochs 200 --lr 0.05 --amp --remode pixel --reprob 0.6 --aug-splits 3 --aa rand-m9-mstd0.5-inc1 --resplit --split-bn --jsd --dist-bn reduce","title":"ResNet50 with JSD loss and RandAugment (clean + 2x RA augs) - 79.04 top-1, 94.39 top-5"},{"location":"training_hparam_examples/#efficientnet-es-edgetpu-small-with-randaugment-78066-top-1-93926-top-5","text":"Trained by Andrew Lavin with 8 V100 cards. Model EMA was not used, final checkpoint is the average of 8 best checkpoints during training. ./distributed_train.sh 8 /imagenet --model efficientnet_es -b 128 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .97 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-path 0.2 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .064","title":"EfficientNet-ES (EdgeTPU-Small) with RandAugment - 78.066 top-1, 93.926 top-5"},{"location":"training_hparam_examples/#mobilenetv3-large-100-75766-top-1-92542-top-5","text":"./distributed_train.sh 2 /imagenet/ --model mobilenetv3_large_100 -b 512 --sched step --epochs 600 --decay-epochs 2.4 --decay-rate .973 --opt rmsproptf --opt-eps .001 -j 7 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-path 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .064 --lr-noise 0.42 0.9","title":"MobileNetV3-Large-100 - 75.766 top-1, 92,542 top-5"},{"location":"training_hparam_examples/#resnext-50-32x4d-w-randaugment-79762-top-1-9460-top-5","text":"These params will also work well for SE-ResNeXt-50 and SK-ResNeXt-50 and likely 101. I used them for the SK-ResNeXt-50 32x4d that I trained with 2 GPU using a slightly higher LR per effective batch size (lr=0.18, b=192 per GPU). The cmd line below are tuned for 8 GPU training. ./distributed_train.sh 8 /imagenet --model resnext50_32x4d --lr 0.6 --warmup-epochs 5 --epochs 240 --weight-decay 1e-4 --sched cosine --reprob 0.4 --recount 3 --remode pixel --aa rand-m7-mstd0.5-inc1 -b 192 -j 6 --amp --dist-bn reduce","title":"ResNeXt-50 32x4d w/ RandAugment - 79.762 top-1, 94.60 top-5"},{"location":"models/adversarial-inception-v3/","text":"Adversarial Inception v3 Inception v3 is a convolutional neural network architecture from the Inception family that makes several improvements including using Label Smoothing , Factorized 7 x 7 convolutions, and the use of an auxiliary classifer to propagate label information lower down the network (along with the use of batch normalization for layers in the sidehead). The key building block is an Inception Module . This particular model was trained for study of adversarial examples (adversarial training). The weights from this model were ported from Tensorflow/Models . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'adv_inception_v3' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. adv_inception_v3 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'adv_inception_v3' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/abs-1804-00097 , author = {Alexey Kurakin and Ian J. Goodfellow and Samy Bengio and Yinpeng Dong and Fangzhou Liao and Ming Liang and Tianyu Pang and Jun Zhu and Xiaolin Hu and Cihang Xie and Jianyu Wang and Zhishuai Zhang and Zhou Ren and Alan L. Yuille and Sangxia Huang and Yao Zhao and Yuzhe Zhao and Zhonglin Han and Junjiajia Long and Yerkebulan Berdibekov and Takuya Akiba and Seiya Tokui and Motoki Abe} , title = {Adversarial Attacks and Defences Competition} , journal = {CoRR} , volume = {abs/1804.00097} , year = {2018} , url = {http://arxiv.org/abs/1804.00097} , archivePrefix = {arXiv} , eprint = {1804.00097} , timestamp = {Thu, 31 Oct 2019 16:31:22 +0100} , biburl = {https://dblp.org/rec/journals/corr/abs-1804-00097.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Adversarial Inception v3"},{"location":"models/adversarial-inception-v3/#adversarial-inception-v3","text":"Inception v3 is a convolutional neural network architecture from the Inception family that makes several improvements including using Label Smoothing , Factorized 7 x 7 convolutions, and the use of an auxiliary classifer to propagate label information lower down the network (along with the use of batch normalization for layers in the sidehead). The key building block is an Inception Module . This particular model was trained for study of adversarial examples (adversarial training). The weights from this model were ported from Tensorflow/Models .","title":"Adversarial Inception v3"},{"location":"models/adversarial-inception-v3/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'adv_inception_v3' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. adv_inception_v3 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/adversarial-inception-v3/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'adv_inception_v3' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/adversarial-inception-v3/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/adversarial-inception-v3/#citation","text":"@article { DBLP:journals/corr/abs-1804-00097 , author = {Alexey Kurakin and Ian J. Goodfellow and Samy Bengio and Yinpeng Dong and Fangzhou Liao and Ming Liang and Tianyu Pang and Jun Zhu and Xiaolin Hu and Cihang Xie and Jianyu Wang and Zhishuai Zhang and Zhou Ren and Alan L. Yuille and Sangxia Huang and Yao Zhao and Yuzhe Zhao and Zhonglin Han and Junjiajia Long and Yerkebulan Berdibekov and Takuya Akiba and Seiya Tokui and Motoki Abe} , title = {Adversarial Attacks and Defences Competition} , journal = {CoRR} , volume = {abs/1804.00097} , year = {2018} , url = {http://arxiv.org/abs/1804.00097} , archivePrefix = {arXiv} , eprint = {1804.00097} , timestamp = {Thu, 31 Oct 2019 16:31:22 +0100} , biburl = {https://dblp.org/rec/journals/corr/abs-1804-00097.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/advprop/","text":"AdvProp (EfficientNet) AdvProp is an adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to the method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. The weights from this model were ported from Tensorflow/TPU . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'tf_efficientnet_b0_ap' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tf_efficientnet_b0_ap . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tf_efficientnet_b0_ap' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { xie2020adversarial , title = {Adversarial Examples Improve Image Recognition} , author = {Cihang Xie and Mingxing Tan and Boqing Gong and Jiang Wang and Alan Yuille and Quoc V. Le} , year = {2020} , eprint = {1911.09665} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"AdvProp (EfficientNet)"},{"location":"models/advprop/#advprop-efficientnet","text":"AdvProp is an adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to the method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. The weights from this model were ported from Tensorflow/TPU .","title":"AdvProp (EfficientNet)"},{"location":"models/advprop/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'tf_efficientnet_b0_ap' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tf_efficientnet_b0_ap . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/advprop/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tf_efficientnet_b0_ap' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/advprop/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/advprop/#citation","text":"@misc { xie2020adversarial , title = {Adversarial Examples Improve Image Recognition} , author = {Cihang Xie and Mingxing Tan and Boqing Gong and Jiang Wang and Alan Yuille and Quoc V. Le} , year = {2020} , eprint = {1911.09665} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/big-transfer/","text":"Big Transfer (BiT) Big Transfer (BiT) is a type of pretraining recipe that pre-trains on a large supervised source dataset, and fine-tunes the weights on the target task. Models are trained on the JFT-300M dataset. The finetuned models contained in this collection are finetuned on ImageNet. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'resnetv2_101x1_bitm' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. resnetv2_101x1_bitm . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'resnetv2_101x1_bitm' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { kolesnikov2020big , title = {Big Transfer (BiT): General Visual Representation Learning} , author = {Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Joan Puigcerver and Jessica Yung and Sylvain Gelly and Neil Houlsby} , year = {2020} , eprint = {1912.11370} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Big Transfer (BiT)"},{"location":"models/big-transfer/#big-transfer-bit","text":"Big Transfer (BiT) is a type of pretraining recipe that pre-trains on a large supervised source dataset, and fine-tunes the weights on the target task. Models are trained on the JFT-300M dataset. The finetuned models contained in this collection are finetuned on ImageNet.","title":"Big Transfer (BiT)"},{"location":"models/big-transfer/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'resnetv2_101x1_bitm' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. resnetv2_101x1_bitm . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/big-transfer/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'resnetv2_101x1_bitm' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/big-transfer/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/big-transfer/#citation","text":"@misc { kolesnikov2020big , title = {Big Transfer (BiT): General Visual Representation Learning} , author = {Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Joan Puigcerver and Jessica Yung and Sylvain Gelly and Neil Houlsby} , year = {2020} , eprint = {1912.11370} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/csp-darknet/","text":"CSP-DarkNet CSPDarknet53 is a convolutional neural network and backbone for object detection that uses DarkNet-53 . It employs a CSPNet strategy to partition the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network. This CNN is used as the backbone for YOLOv4 . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'cspdarknet53' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. cspdarknet53 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'cspdarknet53' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { bochkovskiy2020yolov4 , title = {YOLOv4: Optimal Speed and Accuracy of Object Detection} , author = {Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao} , year = {2020} , eprint = {2004.10934} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"CSP-DarkNet"},{"location":"models/csp-darknet/#csp-darknet","text":"CSPDarknet53 is a convolutional neural network and backbone for object detection that uses DarkNet-53 . It employs a CSPNet strategy to partition the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network. This CNN is used as the backbone for YOLOv4 .","title":"CSP-DarkNet"},{"location":"models/csp-darknet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'cspdarknet53' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. cspdarknet53 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/csp-darknet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'cspdarknet53' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/csp-darknet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/csp-darknet/#citation","text":"@misc { bochkovskiy2020yolov4 , title = {YOLOv4: Optimal Speed and Accuracy of Object Detection} , author = {Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao} , year = {2020} , eprint = {2004.10934} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/csp-resnet/","text":"CSP-ResNet CSPResNet is a convolutional neural network where we apply the Cross Stage Partial Network (CSPNet) approach to ResNet . The CSPNet partitions the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'cspresnet50' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. cspresnet50 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'cspresnet50' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { wang2019cspnet , title = {CSPNet: A New Backbone that can Enhance Learning Capability of CNN} , author = {Chien-Yao Wang and Hong-Yuan Mark Liao and I-Hau Yeh and Yueh-Hua Wu and Ping-Yang Chen and Jun-Wei Hsieh} , year = {2019} , eprint = {1911.11929} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"CSP-ResNet"},{"location":"models/csp-resnet/#csp-resnet","text":"CSPResNet is a convolutional neural network where we apply the Cross Stage Partial Network (CSPNet) approach to ResNet . The CSPNet partitions the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network.","title":"CSP-ResNet"},{"location":"models/csp-resnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'cspresnet50' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. cspresnet50 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/csp-resnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'cspresnet50' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/csp-resnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/csp-resnet/#citation","text":"@misc { wang2019cspnet , title = {CSPNet: A New Backbone that can Enhance Learning Capability of CNN} , author = {Chien-Yao Wang and Hong-Yuan Mark Liao and I-Hau Yeh and Yueh-Hua Wu and Ping-Yang Chen and Jun-Wei Hsieh} , year = {2019} , eprint = {1911.11929} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/csp-resnext/","text":"CSP-ResNeXt CSPResNeXt is a convolutional neural network where we apply the Cross Stage Partial Network (CSPNet) approach to ResNeXt . The CSPNet partitions the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'cspresnext50' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. cspresnext50 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'cspresnext50' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { wang2019cspnet , title = {CSPNet: A New Backbone that can Enhance Learning Capability of CNN} , author = {Chien-Yao Wang and Hong-Yuan Mark Liao and I-Hau Yeh and Yueh-Hua Wu and Ping-Yang Chen and Jun-Wei Hsieh} , year = {2019} , eprint = {1911.11929} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"CSP-ResNeXt"},{"location":"models/csp-resnext/#csp-resnext","text":"CSPResNeXt is a convolutional neural network where we apply the Cross Stage Partial Network (CSPNet) approach to ResNeXt . The CSPNet partitions the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network.","title":"CSP-ResNeXt"},{"location":"models/csp-resnext/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'cspresnext50' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. cspresnext50 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/csp-resnext/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'cspresnext50' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/csp-resnext/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/csp-resnext/#citation","text":"@misc { wang2019cspnet , title = {CSPNet: A New Backbone that can Enhance Learning Capability of CNN} , author = {Chien-Yao Wang and Hong-Yuan Mark Liao and I-Hau Yeh and Yueh-Hua Wu and Ping-Yang Chen and Jun-Wei Hsieh} , year = {2019} , eprint = {1911.11929} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/densenet/","text":"DenseNet DenseNet is a type of convolutional neural network that utilises dense connections between layers, through Dense Blocks , where we connect all layers (with matching feature-map sizes) directly with each other. To preserve the feed-forward nature, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. The DenseNet Blur variant in this collection by Ross Wightman employs Blur Pooling How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'densenet121' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. densenet121 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'densenet121' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/HuangLW16a , author = {Gao Huang and Zhuang Liu and Kilian Q. Weinberger} , title = {Densely Connected Convolutional Networks} , journal = {CoRR} , volume = {abs/1608.06993} , year = {2016} , url = {http://arxiv.org/abs/1608.06993} , archivePrefix = {arXiv} , eprint = {1608.06993} , timestamp = {Mon, 10 Sep 2018 15:49:32 +0200} , biburl = {https://dblp.org/rec/journals/corr/HuangLW16a.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } @misc{rw2019timm, author = {Ross Wightman}, title = {PyTorch Image Models}, year = {2019}, publisher = {GitHub}, journal = {GitHub repository}, doi = {10.5281/zenodo.4414861}, howpublished = {\\url{https://github.com/rwightman/pytorch-image-models}} }","title":"DenseNet"},{"location":"models/densenet/#densenet","text":"DenseNet is a type of convolutional neural network that utilises dense connections between layers, through Dense Blocks , where we connect all layers (with matching feature-map sizes) directly with each other. To preserve the feed-forward nature, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. The DenseNet Blur variant in this collection by Ross Wightman employs Blur Pooling","title":"DenseNet"},{"location":"models/densenet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'densenet121' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. densenet121 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/densenet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'densenet121' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/densenet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/densenet/#citation","text":"@article { DBLP:journals/corr/HuangLW16a , author = {Gao Huang and Zhuang Liu and Kilian Q. Weinberger} , title = {Densely Connected Convolutional Networks} , journal = {CoRR} , volume = {abs/1608.06993} , year = {2016} , url = {http://arxiv.org/abs/1608.06993} , archivePrefix = {arXiv} , eprint = {1608.06993} , timestamp = {Mon, 10 Sep 2018 15:49:32 +0200} , biburl = {https://dblp.org/rec/journals/corr/HuangLW16a.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } @misc{rw2019timm, author = {Ross Wightman}, title = {PyTorch Image Models}, year = {2019}, publisher = {GitHub}, journal = {GitHub repository}, doi = {10.5281/zenodo.4414861}, howpublished = {\\url{https://github.com/rwightman/pytorch-image-models}} }","title":"Citation"},{"location":"models/dla/","text":"Deep Layer Aggregation Extending \u201cshallow\u201d skip connections, Dense Layer Aggregation (DLA) incorporates more depth and sharing. The authors introduce two structures for deep layer aggregation (DLA): iterative deep aggregation (IDA) and hierarchical deep aggregation (HDA). These structures are expressed through an architectural framework, independent of the choice of backbone, for compatibility with current and future networks. IDA focuses on fusing resolutions and scales while HDA focuses on merging features from all modules and channels. IDA follows the base hierarchy to refine resolution and aggregate scale stage-bystage. HDA assembles its own hierarchy of tree-structured connections that cross and merge stages to aggregate different levels of representation. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'dla102' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. dla102 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'dla102' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { yu2019deep , title = {Deep Layer Aggregation} , author = {Fisher Yu and Dequan Wang and Evan Shelhamer and Trevor Darrell} , year = {2019} , eprint = {1707.06484} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Deep Layer Aggregation"},{"location":"models/dla/#deep-layer-aggregation","text":"Extending \u201cshallow\u201d skip connections, Dense Layer Aggregation (DLA) incorporates more depth and sharing. The authors introduce two structures for deep layer aggregation (DLA): iterative deep aggregation (IDA) and hierarchical deep aggregation (HDA). These structures are expressed through an architectural framework, independent of the choice of backbone, for compatibility with current and future networks. IDA focuses on fusing resolutions and scales while HDA focuses on merging features from all modules and channels. IDA follows the base hierarchy to refine resolution and aggregate scale stage-bystage. HDA assembles its own hierarchy of tree-structured connections that cross and merge stages to aggregate different levels of representation.","title":"Deep Layer Aggregation"},{"location":"models/dla/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'dla102' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. dla102 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/dla/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'dla102' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/dla/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/dla/#citation","text":"@misc { yu2019deep , title = {Deep Layer Aggregation} , author = {Fisher Yu and Dequan Wang and Evan Shelhamer and Trevor Darrell} , year = {2019} , eprint = {1707.06484} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/dpn/","text":"Dual Path Network (DPN) A Dual Path Network (DPN) is a convolutional neural network which presents a new topology of connection paths internally. The intuition is that ResNets enables feature re-usage while DenseNet enables new feature exploration, and both are important for learning good representations. To enjoy the benefits from both path topologies, Dual Path Networks share common features while maintaining the flexibility to explore new features through dual path architectures. The principal building block is an DPN Block . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'dpn107' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. dpn107 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'dpn107' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { chen2017dual , title = {Dual Path Networks} , author = {Yunpeng Chen and Jianan Li and Huaxin Xiao and Xiaojie Jin and Shuicheng Yan and Jiashi Feng} , year = {2017} , eprint = {1707.01629} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Dual Path Network (DPN)"},{"location":"models/dpn/#dual-path-network-dpn","text":"A Dual Path Network (DPN) is a convolutional neural network which presents a new topology of connection paths internally. The intuition is that ResNets enables feature re-usage while DenseNet enables new feature exploration, and both are important for learning good representations. To enjoy the benefits from both path topologies, Dual Path Networks share common features while maintaining the flexibility to explore new features through dual path architectures. The principal building block is an DPN Block .","title":"Dual Path Network (DPN)"},{"location":"models/dpn/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'dpn107' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. dpn107 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/dpn/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'dpn107' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/dpn/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/dpn/#citation","text":"@misc { chen2017dual , title = {Dual Path Networks} , author = {Yunpeng Chen and Jianan Li and Huaxin Xiao and Xiaojie Jin and Shuicheng Yan and Jiashi Feng} , year = {2017} , eprint = {1707.01629} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/ecaresnet/","text":"ECA-ResNet An ECA ResNet is a variant on a ResNet that utilises an Efficient Channel Attention module . Efficient Channel Attention is an architectural unit based on squeeze-and-excitation blocks that reduces model complexity without dimensionality reduction. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'ecaresnet101d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. ecaresnet101d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'ecaresnet101d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { wang2020ecanet , title = {ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks} , author = {Qilong Wang and Banggu Wu and Pengfei Zhu and Peihua Li and Wangmeng Zuo and Qinghua Hu} , year = {2020} , eprint = {1910.03151} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"ECA-ResNet"},{"location":"models/ecaresnet/#eca-resnet","text":"An ECA ResNet is a variant on a ResNet that utilises an Efficient Channel Attention module . Efficient Channel Attention is an architectural unit based on squeeze-and-excitation blocks that reduces model complexity without dimensionality reduction.","title":"ECA-ResNet"},{"location":"models/ecaresnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'ecaresnet101d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. ecaresnet101d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/ecaresnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'ecaresnet101d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/ecaresnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/ecaresnet/#citation","text":"@misc { wang2020ecanet , title = {ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks} , author = {Qilong Wang and Banggu Wu and Pengfei Zhu and Peihua Li and Wangmeng Zuo and Qinghua Hu} , year = {2020} , eprint = {1910.03151} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/efficientnet-pruned/","text":"EfficientNet (Knapsack Pruned) EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient . Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use 2^N 2^N times more computational resources, then we can simply increase the network depth by \\alpha ^ N \\alpha ^ N , width by \\beta ^ N \\beta ^ N , and image size by \\gamma ^ N \\gamma ^ N , where \\alpha, \\beta, \\gamma \\alpha, \\beta, \\gamma are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\phi \\phi to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2 , in addition to squeeze-and-excitation blocks . This collection consists of pruned EfficientNet models. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'efficientnet_b1_pruned' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. efficientnet_b1_pruned . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'efficientnet_b1_pruned' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { tan2020efficientnet , title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks} , author = {Mingxing Tan and Quoc V. Le} , year = {2020} , eprint = {1905.11946} , archivePrefix = {arXiv} , primaryClass = {cs.LG} } @misc{aflalo2020knapsack, title={Knapsack Pruning with Inner Distillation}, author={Yonathan Aflalo and Asaf Noy and Ming Lin and Itamar Friedman and Lihi Zelnik}, year={2020}, eprint={2002.08258}, archivePrefix={arXiv}, primaryClass={cs.LG} }","title":"EfficientNet (Knapsack Pruned)"},{"location":"models/efficientnet-pruned/#efficientnet-knapsack-pruned","text":"EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient . Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use 2^N 2^N times more computational resources, then we can simply increase the network depth by \\alpha ^ N \\alpha ^ N , width by \\beta ^ N \\beta ^ N , and image size by \\gamma ^ N \\gamma ^ N , where \\alpha, \\beta, \\gamma \\alpha, \\beta, \\gamma are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\phi \\phi to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2 , in addition to squeeze-and-excitation blocks . This collection consists of pruned EfficientNet models.","title":"EfficientNet (Knapsack Pruned)"},{"location":"models/efficientnet-pruned/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'efficientnet_b1_pruned' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. efficientnet_b1_pruned . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/efficientnet-pruned/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'efficientnet_b1_pruned' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/efficientnet-pruned/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/efficientnet-pruned/#citation","text":"@misc { tan2020efficientnet , title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks} , author = {Mingxing Tan and Quoc V. Le} , year = {2020} , eprint = {1905.11946} , archivePrefix = {arXiv} , primaryClass = {cs.LG} } @misc{aflalo2020knapsack, title={Knapsack Pruning with Inner Distillation}, author={Yonathan Aflalo and Asaf Noy and Ming Lin and Itamar Friedman and Lihi Zelnik}, year={2020}, eprint={2002.08258}, archivePrefix={arXiv}, primaryClass={cs.LG} }","title":"Citation"},{"location":"models/efficientnet/","text":"EfficientNet EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient . Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use 2^N 2^N times more computational resources, then we can simply increase the network depth by \\alpha ^ N \\alpha ^ N , width by \\beta ^ N \\beta ^ N , and image size by \\gamma ^ N \\gamma ^ N , where \\alpha, \\beta, \\gamma \\alpha, \\beta, \\gamma are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\phi \\phi to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2 , in addition to squeeze-and-excitation blocks . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'efficientnet_b0' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. efficientnet_b0 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'efficientnet_b0' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { tan2020efficientnet , title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks} , author = {Mingxing Tan and Quoc V. Le} , year = {2020} , eprint = {1905.11946} , archivePrefix = {arXiv} , primaryClass = {cs.LG} }","title":"EfficientNet"},{"location":"models/efficientnet/#efficientnet","text":"EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient . Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use 2^N 2^N times more computational resources, then we can simply increase the network depth by \\alpha ^ N \\alpha ^ N , width by \\beta ^ N \\beta ^ N , and image size by \\gamma ^ N \\gamma ^ N , where \\alpha, \\beta, \\gamma \\alpha, \\beta, \\gamma are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\phi \\phi to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2 , in addition to squeeze-and-excitation blocks .","title":"EfficientNet"},{"location":"models/efficientnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'efficientnet_b0' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. efficientnet_b0 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/efficientnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'efficientnet_b0' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/efficientnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/efficientnet/#citation","text":"@misc { tan2020efficientnet , title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks} , author = {Mingxing Tan and Quoc V. Le} , year = {2020} , eprint = {1905.11946} , archivePrefix = {arXiv} , primaryClass = {cs.LG} }","title":"Citation"},{"location":"models/ensemble-adversarial/","text":"# Ensemble Adversarial Inception ResNet v2 Inception-ResNet-v2 is a convolutional neural architecture that builds on the Inception family of architectures but incorporates residual connections (replacing the filter concatenation stage of the Inception architecture). This particular model was trained for study of adversarial examples (adversarial training). The weights from this model were ported from Tensorflow/Models . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'ens_adv_inception_resnet_v2' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. ens_adv_inception_resnet_v2 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'ens_adv_inception_resnet_v2' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/abs-1804-00097 , author = {Alexey Kurakin and Ian J. Goodfellow and Samy Bengio and Yinpeng Dong and Fangzhou Liao and Ming Liang and Tianyu Pang and Jun Zhu and Xiaolin Hu and Cihang Xie and Jianyu Wang and Zhishuai Zhang and Zhou Ren and Alan L. Yuille and Sangxia Huang and Yao Zhao and Yuzhe Zhao and Zhonglin Han and Junjiajia Long and Yerkebulan Berdibekov and Takuya Akiba and Seiya Tokui and Motoki Abe} , title = {Adversarial Attacks and Defences Competition} , journal = {CoRR} , volume = {abs/1804.00097} , year = {2018} , url = {http://arxiv.org/abs/1804.00097} , archivePrefix = {arXiv} , eprint = {1804.00097} , timestamp = {Thu, 31 Oct 2019 16:31:22 +0100} , biburl = {https://dblp.org/rec/journals/corr/abs-1804-00097.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Ensemble Adversarial Inception ResNet v2"},{"location":"models/ensemble-adversarial/#ensemble-adversarial-inception-resnet-v2","text":"Inception-ResNet-v2 is a convolutional neural architecture that builds on the Inception family of architectures but incorporates residual connections (replacing the filter concatenation stage of the Inception architecture). This particular model was trained for study of adversarial examples (adversarial training). The weights from this model were ported from Tensorflow/Models .","title":"# Ensemble Adversarial Inception ResNet v2"},{"location":"models/ensemble-adversarial/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'ens_adv_inception_resnet_v2' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. ens_adv_inception_resnet_v2 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/ensemble-adversarial/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'ens_adv_inception_resnet_v2' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/ensemble-adversarial/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/ensemble-adversarial/#citation","text":"@article { DBLP:journals/corr/abs-1804-00097 , author = {Alexey Kurakin and Ian J. Goodfellow and Samy Bengio and Yinpeng Dong and Fangzhou Liao and Ming Liang and Tianyu Pang and Jun Zhu and Xiaolin Hu and Cihang Xie and Jianyu Wang and Zhishuai Zhang and Zhou Ren and Alan L. Yuille and Sangxia Huang and Yao Zhao and Yuzhe Zhao and Zhonglin Han and Junjiajia Long and Yerkebulan Berdibekov and Takuya Akiba and Seiya Tokui and Motoki Abe} , title = {Adversarial Attacks and Defences Competition} , journal = {CoRR} , volume = {abs/1804.00097} , year = {2018} , url = {http://arxiv.org/abs/1804.00097} , archivePrefix = {arXiv} , eprint = {1804.00097} , timestamp = {Thu, 31 Oct 2019 16:31:22 +0100} , biburl = {https://dblp.org/rec/journals/corr/abs-1804-00097.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/ese-vovnet/","text":"ESE-VoVNet VoVNet is a convolutional neural network that seeks to make DenseNet more efficient by concatenating all features only once in the last feature map, which makes input size constant and enables enlarging new output channel. Read about one-shot aggregation here . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'ese_vovnet19b_dw' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. ese_vovnet19b_dw . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'ese_vovnet19b_dw' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { lee2019energy , title = {An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection} , author = {Youngwan Lee and Joong-won Hwang and Sangrok Lee and Yuseok Bae and Jongyoul Park} , year = {2019} , eprint = {1904.09730} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"ESE-VoVNet"},{"location":"models/ese-vovnet/#ese-vovnet","text":"VoVNet is a convolutional neural network that seeks to make DenseNet more efficient by concatenating all features only once in the last feature map, which makes input size constant and enables enlarging new output channel. Read about one-shot aggregation here .","title":"ESE-VoVNet"},{"location":"models/ese-vovnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'ese_vovnet19b_dw' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. ese_vovnet19b_dw . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/ese-vovnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'ese_vovnet19b_dw' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/ese-vovnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/ese-vovnet/#citation","text":"@misc { lee2019energy , title = {An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection} , author = {Youngwan Lee and Joong-won Hwang and Sangrok Lee and Yuseok Bae and Jongyoul Park} , year = {2019} , eprint = {1904.09730} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/fbnet/","text":"FBNet FBNet is a type of convolutional neural architectures discovered through DNAS neural architecture search. It utilises a basic type of image model block inspired by MobileNetv2 that utilises depthwise convolutions and an inverted residual structure (see components). The principal building block is the FBNet Block . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'fbnetc_100' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. fbnetc_100 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'fbnetc_100' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { wu2019fbnet , title = {FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search} , author = {Bichen Wu and Xiaoliang Dai and Peizhao Zhang and Yanghan Wang and Fei Sun and Yiming Wu and Yuandong Tian and Peter Vajda and Yangqing Jia and Kurt Keutzer} , year = {2019} , eprint = {1812.03443} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"FBNet"},{"location":"models/fbnet/#fbnet","text":"FBNet is a type of convolutional neural architectures discovered through DNAS neural architecture search. It utilises a basic type of image model block inspired by MobileNetv2 that utilises depthwise convolutions and an inverted residual structure (see components). The principal building block is the FBNet Block .","title":"FBNet"},{"location":"models/fbnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'fbnetc_100' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. fbnetc_100 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/fbnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'fbnetc_100' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/fbnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/fbnet/#citation","text":"@misc { wu2019fbnet , title = {FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search} , author = {Bichen Wu and Xiaoliang Dai and Peizhao Zhang and Yanghan Wang and Fei Sun and Yiming Wu and Yuandong Tian and Peter Vajda and Yangqing Jia and Kurt Keutzer} , year = {2019} , eprint = {1812.03443} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/gloun-inception-v3/","text":"(Gluon) Inception v3 Inception v3 is a convolutional neural network architecture from the Inception family that makes several improvements including using Label Smoothing , Factorized 7 x 7 convolutions, and the use of an auxiliary classifer to propagate label information lower down the network (along with the use of batch normalization for layers in the sidehead). The key building block is an Inception Module . The weights from this model were ported from Gluon . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'gluon_inception_v3' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. gluon_inception_v3 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'gluon_inception_v3' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/SzegedyVISW15 , author = {Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna} , title = {Rethinking the Inception Architecture for Computer Vision} , journal = {CoRR} , volume = {abs/1512.00567} , year = {2015} , url = {http://arxiv.org/abs/1512.00567} , archivePrefix = {arXiv} , eprint = {1512.00567} , timestamp = {Mon, 13 Aug 2018 16:49:07 +0200} , biburl = {https://dblp.org/rec/journals/corr/SzegedyVISW15.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"(Gluon) Inception v3"},{"location":"models/gloun-inception-v3/#gluon-inception-v3","text":"Inception v3 is a convolutional neural network architecture from the Inception family that makes several improvements including using Label Smoothing , Factorized 7 x 7 convolutions, and the use of an auxiliary classifer to propagate label information lower down the network (along with the use of batch normalization for layers in the sidehead). The key building block is an Inception Module . The weights from this model were ported from Gluon .","title":"(Gluon) Inception v3"},{"location":"models/gloun-inception-v3/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'gluon_inception_v3' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. gluon_inception_v3 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/gloun-inception-v3/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'gluon_inception_v3' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/gloun-inception-v3/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/gloun-inception-v3/#citation","text":"@article { DBLP:journals/corr/SzegedyVISW15 , author = {Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna} , title = {Rethinking the Inception Architecture for Computer Vision} , journal = {CoRR} , volume = {abs/1512.00567} , year = {2015} , url = {http://arxiv.org/abs/1512.00567} , archivePrefix = {arXiv} , eprint = {1512.00567} , timestamp = {Mon, 13 Aug 2018 16:49:07 +0200} , biburl = {https://dblp.org/rec/journals/corr/SzegedyVISW15.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/gloun-resnet/","text":"(Gluon) ResNet Residual Networks , or ResNets , learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack residual blocks ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. The weights from this model were ported from Gluon . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'gluon_resnet101_v1b' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. gluon_resnet101_v1b . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'gluon_resnet101_v1b' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/HeZRS15 , author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun} , title = {Deep Residual Learning for Image Recognition} , journal = {CoRR} , volume = {abs/1512.03385} , year = {2015} , url = {http://arxiv.org/abs/1512.03385} , archivePrefix = {arXiv} , eprint = {1512.03385} , timestamp = {Wed, 17 Apr 2019 17:23:45 +0200} , biburl = {https://dblp.org/rec/journals/corr/HeZRS15.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"(Gluon) ResNet"},{"location":"models/gloun-resnet/#gluon-resnet","text":"Residual Networks , or ResNets , learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack residual blocks ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. The weights from this model were ported from Gluon .","title":"(Gluon) ResNet"},{"location":"models/gloun-resnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'gluon_resnet101_v1b' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. gluon_resnet101_v1b . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/gloun-resnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'gluon_resnet101_v1b' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/gloun-resnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/gloun-resnet/#citation","text":"@article { DBLP:journals/corr/HeZRS15 , author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun} , title = {Deep Residual Learning for Image Recognition} , journal = {CoRR} , volume = {abs/1512.03385} , year = {2015} , url = {http://arxiv.org/abs/1512.03385} , archivePrefix = {arXiv} , eprint = {1512.03385} , timestamp = {Wed, 17 Apr 2019 17:23:45 +0200} , biburl = {https://dblp.org/rec/journals/corr/HeZRS15.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/gloun-resnext/","text":"(Gluon) ResNeXt A ResNeXt repeats a building block that aggregates a set of transformations with the same topology. Compared to a ResNet , it exposes a new dimension, cardinality (the size of the set of transformations) C C , as an essential factor in addition to the dimensions of depth and width. The weights from this model were ported from Gluon . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'gluon_resnext101_32x4d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. gluon_resnext101_32x4d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'gluon_resnext101_32x4d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/XieGDTH16 , author = {Saining Xie and Ross B. Girshick and Piotr Doll{\\'{a}}r and Zhuowen Tu and Kaiming He} , title = {Aggregated Residual Transformations for Deep Neural Networks} , journal = {CoRR} , volume = {abs/1611.05431} , year = {2016} , url = {http://arxiv.org/abs/1611.05431} , archivePrefix = {arXiv} , eprint = {1611.05431} , timestamp = {Mon, 13 Aug 2018 16:45:58 +0200} , biburl = {https://dblp.org/rec/journals/corr/XieGDTH16.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"(Gluon) ResNeXt"},{"location":"models/gloun-resnext/#gluon-resnext","text":"A ResNeXt repeats a building block that aggregates a set of transformations with the same topology. Compared to a ResNet , it exposes a new dimension, cardinality (the size of the set of transformations) C C , as an essential factor in addition to the dimensions of depth and width. The weights from this model were ported from Gluon .","title":"(Gluon) ResNeXt"},{"location":"models/gloun-resnext/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'gluon_resnext101_32x4d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. gluon_resnext101_32x4d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/gloun-resnext/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'gluon_resnext101_32x4d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/gloun-resnext/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/gloun-resnext/#citation","text":"@article { DBLP:journals/corr/XieGDTH16 , author = {Saining Xie and Ross B. Girshick and Piotr Doll{\\'{a}}r and Zhuowen Tu and Kaiming He} , title = {Aggregated Residual Transformations for Deep Neural Networks} , journal = {CoRR} , volume = {abs/1611.05431} , year = {2016} , url = {http://arxiv.org/abs/1611.05431} , archivePrefix = {arXiv} , eprint = {1611.05431} , timestamp = {Mon, 13 Aug 2018 16:45:58 +0200} , biburl = {https://dblp.org/rec/journals/corr/XieGDTH16.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/gloun-senet/","text":"(Gluon) SENet A SENet is a convolutional neural network architecture that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration. The weights from this model were ported from Gluon . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'gluon_senet154' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. gluon_senet154 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'gluon_senet154' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { hu2019squeezeandexcitation , title = {Squeeze-and-Excitation Networks} , author = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu} , year = {2019} , eprint = {1709.01507} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"(Gluon) SENet"},{"location":"models/gloun-senet/#gluon-senet","text":"A SENet is a convolutional neural network architecture that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration. The weights from this model were ported from Gluon .","title":"(Gluon) SENet"},{"location":"models/gloun-senet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'gluon_senet154' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. gluon_senet154 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/gloun-senet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'gluon_senet154' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/gloun-senet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/gloun-senet/#citation","text":"@misc { hu2019squeezeandexcitation , title = {Squeeze-and-Excitation Networks} , author = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu} , year = {2019} , eprint = {1709.01507} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/gloun-seresnext/","text":"(Gluon) SE-ResNeXt SE ResNeXt is a variant of a ResNext that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration. The weights from this model were ported from Gluon . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'gluon_seresnext101_32x4d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. gluon_seresnext101_32x4d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'gluon_seresnext101_32x4d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { hu2019squeezeandexcitation , title = {Squeeze-and-Excitation Networks} , author = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu} , year = {2019} , eprint = {1709.01507} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"(Gluon) SE-ResNeXt"},{"location":"models/gloun-seresnext/#gluon-se-resnext","text":"SE ResNeXt is a variant of a ResNext that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration. The weights from this model were ported from Gluon .","title":"(Gluon) SE-ResNeXt"},{"location":"models/gloun-seresnext/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'gluon_seresnext101_32x4d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. gluon_seresnext101_32x4d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/gloun-seresnext/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'gluon_seresnext101_32x4d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/gloun-seresnext/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/gloun-seresnext/#citation","text":"@misc { hu2019squeezeandexcitation , title = {Squeeze-and-Excitation Networks} , author = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu} , year = {2019} , eprint = {1709.01507} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/gloun-xception/","text":"(Gluon) Xception Xception is a convolutional neural network architecture that relies solely on depthwise separable convolution layers. The weights from this model were ported from Gluon . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'gluon_xception65' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. gluon_xception65 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'gluon_xception65' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { chollet2017xception , title = {Xception: Deep Learning with Depthwise Separable Convolutions} , author = {Fran\u00e7ois Chollet} , year = {2017} , eprint = {1610.02357} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"(Gluon) Xception"},{"location":"models/gloun-xception/#gluon-xception","text":"Xception is a convolutional neural network architecture that relies solely on depthwise separable convolution layers. The weights from this model were ported from Gluon .","title":"(Gluon) Xception"},{"location":"models/gloun-xception/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'gluon_xception65' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. gluon_xception65 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/gloun-xception/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'gluon_xception65' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/gloun-xception/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/gloun-xception/#citation","text":"@misc { chollet2017xception , title = {Xception: Deep Learning with Depthwise Separable Convolutions} , author = {Fran\u00e7ois Chollet} , year = {2017} , eprint = {1610.02357} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/hrnet/","text":"HRNet HRNet , or High-Resolution Net , is a general purpose convolutional neural network for tasks like semantic segmentation, object detection and image classification. It is able to maintain high resolution representations through the whole process. We start from a high-resolution convolution stream, gradually add high-to-low resolution convolution streams one by one, and connect the multi-resolution streams in parallel. The resulting network consists of several ( 4 4 in the paper) stages and the n n th stage contains n n streams corresponding to n n resolutions. The authors conduct repeated multi-resolution fusions by exchanging the information across the parallel streams over and over. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'hrnet_w18' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. hrnet_w18 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'hrnet_w18' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { sun2019highresolution , title = {High-Resolution Representations for Labeling Pixels and Regions} , author = {Ke Sun and Yang Zhao and Borui Jiang and Tianheng Cheng and Bin Xiao and Dong Liu and Yadong Mu and Xinggang Wang and Wenyu Liu and Jingdong Wang} , year = {2019} , eprint = {1904.04514} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"HRNet"},{"location":"models/hrnet/#hrnet","text":"HRNet , or High-Resolution Net , is a general purpose convolutional neural network for tasks like semantic segmentation, object detection and image classification. It is able to maintain high resolution representations through the whole process. We start from a high-resolution convolution stream, gradually add high-to-low resolution convolution streams one by one, and connect the multi-resolution streams in parallel. The resulting network consists of several ( 4 4 in the paper) stages and the n n th stage contains n n streams corresponding to n n resolutions. The authors conduct repeated multi-resolution fusions by exchanging the information across the parallel streams over and over.","title":"HRNet"},{"location":"models/hrnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'hrnet_w18' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. hrnet_w18 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/hrnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'hrnet_w18' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/hrnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/hrnet/#citation","text":"@misc { sun2019highresolution , title = {High-Resolution Representations for Labeling Pixels and Regions} , author = {Ke Sun and Yang Zhao and Borui Jiang and Tianheng Cheng and Bin Xiao and Dong Liu and Yadong Mu and Xinggang Wang and Wenyu Liu and Jingdong Wang} , year = {2019} , eprint = {1904.04514} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/ig-resnext/","text":"Instagram ResNeXt WSL A ResNeXt repeats a building block that aggregates a set of transformations with the same topology. Compared to a ResNet , it exposes a new dimension, cardinality (the size of the set of transformations) C C , as an essential factor in addition to the dimensions of depth and width. This model was trained on billions of Instagram images using thousands of distinct hashtags as labels exhibit excellent transfer learning performance. Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'ig_resnext101_32x16d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. ig_resnext101_32x16d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'ig_resnext101_32x16d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { mahajan2018exploring , title = {Exploring the Limits of Weakly Supervised Pretraining} , author = {Dhruv Mahajan and Ross Girshick and Vignesh Ramanathan and Kaiming He and Manohar Paluri and Yixuan Li and Ashwin Bharambe and Laurens van der Maaten} , year = {2018} , eprint = {1805.00932} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Instagram ResNeXt WSL"},{"location":"models/ig-resnext/#instagram-resnext-wsl","text":"A ResNeXt repeats a building block that aggregates a set of transformations with the same topology. Compared to a ResNet , it exposes a new dimension, cardinality (the size of the set of transformations) C C , as an essential factor in addition to the dimensions of depth and width. This model was trained on billions of Instagram images using thousands of distinct hashtags as labels exhibit excellent transfer learning performance. Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.","title":"Instagram ResNeXt WSL"},{"location":"models/ig-resnext/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'ig_resnext101_32x16d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. ig_resnext101_32x16d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/ig-resnext/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'ig_resnext101_32x16d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/ig-resnext/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/ig-resnext/#citation","text":"@misc { mahajan2018exploring , title = {Exploring the Limits of Weakly Supervised Pretraining} , author = {Dhruv Mahajan and Ross Girshick and Vignesh Ramanathan and Kaiming He and Manohar Paluri and Yixuan Li and Ashwin Bharambe and Laurens van der Maaten} , year = {2018} , eprint = {1805.00932} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/inception-resnet-v2/","text":"Inception ResNet v2 Inception-ResNet-v2 is a convolutional neural architecture that builds on the Inception family of architectures but incorporates residual connections (replacing the filter concatenation stage of the Inception architecture). How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'inception_resnet_v2' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. inception_resnet_v2 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'inception_resnet_v2' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { szegedy2016inceptionv4 , title = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning} , author = {Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alex Alemi} , year = {2016} , eprint = {1602.07261} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Inception ResNet v2"},{"location":"models/inception-resnet-v2/#inception-resnet-v2","text":"Inception-ResNet-v2 is a convolutional neural architecture that builds on the Inception family of architectures but incorporates residual connections (replacing the filter concatenation stage of the Inception architecture).","title":"Inception ResNet v2"},{"location":"models/inception-resnet-v2/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'inception_resnet_v2' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. inception_resnet_v2 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/inception-resnet-v2/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'inception_resnet_v2' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/inception-resnet-v2/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/inception-resnet-v2/#citation","text":"@misc { szegedy2016inceptionv4 , title = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning} , author = {Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alex Alemi} , year = {2016} , eprint = {1602.07261} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/inception-v3/","text":"Inception v3 Inception v3 is a convolutional neural network architecture from the Inception family that makes several improvements including using Label Smoothing , Factorized 7 x 7 convolutions, and the use of an auxiliary classifer to propagate label information lower down the network (along with the use of batch normalization for layers in the sidehead). The key building block is an Inception Module . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'inception_v3' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. inception_v3 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'inception_v3' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/SzegedyVISW15 , author = {Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna} , title = {Rethinking the Inception Architecture for Computer Vision} , journal = {CoRR} , volume = {abs/1512.00567} , year = {2015} , url = {http://arxiv.org/abs/1512.00567} , archivePrefix = {arXiv} , eprint = {1512.00567} , timestamp = {Mon, 13 Aug 2018 16:49:07 +0200} , biburl = {https://dblp.org/rec/journals/corr/SzegedyVISW15.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Inception v3"},{"location":"models/inception-v3/#inception-v3","text":"Inception v3 is a convolutional neural network architecture from the Inception family that makes several improvements including using Label Smoothing , Factorized 7 x 7 convolutions, and the use of an auxiliary classifer to propagate label information lower down the network (along with the use of batch normalization for layers in the sidehead). The key building block is an Inception Module .","title":"Inception v3"},{"location":"models/inception-v3/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'inception_v3' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. inception_v3 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/inception-v3/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'inception_v3' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/inception-v3/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/inception-v3/#citation","text":"@article { DBLP:journals/corr/SzegedyVISW15 , author = {Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna} , title = {Rethinking the Inception Architecture for Computer Vision} , journal = {CoRR} , volume = {abs/1512.00567} , year = {2015} , url = {http://arxiv.org/abs/1512.00567} , archivePrefix = {arXiv} , eprint = {1512.00567} , timestamp = {Mon, 13 Aug 2018 16:49:07 +0200} , biburl = {https://dblp.org/rec/journals/corr/SzegedyVISW15.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/inception-v4/","text":"Inception v4 Inception-v4 is a convolutional neural network architecture that builds on previous iterations of the Inception family by simplifying the architecture and using more inception modules than Inception-v3 . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'inception_v4' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. inception_v4 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'inception_v4' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { szegedy2016inceptionv4 , title = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning} , author = {Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alex Alemi} , year = {2016} , eprint = {1602.07261} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Inception v4"},{"location":"models/inception-v4/#inception-v4","text":"Inception-v4 is a convolutional neural network architecture that builds on previous iterations of the Inception family by simplifying the architecture and using more inception modules than Inception-v3 .","title":"Inception v4"},{"location":"models/inception-v4/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'inception_v4' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. inception_v4 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/inception-v4/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'inception_v4' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/inception-v4/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/inception-v4/#citation","text":"@misc { szegedy2016inceptionv4 , title = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning} , author = {Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alex Alemi} , year = {2016} , eprint = {1602.07261} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/legacy-se-resnet/","text":"(Legacy) SE-ResNet SE ResNet is a variant of a ResNet that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'legacy_seresnet101' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. legacy_seresnet101 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'legacy_seresnet101' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { hu2019squeezeandexcitation , title = {Squeeze-and-Excitation Networks} , author = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu} , year = {2019} , eprint = {1709.01507} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"(Legacy) SE-ResNet"},{"location":"models/legacy-se-resnet/#legacy-se-resnet","text":"SE ResNet is a variant of a ResNet that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration.","title":"(Legacy) SE-ResNet"},{"location":"models/legacy-se-resnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'legacy_seresnet101' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. legacy_seresnet101 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/legacy-se-resnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'legacy_seresnet101' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/legacy-se-resnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/legacy-se-resnet/#citation","text":"@misc { hu2019squeezeandexcitation , title = {Squeeze-and-Excitation Networks} , author = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu} , year = {2019} , eprint = {1709.01507} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/legacy-se-resnext/","text":"(Legacy) SE-ResNeXt SE ResNeXt is a variant of a ResNeXt that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'legacy_seresnext101_32x4d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. legacy_seresnext101_32x4d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'legacy_seresnext101_32x4d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { hu2019squeezeandexcitation , title = {Squeeze-and-Excitation Networks} , author = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu} , year = {2019} , eprint = {1709.01507} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"(Legacy) SE-ResNeXt"},{"location":"models/legacy-se-resnext/#legacy-se-resnext","text":"SE ResNeXt is a variant of a ResNeXt that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration.","title":"(Legacy) SE-ResNeXt"},{"location":"models/legacy-se-resnext/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'legacy_seresnext101_32x4d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. legacy_seresnext101_32x4d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/legacy-se-resnext/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'legacy_seresnext101_32x4d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/legacy-se-resnext/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/legacy-se-resnext/#citation","text":"@misc { hu2019squeezeandexcitation , title = {Squeeze-and-Excitation Networks} , author = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu} , year = {2019} , eprint = {1709.01507} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/legacy-senet/","text":"(Legacy) SENet A SENet is a convolutional neural network architecture that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration. The weights from this model were ported from Gluon. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'legacy_senet154' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. legacy_senet154 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'legacy_senet154' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { hu2019squeezeandexcitation , title = {Squeeze-and-Excitation Networks} , author = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu} , year = {2019} , eprint = {1709.01507} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"(Legacy) SENet"},{"location":"models/legacy-senet/#legacy-senet","text":"A SENet is a convolutional neural network architecture that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration. The weights from this model were ported from Gluon.","title":"(Legacy) SENet"},{"location":"models/legacy-senet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'legacy_senet154' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. legacy_senet154 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/legacy-senet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'legacy_senet154' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/legacy-senet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/legacy-senet/#citation","text":"@misc { hu2019squeezeandexcitation , title = {Squeeze-and-Excitation Networks} , author = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu} , year = {2019} , eprint = {1709.01507} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/mixnet/","text":"MixNet MixNet is a type of convolutional neural network discovered via AutoML that utilises MixConvs instead of regular depthwise convolutions . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'mixnet_l' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. mixnet_l . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'mixnet_l' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { tan2019mixconv , title = {MixConv: Mixed Depthwise Convolutional Kernels} , author = {Mingxing Tan and Quoc V. Le} , year = {2019} , eprint = {1907.09595} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"MixNet"},{"location":"models/mixnet/#mixnet","text":"MixNet is a type of convolutional neural network discovered via AutoML that utilises MixConvs instead of regular depthwise convolutions .","title":"MixNet"},{"location":"models/mixnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'mixnet_l' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. mixnet_l . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/mixnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'mixnet_l' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/mixnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/mixnet/#citation","text":"@misc { tan2019mixconv , title = {MixConv: Mixed Depthwise Convolutional Kernels} , author = {Mingxing Tan and Quoc V. Le} , year = {2019} , eprint = {1907.09595} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/mnasnet/","text":"MnasNet MnasNet is a type of convolutional neural network optimized for mobile devices that is discovered through mobile neural architecture search, which explicitly incorporates model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. The main building block is an inverted residual block (from MobileNetV2 ). How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'mnasnet_100' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. mnasnet_100 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'mnasnet_100' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { tan2019mnasnet , title = {MnasNet: Platform-Aware Neural Architecture Search for Mobile} , author = {Mingxing Tan and Bo Chen and Ruoming Pang and Vijay Vasudevan and Mark Sandler and Andrew Howard and Quoc V. Le} , year = {2019} , eprint = {1807.11626} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"MnasNet"},{"location":"models/mnasnet/#mnasnet","text":"MnasNet is a type of convolutional neural network optimized for mobile devices that is discovered through mobile neural architecture search, which explicitly incorporates model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. The main building block is an inverted residual block (from MobileNetV2 ).","title":"MnasNet"},{"location":"models/mnasnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'mnasnet_100' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. mnasnet_100 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/mnasnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'mnasnet_100' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/mnasnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/mnasnet/#citation","text":"@misc { tan2019mnasnet , title = {MnasNet: Platform-Aware Neural Architecture Search for Mobile} , author = {Mingxing Tan and Bo Chen and Ruoming Pang and Vijay Vasudevan and Mark Sandler and Andrew Howard and Quoc V. Le} , year = {2019} , eprint = {1807.11626} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/mobilenet-v2/","text":"MobileNet v2 MobileNetV2 is a convolutional neural network architecture that seeks to perform well on mobile devices. It is based on an inverted residual structure where the residual connections are between the bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. As a whole, the architecture of MobileNetV2 contains the initial fully convolution layer with 32 filters, followed by 19 residual bottleneck layers. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'mobilenetv2_100' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. mobilenetv2_100 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'mobilenetv2_100' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/abs-1801-04381 , author = {Mark Sandler and Andrew G. Howard and Menglong Zhu and Andrey Zhmoginov and Liang{-}Chieh Chen} , title = {Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation} , journal = {CoRR} , volume = {abs/1801.04381} , year = {2018} , url = {http://arxiv.org/abs/1801.04381} , archivePrefix = {arXiv} , eprint = {1801.04381} , timestamp = {Tue, 12 Jan 2021 15:30:06 +0100} , biburl = {https://dblp.org/rec/journals/corr/abs-1801-04381.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"MobileNet v2"},{"location":"models/mobilenet-v2/#mobilenet-v2","text":"MobileNetV2 is a convolutional neural network architecture that seeks to perform well on mobile devices. It is based on an inverted residual structure where the residual connections are between the bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. As a whole, the architecture of MobileNetV2 contains the initial fully convolution layer with 32 filters, followed by 19 residual bottleneck layers.","title":"MobileNet v2"},{"location":"models/mobilenet-v2/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'mobilenetv2_100' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. mobilenetv2_100 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/mobilenet-v2/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'mobilenetv2_100' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/mobilenet-v2/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/mobilenet-v2/#citation","text":"@article { DBLP:journals/corr/abs-1801-04381 , author = {Mark Sandler and Andrew G. Howard and Menglong Zhu and Andrey Zhmoginov and Liang{-}Chieh Chen} , title = {Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation} , journal = {CoRR} , volume = {abs/1801.04381} , year = {2018} , url = {http://arxiv.org/abs/1801.04381} , archivePrefix = {arXiv} , eprint = {1801.04381} , timestamp = {Tue, 12 Jan 2021 15:30:06 +0100} , biburl = {https://dblp.org/rec/journals/corr/abs-1801-04381.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/mobilenet-v3/","text":"MobileNet v3 MobileNetV3 is a convolutional neural network that is designed for mobile phone CPUs. The network design includes the use of a hard swish activation and squeeze-and-excitation modules in the MBConv blocks . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'mobilenetv3_large_100' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. mobilenetv3_large_100 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'mobilenetv3_large_100' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/abs-1905-02244 , author = {Andrew Howard and Mark Sandler and Grace Chu and Liang{-}Chieh Chen and Bo Chen and Mingxing Tan and Weijun Wang and Yukun Zhu and Ruoming Pang and Vijay Vasudevan and Quoc V. Le and Hartwig Adam} , title = {Searching for MobileNetV3} , journal = {CoRR} , volume = {abs/1905.02244} , year = {2019} , url = {http://arxiv.org/abs/1905.02244} , archivePrefix = {arXiv} , eprint = {1905.02244} , timestamp = {Tue, 12 Jan 2021 15:30:06 +0100} , biburl = {https://dblp.org/rec/journals/corr/abs-1905-02244.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"MobileNet v3"},{"location":"models/mobilenet-v3/#mobilenet-v3","text":"MobileNetV3 is a convolutional neural network that is designed for mobile phone CPUs. The network design includes the use of a hard swish activation and squeeze-and-excitation modules in the MBConv blocks .","title":"MobileNet v3"},{"location":"models/mobilenet-v3/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'mobilenetv3_large_100' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. mobilenetv3_large_100 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/mobilenet-v3/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'mobilenetv3_large_100' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/mobilenet-v3/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/mobilenet-v3/#citation","text":"@article { DBLP:journals/corr/abs-1905-02244 , author = {Andrew Howard and Mark Sandler and Grace Chu and Liang{-}Chieh Chen and Bo Chen and Mingxing Tan and Weijun Wang and Yukun Zhu and Ruoming Pang and Vijay Vasudevan and Quoc V. Le and Hartwig Adam} , title = {Searching for MobileNetV3} , journal = {CoRR} , volume = {abs/1905.02244} , year = {2019} , url = {http://arxiv.org/abs/1905.02244} , archivePrefix = {arXiv} , eprint = {1905.02244} , timestamp = {Tue, 12 Jan 2021 15:30:06 +0100} , biburl = {https://dblp.org/rec/journals/corr/abs-1905-02244.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/nasnet/","text":"NASNet NASNet is a type of convolutional neural network discovered through neural architecture search. The building blocks consist of normal and reduction cells. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'nasnetalarge' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. nasnetalarge . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'nasnetalarge' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { zoph2018learning , title = {Learning Transferable Architectures for Scalable Image Recognition} , author = {Barret Zoph and Vijay Vasudevan and Jonathon Shlens and Quoc V. Le} , year = {2018} , eprint = {1707.07012} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"NASNet"},{"location":"models/nasnet/#nasnet","text":"NASNet is a type of convolutional neural network discovered through neural architecture search. The building blocks consist of normal and reduction cells.","title":"NASNet"},{"location":"models/nasnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'nasnetalarge' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. nasnetalarge . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/nasnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'nasnetalarge' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/nasnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/nasnet/#citation","text":"@misc { zoph2018learning , title = {Learning Transferable Architectures for Scalable Image Recognition} , author = {Barret Zoph and Vijay Vasudevan and Jonathon Shlens and Quoc V. Le} , year = {2018} , eprint = {1707.07012} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/noisy-student/","text":"Noisy Student (EfficientNet) Noisy Student Training is a semi-supervised learning approach. It extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. It has three main steps: train a teacher model on labeled images use the teacher to generate pseudo labels on unlabeled images train a student model on the combination of labeled images and pseudo labeled images. The algorithm is iterated a few times by treating the student as a teacher to relabel the unlabeled data and training a new student. Noisy Student Training seeks to improve on self-training and distillation in two ways. First, it makes the student larger than, or at least equal to, the teacher so the student can better learn from a larger dataset. Second, it adds noise to the student so the noised student is forced to learn harder from the pseudo labels. To noise the student, it uses input noise such as RandAugment data augmentation, and model noise such as dropout and stochastic depth during training. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'tf_efficientnet_b0_ns' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tf_efficientnet_b0_ns . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tf_efficientnet_b0_ns' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { xie2020selftraining , title = {Self-training with Noisy Student improves ImageNet classification} , author = {Qizhe Xie and Minh-Thang Luong and Eduard Hovy and Quoc V. Le} , year = {2020} , eprint = {1911.04252} , archivePrefix = {arXiv} , primaryClass = {cs.LG} }","title":"Noisy Student (EfficientNet)"},{"location":"models/noisy-student/#noisy-student-efficientnet","text":"Noisy Student Training is a semi-supervised learning approach. It extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. It has three main steps: train a teacher model on labeled images use the teacher to generate pseudo labels on unlabeled images train a student model on the combination of labeled images and pseudo labeled images. The algorithm is iterated a few times by treating the student as a teacher to relabel the unlabeled data and training a new student. Noisy Student Training seeks to improve on self-training and distillation in two ways. First, it makes the student larger than, or at least equal to, the teacher so the student can better learn from a larger dataset. Second, it adds noise to the student so the noised student is forced to learn harder from the pseudo labels. To noise the student, it uses input noise such as RandAugment data augmentation, and model noise such as dropout and stochastic depth during training.","title":"Noisy Student (EfficientNet)"},{"location":"models/noisy-student/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'tf_efficientnet_b0_ns' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tf_efficientnet_b0_ns . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/noisy-student/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tf_efficientnet_b0_ns' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/noisy-student/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/noisy-student/#citation","text":"@misc { xie2020selftraining , title = {Self-training with Noisy Student improves ImageNet classification} , author = {Qizhe Xie and Minh-Thang Luong and Eduard Hovy and Quoc V. Le} , year = {2020} , eprint = {1911.04252} , archivePrefix = {arXiv} , primaryClass = {cs.LG} }","title":"Citation"},{"location":"models/pnasnet/","text":"PNASNet Progressive Neural Architecture Search , or PNAS , is a method for learning the structure of convolutional neural networks (CNNs). It uses a sequential model-based optimization (SMBO) strategy, where we search the space of cell structures, starting with simple (shallow) models and progressing to complex ones, pruning out unpromising structures as we go. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'pnasnet5large' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. pnasnet5large . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'pnasnet5large' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { liu2018progressive , title = {Progressive Neural Architecture Search} , author = {Chenxi Liu and Barret Zoph and Maxim Neumann and Jonathon Shlens and Wei Hua and Li-Jia Li and Li Fei-Fei and Alan Yuille and Jonathan Huang and Kevin Murphy} , year = {2018} , eprint = {1712.00559} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"PNASNet"},{"location":"models/pnasnet/#pnasnet","text":"Progressive Neural Architecture Search , or PNAS , is a method for learning the structure of convolutional neural networks (CNNs). It uses a sequential model-based optimization (SMBO) strategy, where we search the space of cell structures, starting with simple (shallow) models and progressing to complex ones, pruning out unpromising structures as we go.","title":"PNASNet"},{"location":"models/pnasnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'pnasnet5large' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. pnasnet5large . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/pnasnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'pnasnet5large' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/pnasnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/pnasnet/#citation","text":"@misc { liu2018progressive , title = {Progressive Neural Architecture Search} , author = {Chenxi Liu and Barret Zoph and Maxim Neumann and Jonathon Shlens and Wei Hua and Li-Jia Li and Li Fei-Fei and Alan Yuille and Jonathan Huang and Kevin Murphy} , year = {2018} , eprint = {1712.00559} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/regnetx/","text":"RegNetX RegNetX is a convolutional network design space with simple, regular models with parameters: depth d d , initial width w\\_{0} > 0 w\\_{0} > 0 , and slope w\\_{a} > 0 w\\_{a} > 0 , and generates a different block width u\\_{j} u\\_{j} for each block j < d j < d . The key restriction for the RegNet types of model is that there is a linear parameterisation of block widths (the design space only contains models with this linear structure): u\\_{j} = w\\_{0} + w\\_{a}\\cdot{j} u\\_{j} = w\\_{0} + w\\_{a}\\cdot{j} For RegNetX we have additional restrictions: we set b = 1 b = 1 (the bottleneck ratio), 12 \\leq d \\leq 28 12 \\leq d \\leq 28 , and w\\_{m} \\geq 2 w\\_{m} \\geq 2 (the width multiplier). How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'regnetx_002' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. regnetx_002 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'regnetx_002' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { radosavovic2020designing , title = {Designing Network Design Spaces} , author = {Ilija Radosavovic and Raj Prateek Kosaraju and Ross Girshick and Kaiming He and Piotr Doll\u00e1r} , year = {2020} , eprint = {2003.13678} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"RegNetX"},{"location":"models/regnetx/#regnetx","text":"RegNetX is a convolutional network design space with simple, regular models with parameters: depth d d , initial width w\\_{0} > 0 w\\_{0} > 0 , and slope w\\_{a} > 0 w\\_{a} > 0 , and generates a different block width u\\_{j} u\\_{j} for each block j < d j < d . The key restriction for the RegNet types of model is that there is a linear parameterisation of block widths (the design space only contains models with this linear structure): u\\_{j} = w\\_{0} + w\\_{a}\\cdot{j} u\\_{j} = w\\_{0} + w\\_{a}\\cdot{j} For RegNetX we have additional restrictions: we set b = 1 b = 1 (the bottleneck ratio), 12 \\leq d \\leq 28 12 \\leq d \\leq 28 , and w\\_{m} \\geq 2 w\\_{m} \\geq 2 (the width multiplier).","title":"RegNetX"},{"location":"models/regnetx/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'regnetx_002' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. regnetx_002 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/regnetx/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'regnetx_002' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/regnetx/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/regnetx/#citation","text":"@misc { radosavovic2020designing , title = {Designing Network Design Spaces} , author = {Ilija Radosavovic and Raj Prateek Kosaraju and Ross Girshick and Kaiming He and Piotr Doll\u00e1r} , year = {2020} , eprint = {2003.13678} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/regnety/","text":"RegNetY RegNetY is a convolutional network design space with simple, regular models with parameters: depth d d , initial width w\\_{0} > 0 w\\_{0} > 0 , and slope w\\_{a} > 0 w\\_{a} > 0 , and generates a different block width u\\_{j} u\\_{j} for each block j < d j < d . The key restriction for the RegNet types of model is that there is a linear parameterisation of block widths (the design space only contains models with this linear structure): u\\_{j} = w\\_{0} + w\\_{a}\\cdot{j} u\\_{j} = w\\_{0} + w\\_{a}\\cdot{j} For RegNetX authors have additional restrictions: we set b = 1 b = 1 (the bottleneck ratio), 12 \\leq d \\leq 28 12 \\leq d \\leq 28 , and w\\_{m} \\geq 2 w\\_{m} \\geq 2 (the width multiplier). For RegNetY authors make one change, which is to include Squeeze-and-Excitation blocks . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'regnety_002' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. regnety_002 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'regnety_002' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { radosavovic2020designing , title = {Designing Network Design Spaces} , author = {Ilija Radosavovic and Raj Prateek Kosaraju and Ross Girshick and Kaiming He and Piotr Doll\u00e1r} , year = {2020} , eprint = {2003.13678} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"RegNetY"},{"location":"models/regnety/#regnety","text":"RegNetY is a convolutional network design space with simple, regular models with parameters: depth d d , initial width w\\_{0} > 0 w\\_{0} > 0 , and slope w\\_{a} > 0 w\\_{a} > 0 , and generates a different block width u\\_{j} u\\_{j} for each block j < d j < d . The key restriction for the RegNet types of model is that there is a linear parameterisation of block widths (the design space only contains models with this linear structure): u\\_{j} = w\\_{0} + w\\_{a}\\cdot{j} u\\_{j} = w\\_{0} + w\\_{a}\\cdot{j} For RegNetX authors have additional restrictions: we set b = 1 b = 1 (the bottleneck ratio), 12 \\leq d \\leq 28 12 \\leq d \\leq 28 , and w\\_{m} \\geq 2 w\\_{m} \\geq 2 (the width multiplier). For RegNetY authors make one change, which is to include Squeeze-and-Excitation blocks .","title":"RegNetY"},{"location":"models/regnety/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'regnety_002' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. regnety_002 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/regnety/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'regnety_002' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/regnety/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/regnety/#citation","text":"@misc { radosavovic2020designing , title = {Designing Network Design Spaces} , author = {Ilija Radosavovic and Raj Prateek Kosaraju and Ross Girshick and Kaiming He and Piotr Doll\u00e1r} , year = {2020} , eprint = {2003.13678} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/res2net/","text":"Res2Net Res2Net is an image model that employs a variation on bottleneck residual blocks, Res2Net Blocks . The motivation is to be able to represent features at multiple scales. This is achieved through a novel building block for CNNs that constructs hierarchical residual-like connections within one single residual block. This represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'res2net101_26w_4s' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. res2net101_26w_4s . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'res2net101_26w_4s' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { Gao_2021 , title = {Res2Net: A New Multi-Scale Backbone Architecture} , volume = {43} , ISSN = {1939-3539} , url = {http://dx.doi.org/10.1109/TPAMI.2019.2938758} , DOI = {10.1109/tpami.2019.2938758} , number = {2} , journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence} , publisher = {Institute of Electrical and Electronics Engineers (IEEE)} , author = {Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip} , year = {2021} , month = {Feb} , pages = {652\u2013662} }","title":"Res2Net"},{"location":"models/res2net/#res2net","text":"Res2Net is an image model that employs a variation on bottleneck residual blocks, Res2Net Blocks . The motivation is to be able to represent features at multiple scales. This is achieved through a novel building block for CNNs that constructs hierarchical residual-like connections within one single residual block. This represents multi-scale features at a granular level and increases the range of receptive fields for each network layer.","title":"Res2Net"},{"location":"models/res2net/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'res2net101_26w_4s' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. res2net101_26w_4s . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/res2net/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'res2net101_26w_4s' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/res2net/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/res2net/#citation","text":"@article { Gao_2021 , title = {Res2Net: A New Multi-Scale Backbone Architecture} , volume = {43} , ISSN = {1939-3539} , url = {http://dx.doi.org/10.1109/TPAMI.2019.2938758} , DOI = {10.1109/tpami.2019.2938758} , number = {2} , journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence} , publisher = {Institute of Electrical and Electronics Engineers (IEEE)} , author = {Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip} , year = {2021} , month = {Feb} , pages = {652\u2013662} }","title":"Citation"},{"location":"models/res2next/","text":"Res2NeXt Res2NeXt is an image model that employs a variation on ResNeXt bottleneck residual blocks. The motivation is to be able to represent features at multiple scales. This is achieved through a novel building block for CNNs that constructs hierarchical residual-like connections within one single residual block. This represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'res2next50' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. res2next50 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'res2next50' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { Gao_2021 , title = {Res2Net: A New Multi-Scale Backbone Architecture} , volume = {43} , ISSN = {1939-3539} , url = {http://dx.doi.org/10.1109/TPAMI.2019.2938758} , DOI = {10.1109/tpami.2019.2938758} , number = {2} , journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence} , publisher = {Institute of Electrical and Electronics Engineers (IEEE)} , author = {Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip} , year = {2021} , month = {Feb} , pages = {652\u2013662} }","title":"Res2NeXt"},{"location":"models/res2next/#res2next","text":"Res2NeXt is an image model that employs a variation on ResNeXt bottleneck residual blocks. The motivation is to be able to represent features at multiple scales. This is achieved through a novel building block for CNNs that constructs hierarchical residual-like connections within one single residual block. This represents multi-scale features at a granular level and increases the range of receptive fields for each network layer.","title":"Res2NeXt"},{"location":"models/res2next/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'res2next50' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. res2next50 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/res2next/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'res2next50' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/res2next/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/res2next/#citation","text":"@article { Gao_2021 , title = {Res2Net: A New Multi-Scale Backbone Architecture} , volume = {43} , ISSN = {1939-3539} , url = {http://dx.doi.org/10.1109/TPAMI.2019.2938758} , DOI = {10.1109/tpami.2019.2938758} , number = {2} , journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence} , publisher = {Institute of Electrical and Electronics Engineers (IEEE)} , author = {Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip} , year = {2021} , month = {Feb} , pages = {652\u2013662} }","title":"Citation"},{"location":"models/resnest/","text":"ResNeSt A ResNeSt is a variant on a ResNet , which instead stacks Split-Attention blocks . The cardinal group representations are then concatenated along the channel dimension: V = \\text{Concat} V = \\text{Concat} { V^{1},V^{2},\\cdots{V}^{K} V^{1},V^{2},\\cdots{V}^{K} }. As in standard residual blocks, the final output Y Y of otheur Split-Attention block is produced using a shortcut connection: Y=V+X Y=V+X , if the input and output feature-map share the same shape. For blocks with a stride, an appropriate transformation \\mathcal{T} \\mathcal{T} is applied to the shortcut connection to align the output shapes: Y=V+\\mathcal{T}(X) Y=V+\\mathcal{T}(X) . For example, \\mathcal{T} \\mathcal{T} can be strided convolution or combined convolution-with-pooling. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'resnest101e' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. resnest101e . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'resnest101e' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { zhang2020resnest , title = {ResNeSt: Split-Attention Networks} , author = {Hang Zhang and Chongruo Wu and Zhongyue Zhang and Yi Zhu and Haibin Lin and Zhi Zhang and Yue Sun and Tong He and Jonas Mueller and R. Manmatha and Mu Li and Alexander Smola} , year = {2020} , eprint = {2004.08955} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"ResNeSt"},{"location":"models/resnest/#resnest","text":"A ResNeSt is a variant on a ResNet , which instead stacks Split-Attention blocks . The cardinal group representations are then concatenated along the channel dimension: V = \\text{Concat} V = \\text{Concat} { V^{1},V^{2},\\cdots{V}^{K} V^{1},V^{2},\\cdots{V}^{K} }. As in standard residual blocks, the final output Y Y of otheur Split-Attention block is produced using a shortcut connection: Y=V+X Y=V+X , if the input and output feature-map share the same shape. For blocks with a stride, an appropriate transformation \\mathcal{T} \\mathcal{T} is applied to the shortcut connection to align the output shapes: Y=V+\\mathcal{T}(X) Y=V+\\mathcal{T}(X) . For example, \\mathcal{T} \\mathcal{T} can be strided convolution or combined convolution-with-pooling.","title":"ResNeSt"},{"location":"models/resnest/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'resnest101e' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. resnest101e . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/resnest/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'resnest101e' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/resnest/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/resnest/#citation","text":"@misc { zhang2020resnest , title = {ResNeSt: Split-Attention Networks} , author = {Hang Zhang and Chongruo Wu and Zhongyue Zhang and Yi Zhu and Haibin Lin and Zhi Zhang and Yue Sun and Tong He and Jonas Mueller and R. Manmatha and Mu Li and Alexander Smola} , year = {2020} , eprint = {2004.08955} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/resnet-d/","text":"ResNet-D ResNet-D is a modification on the ResNet architecture that utilises an average pooling tweak for downsampling. The motivation is that in the unmodified ResNet, the 1\u00d71 convolution for the downsampling block ignores \u00be of input feature maps, so this is modified so no information will be ignored How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'resnet101d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. resnet101d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'resnet101d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { he2018bag , title = {Bag of Tricks for Image Classification with Convolutional Neural Networks} , author = {Tong He and Zhi Zhang and Hang Zhang and Zhongyue Zhang and Junyuan Xie and Mu Li} , year = {2018} , eprint = {1812.01187} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"ResNet-D"},{"location":"models/resnet-d/#resnet-d","text":"ResNet-D is a modification on the ResNet architecture that utilises an average pooling tweak for downsampling. The motivation is that in the unmodified ResNet, the 1\u00d71 convolution for the downsampling block ignores \u00be of input feature maps, so this is modified so no information will be ignored","title":"ResNet-D"},{"location":"models/resnet-d/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'resnet101d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. resnet101d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/resnet-d/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'resnet101d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/resnet-d/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/resnet-d/#citation","text":"@misc { he2018bag , title = {Bag of Tricks for Image Classification with Convolutional Neural Networks} , author = {Tong He and Zhi Zhang and Hang Zhang and Zhongyue Zhang and Junyuan Xie and Mu Li} , year = {2018} , eprint = {1812.01187} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/resnet/","text":"ResNet Residual Networks , or ResNets , learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack residual blocks ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'resnet18' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. resnet18 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'resnet18' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/HeZRS15 , author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun} , title = {Deep Residual Learning for Image Recognition} , journal = {CoRR} , volume = {abs/1512.03385} , year = {2015} , url = {http://arxiv.org/abs/1512.03385} , archivePrefix = {arXiv} , eprint = {1512.03385} , timestamp = {Wed, 17 Apr 2019 17:23:45 +0200} , biburl = {https://dblp.org/rec/journals/corr/HeZRS15.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"ResNet"},{"location":"models/resnet/#resnet","text":"Residual Networks , or ResNets , learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack residual blocks ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks.","title":"ResNet"},{"location":"models/resnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'resnet18' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. resnet18 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/resnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'resnet18' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/resnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/resnet/#citation","text":"@article { DBLP:journals/corr/HeZRS15 , author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun} , title = {Deep Residual Learning for Image Recognition} , journal = {CoRR} , volume = {abs/1512.03385} , year = {2015} , url = {http://arxiv.org/abs/1512.03385} , archivePrefix = {arXiv} , eprint = {1512.03385} , timestamp = {Wed, 17 Apr 2019 17:23:45 +0200} , biburl = {https://dblp.org/rec/journals/corr/HeZRS15.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/resnext/","text":"ResNeXt A ResNeXt repeats a building block that aggregates a set of transformations with the same topology. Compared to a ResNet , it exposes a new dimension, cardinality (the size of the set of transformations) C C , as an essential factor in addition to the dimensions of depth and width. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'resnext101_32x8d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. resnext101_32x8d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'resnext101_32x8d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/XieGDTH16 , author = {Saining Xie and Ross B. Girshick and Piotr Doll{\\'{a}}r and Zhuowen Tu and Kaiming He} , title = {Aggregated Residual Transformations for Deep Neural Networks} , journal = {CoRR} , volume = {abs/1611.05431} , year = {2016} , url = {http://arxiv.org/abs/1611.05431} , archivePrefix = {arXiv} , eprint = {1611.05431} , timestamp = {Mon, 13 Aug 2018 16:45:58 +0200} , biburl = {https://dblp.org/rec/journals/corr/XieGDTH16.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"ResNeXt"},{"location":"models/resnext/#resnext","text":"A ResNeXt repeats a building block that aggregates a set of transformations with the same topology. Compared to a ResNet , it exposes a new dimension, cardinality (the size of the set of transformations) C C , as an essential factor in addition to the dimensions of depth and width.","title":"ResNeXt"},{"location":"models/resnext/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'resnext101_32x8d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. resnext101_32x8d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/resnext/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'resnext101_32x8d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/resnext/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/resnext/#citation","text":"@article { DBLP:journals/corr/XieGDTH16 , author = {Saining Xie and Ross B. Girshick and Piotr Doll{\\'{a}}r and Zhuowen Tu and Kaiming He} , title = {Aggregated Residual Transformations for Deep Neural Networks} , journal = {CoRR} , volume = {abs/1611.05431} , year = {2016} , url = {http://arxiv.org/abs/1611.05431} , archivePrefix = {arXiv} , eprint = {1611.05431} , timestamp = {Mon, 13 Aug 2018 16:45:58 +0200} , biburl = {https://dblp.org/rec/journals/corr/XieGDTH16.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/rexnet/","text":"RexNet Rank Expansion Networks (ReXNets) follow a set of new design principles for designing bottlenecks in image classification models. Authors refine each layer by 1) expanding the input channel size of the convolution layer and 2) replacing the ReLU6s . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'rexnet_100' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. rexnet_100 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'rexnet_100' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { han2020rexnet , title = {ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network} , author = {Dongyoon Han and Sangdoo Yun and Byeongho Heo and YoungJoon Yoo} , year = {2020} , eprint = {2007.00992} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"RexNet"},{"location":"models/rexnet/#rexnet","text":"Rank Expansion Networks (ReXNets) follow a set of new design principles for designing bottlenecks in image classification models. Authors refine each layer by 1) expanding the input channel size of the convolution layer and 2) replacing the ReLU6s .","title":"RexNet"},{"location":"models/rexnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'rexnet_100' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. rexnet_100 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/rexnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'rexnet_100' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/rexnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/rexnet/#citation","text":"@misc { han2020rexnet , title = {ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network} , author = {Dongyoon Han and Sangdoo Yun and Byeongho Heo and YoungJoon Yoo} , year = {2020} , eprint = {2007.00992} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/se-resnet/","text":"SE-ResNet SE ResNet is a variant of a ResNet that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'seresnet152d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. seresnet152d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'seresnet152d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { hu2019squeezeandexcitation , title = {Squeeze-and-Excitation Networks} , author = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu} , year = {2019} , eprint = {1709.01507} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"SE-ResNet"},{"location":"models/se-resnet/#se-resnet","text":"SE ResNet is a variant of a ResNet that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration.","title":"SE-ResNet"},{"location":"models/se-resnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'seresnet152d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. seresnet152d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/se-resnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'seresnet152d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/se-resnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/se-resnet/#citation","text":"@misc { hu2019squeezeandexcitation , title = {Squeeze-and-Excitation Networks} , author = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu} , year = {2019} , eprint = {1709.01507} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/selecsls/","text":"SelecSLS SelecSLS uses novel selective long and short range skip connections to improve the information flow allowing for a drastically faster network without compromising accuracy. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'selecsls42b' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. selecsls42b . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'selecsls42b' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { Mehta_2020 , title = {XNect} , volume = {39} , ISSN = {1557-7368} , url = {http://dx.doi.org/10.1145/3386569.3392410} , DOI = {10.1145/3386569.3392410} , number = {4} , journal = {ACM Transactions on Graphics} , publisher = {Association for Computing Machinery (ACM)} , author = {Mehta, Dushyant and Sotnychenko, Oleksandr and Mueller, Franziska and Xu, Weipeng and Elgharib, Mohamed and Fua, Pascal and Seidel, Hans-Peter and Rhodin, Helge and Pons-Moll, Gerard and Theobalt, Christian} , year = {2020} , month = {Jul} }","title":"SelecSLS"},{"location":"models/selecsls/#selecsls","text":"SelecSLS uses novel selective long and short range skip connections to improve the information flow allowing for a drastically faster network without compromising accuracy.","title":"SelecSLS"},{"location":"models/selecsls/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'selecsls42b' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. selecsls42b . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/selecsls/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'selecsls42b' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/selecsls/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/selecsls/#citation","text":"@article { Mehta_2020 , title = {XNect} , volume = {39} , ISSN = {1557-7368} , url = {http://dx.doi.org/10.1145/3386569.3392410} , DOI = {10.1145/3386569.3392410} , number = {4} , journal = {ACM Transactions on Graphics} , publisher = {Association for Computing Machinery (ACM)} , author = {Mehta, Dushyant and Sotnychenko, Oleksandr and Mueller, Franziska and Xu, Weipeng and Elgharib, Mohamed and Fua, Pascal and Seidel, Hans-Peter and Rhodin, Helge and Pons-Moll, Gerard and Theobalt, Christian} , year = {2020} , month = {Jul} }","title":"Citation"},{"location":"models/seresnext/","text":"SE-ResNeXt SE ResNeXt is a variant of a ResNext that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'seresnext26d_32x4d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. seresnext26d_32x4d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'seresnext26d_32x4d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { hu2019squeezeandexcitation , title = {Squeeze-and-Excitation Networks} , author = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu} , year = {2019} , eprint = {1709.01507} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"SE-ResNeXt"},{"location":"models/seresnext/#se-resnext","text":"SE ResNeXt is a variant of a ResNext that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration.","title":"SE-ResNeXt"},{"location":"models/seresnext/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'seresnext26d_32x4d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. seresnext26d_32x4d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/seresnext/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'seresnext26d_32x4d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/seresnext/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/seresnext/#citation","text":"@misc { hu2019squeezeandexcitation , title = {Squeeze-and-Excitation Networks} , author = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu} , year = {2019} , eprint = {1709.01507} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/skresnet/","text":"SK-ResNet SK ResNet is a variant of a ResNet that employs a Selective Kernel unit. In general, all the large kernel convolutions in the original bottleneck blocks in ResNet are replaced by the proposed SK convolutions , enabling the network to choose appropriate receptive field sizes in an adaptive manner. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'skresnet18' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. skresnet18 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'skresnet18' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { li2019selective , title = {Selective Kernel Networks} , author = {Xiang Li and Wenhai Wang and Xiaolin Hu and Jian Yang} , year = {2019} , eprint = {1903.06586} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"SK-ResNet"},{"location":"models/skresnet/#sk-resnet","text":"SK ResNet is a variant of a ResNet that employs a Selective Kernel unit. In general, all the large kernel convolutions in the original bottleneck blocks in ResNet are replaced by the proposed SK convolutions , enabling the network to choose appropriate receptive field sizes in an adaptive manner.","title":"SK-ResNet"},{"location":"models/skresnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'skresnet18' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. skresnet18 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/skresnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'skresnet18' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/skresnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/skresnet/#citation","text":"@misc { li2019selective , title = {Selective Kernel Networks} , author = {Xiang Li and Wenhai Wang and Xiaolin Hu and Jian Yang} , year = {2019} , eprint = {1903.06586} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/skresnext/","text":"SK-ResNeXt SK ResNeXt is a variant of a ResNeXt that employs a Selective Kernel unit. In general, all the large kernel convolutions in the original bottleneck blocks in ResNext are replaced by the proposed SK convolutions , enabling the network to choose appropriate receptive field sizes in an adaptive manner. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'skresnext50_32x4d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. skresnext50_32x4d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'skresnext50_32x4d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { li2019selective , title = {Selective Kernel Networks} , author = {Xiang Li and Wenhai Wang and Xiaolin Hu and Jian Yang} , year = {2019} , eprint = {1903.06586} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"SK-ResNeXt"},{"location":"models/skresnext/#sk-resnext","text":"SK ResNeXt is a variant of a ResNeXt that employs a Selective Kernel unit. In general, all the large kernel convolutions in the original bottleneck blocks in ResNext are replaced by the proposed SK convolutions , enabling the network to choose appropriate receptive field sizes in an adaptive manner.","title":"SK-ResNeXt"},{"location":"models/skresnext/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'skresnext50_32x4d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. skresnext50_32x4d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/skresnext/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'skresnext50_32x4d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/skresnext/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/skresnext/#citation","text":"@misc { li2019selective , title = {Selective Kernel Networks} , author = {Xiang Li and Wenhai Wang and Xiaolin Hu and Jian Yang} , year = {2019} , eprint = {1903.06586} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/spnasnet/","text":"SPNASNet Single-Path NAS is a novel differentiable NAS method for designing hardware-efficient ConvNets in less than 4 hours. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'spnasnet_100' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. spnasnet_100 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'spnasnet_100' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { stamoulis2019singlepath , title = {Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours} , author = {Dimitrios Stamoulis and Ruizhou Ding and Di Wang and Dimitrios Lymberopoulos and Bodhi Priyantha and Jie Liu and Diana Marculescu} , year = {2019} , eprint = {1904.02877} , archivePrefix = {arXiv} , primaryClass = {cs.LG} }","title":"SPNASNet"},{"location":"models/spnasnet/#spnasnet","text":"Single-Path NAS is a novel differentiable NAS method for designing hardware-efficient ConvNets in less than 4 hours.","title":"SPNASNet"},{"location":"models/spnasnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'spnasnet_100' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. spnasnet_100 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/spnasnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'spnasnet_100' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/spnasnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/spnasnet/#citation","text":"@misc { stamoulis2019singlepath , title = {Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours} , author = {Dimitrios Stamoulis and Ruizhou Ding and Di Wang and Dimitrios Lymberopoulos and Bodhi Priyantha and Jie Liu and Diana Marculescu} , year = {2019} , eprint = {1904.02877} , archivePrefix = {arXiv} , primaryClass = {cs.LG} }","title":"Citation"},{"location":"models/ssl-resnet/","text":"SSL ResNet Residual Networks , or ResNets , learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack residual blocks ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. The model in this collection utilises semi-supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'ssl_resnet18' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. ssl_resnet18 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'ssl_resnet18' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/abs-1905-00546 , author = {I. Zeki Yalniz and Herv{\\'{e}} J{\\'{e}}gou and Kan Chen and Manohar Paluri and Dhruv Mahajan} , title = {Billion-scale semi-supervised learning for image classification} , journal = {CoRR} , volume = {abs/1905.00546} , year = {2019} , url = {http://arxiv.org/abs/1905.00546} , archivePrefix = {arXiv} , eprint = {1905.00546} , timestamp = {Mon, 28 Sep 2020 08:19:37 +0200} , biburl = {https://dblp.org/rec/journals/corr/abs-1905-00546.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"SSL ResNet"},{"location":"models/ssl-resnet/#ssl-resnet","text":"Residual Networks , or ResNets , learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack residual blocks ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. The model in this collection utilises semi-supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.","title":"SSL ResNet"},{"location":"models/ssl-resnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'ssl_resnet18' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. ssl_resnet18 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/ssl-resnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'ssl_resnet18' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/ssl-resnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/ssl-resnet/#citation","text":"@article { DBLP:journals/corr/abs-1905-00546 , author = {I. Zeki Yalniz and Herv{\\'{e}} J{\\'{e}}gou and Kan Chen and Manohar Paluri and Dhruv Mahajan} , title = {Billion-scale semi-supervised learning for image classification} , journal = {CoRR} , volume = {abs/1905.00546} , year = {2019} , url = {http://arxiv.org/abs/1905.00546} , archivePrefix = {arXiv} , eprint = {1905.00546} , timestamp = {Mon, 28 Sep 2020 08:19:37 +0200} , biburl = {https://dblp.org/rec/journals/corr/abs-1905-00546.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/ssl-resnext/","text":"SSL ResNeXT A ResNeXt repeats a building block that aggregates a set of transformations with the same topology. Compared to a ResNet , it exposes a new dimension, cardinality (the size of the set of transformations) C C , as an essential factor in addition to the dimensions of depth and width. The model in this collection utilises semi-supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'ssl_resnext101_32x16d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. ssl_resnext101_32x16d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'ssl_resnext101_32x16d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/abs-1905-00546 , author = {I. Zeki Yalniz and Herv{\\'{e}} J{\\'{e}}gou and Kan Chen and Manohar Paluri and Dhruv Mahajan} , title = {Billion-scale semi-supervised learning for image classification} , journal = {CoRR} , volume = {abs/1905.00546} , year = {2019} , url = {http://arxiv.org/abs/1905.00546} , archivePrefix = {arXiv} , eprint = {1905.00546} , timestamp = {Mon, 28 Sep 2020 08:19:37 +0200} , biburl = {https://dblp.org/rec/journals/corr/abs-1905-00546.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"SSL ResNeXT"},{"location":"models/ssl-resnext/#ssl-resnext","text":"A ResNeXt repeats a building block that aggregates a set of transformations with the same topology. Compared to a ResNet , it exposes a new dimension, cardinality (the size of the set of transformations) C C , as an essential factor in addition to the dimensions of depth and width. The model in this collection utilises semi-supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.","title":"SSL ResNeXT"},{"location":"models/ssl-resnext/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'ssl_resnext101_32x16d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. ssl_resnext101_32x16d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/ssl-resnext/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'ssl_resnext101_32x16d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/ssl-resnext/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/ssl-resnext/#citation","text":"@article { DBLP:journals/corr/abs-1905-00546 , author = {I. Zeki Yalniz and Herv{\\'{e}} J{\\'{e}}gou and Kan Chen and Manohar Paluri and Dhruv Mahajan} , title = {Billion-scale semi-supervised learning for image classification} , journal = {CoRR} , volume = {abs/1905.00546} , year = {2019} , url = {http://arxiv.org/abs/1905.00546} , archivePrefix = {arXiv} , eprint = {1905.00546} , timestamp = {Mon, 28 Sep 2020 08:19:37 +0200} , biburl = {https://dblp.org/rec/journals/corr/abs-1905-00546.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/swsl-resnet/","text":"SWSL ResNet Residual Networks , or ResNets , learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack residual blocks ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. The models in this collection utilise semi-weakly supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'swsl_resnet18' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. swsl_resnet18 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'swsl_resnet18' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/abs-1905-00546 , author = {I. Zeki Yalniz and Herv{\\'{e}} J{\\'{e}}gou and Kan Chen and Manohar Paluri and Dhruv Mahajan} , title = {Billion-scale semi-supervised learning for image classification} , journal = {CoRR} , volume = {abs/1905.00546} , year = {2019} , url = {http://arxiv.org/abs/1905.00546} , archivePrefix = {arXiv} , eprint = {1905.00546} , timestamp = {Mon, 28 Sep 2020 08:19:37 +0200} , biburl = {https://dblp.org/rec/journals/corr/abs-1905-00546.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"SWSL ResNet"},{"location":"models/swsl-resnet/#swsl-resnet","text":"Residual Networks , or ResNets , learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack residual blocks ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. The models in this collection utilise semi-weakly supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.","title":"SWSL ResNet"},{"location":"models/swsl-resnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'swsl_resnet18' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. swsl_resnet18 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/swsl-resnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'swsl_resnet18' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/swsl-resnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/swsl-resnet/#citation","text":"@article { DBLP:journals/corr/abs-1905-00546 , author = {I. Zeki Yalniz and Herv{\\'{e}} J{\\'{e}}gou and Kan Chen and Manohar Paluri and Dhruv Mahajan} , title = {Billion-scale semi-supervised learning for image classification} , journal = {CoRR} , volume = {abs/1905.00546} , year = {2019} , url = {http://arxiv.org/abs/1905.00546} , archivePrefix = {arXiv} , eprint = {1905.00546} , timestamp = {Mon, 28 Sep 2020 08:19:37 +0200} , biburl = {https://dblp.org/rec/journals/corr/abs-1905-00546.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/swsl-resnext/","text":"SWSL ResNeXt A ResNeXt repeats a building block that aggregates a set of transformations with the same topology. Compared to a ResNet , it exposes a new dimension, cardinality (the size of the set of transformations) C C , as an essential factor in addition to the dimensions of depth and width. The models in this collection utilise semi-weakly supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'swsl_resnext101_32x16d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. swsl_resnext101_32x16d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'swsl_resnext101_32x16d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/abs-1905-00546 , author = {I. Zeki Yalniz and Herv{\\'{e}} J{\\'{e}}gou and Kan Chen and Manohar Paluri and Dhruv Mahajan} , title = {Billion-scale semi-supervised learning for image classification} , journal = {CoRR} , volume = {abs/1905.00546} , year = {2019} , url = {http://arxiv.org/abs/1905.00546} , archivePrefix = {arXiv} , eprint = {1905.00546} , timestamp = {Mon, 28 Sep 2020 08:19:37 +0200} , biburl = {https://dblp.org/rec/journals/corr/abs-1905-00546.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"SWSL ResNeXt"},{"location":"models/swsl-resnext/#swsl-resnext","text":"A ResNeXt repeats a building block that aggregates a set of transformations with the same topology. Compared to a ResNet , it exposes a new dimension, cardinality (the size of the set of transformations) C C , as an essential factor in addition to the dimensions of depth and width. The models in this collection utilise semi-weakly supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.","title":"SWSL ResNeXt"},{"location":"models/swsl-resnext/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'swsl_resnext101_32x16d' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. swsl_resnext101_32x16d . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/swsl-resnext/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'swsl_resnext101_32x16d' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/swsl-resnext/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/swsl-resnext/#citation","text":"@article { DBLP:journals/corr/abs-1905-00546 , author = {I. Zeki Yalniz and Herv{\\'{e}} J{\\'{e}}gou and Kan Chen and Manohar Paluri and Dhruv Mahajan} , title = {Billion-scale semi-supervised learning for image classification} , journal = {CoRR} , volume = {abs/1905.00546} , year = {2019} , url = {http://arxiv.org/abs/1905.00546} , archivePrefix = {arXiv} , eprint = {1905.00546} , timestamp = {Mon, 28 Sep 2020 08:19:37 +0200} , biburl = {https://dblp.org/rec/journals/corr/abs-1905-00546.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/tf-efficientnet-condconv/","text":"(Tensorflow) EfficientNet CondConv EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient . Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use 2^N 2^N times more computational resources, then we can simply increase the network depth by \\alpha ^ N \\alpha ^ N , width by \\beta ^ N \\beta ^ N , and image size by \\gamma ^ N \\gamma ^ N , where \\alpha, \\beta, \\gamma \\alpha, \\beta, \\gamma are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\phi \\phi to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2 , in addition to squeeze-and-excitation blocks. This collection of models amends EfficientNet by adding CondConv convolutions. The weights from this model were ported from Tensorflow/TPU . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'tf_efficientnet_cc_b0_4e' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tf_efficientnet_cc_b0_4e . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tf_efficientnet_cc_b0_4e' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/abs-1904-04971 , author = {Brandon Yang and Gabriel Bender and Quoc V. Le and Jiquan Ngiam} , title = {Soft Conditional Computation} , journal = {CoRR} , volume = {abs/1904.04971} , year = {2019} , url = {http://arxiv.org/abs/1904.04971} , archivePrefix = {arXiv} , eprint = {1904.04971} , timestamp = {Thu, 25 Apr 2019 13:55:01 +0200} , biburl = {https://dblp.org/rec/journals/corr/abs-1904-04971.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"(Tensorflow) EfficientNet CondConv"},{"location":"models/tf-efficientnet-condconv/#tensorflow-efficientnet-condconv","text":"EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient . Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use 2^N 2^N times more computational resources, then we can simply increase the network depth by \\alpha ^ N \\alpha ^ N , width by \\beta ^ N \\beta ^ N , and image size by \\gamma ^ N \\gamma ^ N , where \\alpha, \\beta, \\gamma \\alpha, \\beta, \\gamma are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\phi \\phi to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2 , in addition to squeeze-and-excitation blocks. This collection of models amends EfficientNet by adding CondConv convolutions. The weights from this model were ported from Tensorflow/TPU .","title":"(Tensorflow) EfficientNet CondConv"},{"location":"models/tf-efficientnet-condconv/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'tf_efficientnet_cc_b0_4e' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tf_efficientnet_cc_b0_4e . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/tf-efficientnet-condconv/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tf_efficientnet_cc_b0_4e' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/tf-efficientnet-condconv/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/tf-efficientnet-condconv/#citation","text":"@article { DBLP:journals/corr/abs-1904-04971 , author = {Brandon Yang and Gabriel Bender and Quoc V. Le and Jiquan Ngiam} , title = {Soft Conditional Computation} , journal = {CoRR} , volume = {abs/1904.04971} , year = {2019} , url = {http://arxiv.org/abs/1904.04971} , archivePrefix = {arXiv} , eprint = {1904.04971} , timestamp = {Thu, 25 Apr 2019 13:55:01 +0200} , biburl = {https://dblp.org/rec/journals/corr/abs-1904-04971.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/tf-efficientnet-lite/","text":"(Tensorflow) EfficientNet Lite EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient . Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use 2^N 2^N times more computational resources, then we can simply increase the network depth by \\alpha ^ N \\alpha ^ N , width by \\beta ^ N \\beta ^ N , and image size by \\gamma ^ N \\gamma ^ N , where \\alpha, \\beta, \\gamma \\alpha, \\beta, \\gamma are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\phi \\phi to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2 . EfficientNet-Lite makes EfficientNet more suitable for mobile devices by introducing ReLU6 activation functions and removing squeeze-and-excitation blocks . The weights from this model were ported from Tensorflow/TPU . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'tf_efficientnet_lite0' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tf_efficientnet_lite0 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tf_efficientnet_lite0' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { tan2020efficientnet , title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks} , author = {Mingxing Tan and Quoc V. Le} , year = {2020} , eprint = {1905.11946} , archivePrefix = {arXiv} , primaryClass = {cs.LG} }","title":"(Tensorflow) EfficientNet Lite"},{"location":"models/tf-efficientnet-lite/#tensorflow-efficientnet-lite","text":"EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient . Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use 2^N 2^N times more computational resources, then we can simply increase the network depth by \\alpha ^ N \\alpha ^ N , width by \\beta ^ N \\beta ^ N , and image size by \\gamma ^ N \\gamma ^ N , where \\alpha, \\beta, \\gamma \\alpha, \\beta, \\gamma are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\phi \\phi to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2 . EfficientNet-Lite makes EfficientNet more suitable for mobile devices by introducing ReLU6 activation functions and removing squeeze-and-excitation blocks . The weights from this model were ported from Tensorflow/TPU .","title":"(Tensorflow) EfficientNet Lite"},{"location":"models/tf-efficientnet-lite/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'tf_efficientnet_lite0' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tf_efficientnet_lite0 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/tf-efficientnet-lite/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tf_efficientnet_lite0' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/tf-efficientnet-lite/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/tf-efficientnet-lite/#citation","text":"@misc { tan2020efficientnet , title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks} , author = {Mingxing Tan and Quoc V. Le} , year = {2020} , eprint = {1905.11946} , archivePrefix = {arXiv} , primaryClass = {cs.LG} }","title":"Citation"},{"location":"models/tf-efficientnet/","text":"(Tensorflow) EfficientNet EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient . Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use 2^N 2^N times more computational resources, then we can simply increase the network depth by \\alpha ^ N \\alpha ^ N , width by \\beta ^ N \\beta ^ N , and image size by \\gamma ^ N \\gamma ^ N , where \\alpha, \\beta, \\gamma \\alpha, \\beta, \\gamma are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\phi \\phi to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2 , in addition to squeeze-and-excitation blocks . The weights from this model were ported from Tensorflow/TPU . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'tf_efficientnet_b0' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tf_efficientnet_b0 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tf_efficientnet_b0' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { tan2020efficientnet , title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks} , author = {Mingxing Tan and Quoc V. Le} , year = {2020} , eprint = {1905.11946} , archivePrefix = {arXiv} , primaryClass = {cs.LG} }","title":"(Tensorflow) EfficientNet"},{"location":"models/tf-efficientnet/#tensorflow-efficientnet","text":"EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient . Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use 2^N 2^N times more computational resources, then we can simply increase the network depth by \\alpha ^ N \\alpha ^ N , width by \\beta ^ N \\beta ^ N , and image size by \\gamma ^ N \\gamma ^ N , where \\alpha, \\beta, \\gamma \\alpha, \\beta, \\gamma are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\phi \\phi to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2 , in addition to squeeze-and-excitation blocks . The weights from this model were ported from Tensorflow/TPU .","title":"(Tensorflow) EfficientNet"},{"location":"models/tf-efficientnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'tf_efficientnet_b0' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tf_efficientnet_b0 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/tf-efficientnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tf_efficientnet_b0' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/tf-efficientnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/tf-efficientnet/#citation","text":"@misc { tan2020efficientnet , title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks} , author = {Mingxing Tan and Quoc V. Le} , year = {2020} , eprint = {1905.11946} , archivePrefix = {arXiv} , primaryClass = {cs.LG} }","title":"Citation"},{"location":"models/tf-inception-v3/","text":"(Tensorflow) Inception v3 Inception v3 is a convolutional neural network architecture from the Inception family that makes several improvements including using Label Smoothing , Factorized 7 x 7 convolutions, and the use of an auxiliary classifer to propagate label information lower down the network (along with the use of batch normalization for layers in the sidehead). The key building block is an Inception Module . The weights from this model were ported from Tensorflow/Models . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'tf_inception_v3' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tf_inception_v3 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tf_inception_v3' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/SzegedyVISW15 , author = {Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna} , title = {Rethinking the Inception Architecture for Computer Vision} , journal = {CoRR} , volume = {abs/1512.00567} , year = {2015} , url = {http://arxiv.org/abs/1512.00567} , archivePrefix = {arXiv} , eprint = {1512.00567} , timestamp = {Mon, 13 Aug 2018 16:49:07 +0200} , biburl = {https://dblp.org/rec/journals/corr/SzegedyVISW15.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"(Tensorflow) Inception v3"},{"location":"models/tf-inception-v3/#tensorflow-inception-v3","text":"Inception v3 is a convolutional neural network architecture from the Inception family that makes several improvements including using Label Smoothing , Factorized 7 x 7 convolutions, and the use of an auxiliary classifer to propagate label information lower down the network (along with the use of batch normalization for layers in the sidehead). The key building block is an Inception Module . The weights from this model were ported from Tensorflow/Models .","title":"(Tensorflow) Inception v3"},{"location":"models/tf-inception-v3/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'tf_inception_v3' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tf_inception_v3 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/tf-inception-v3/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tf_inception_v3' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/tf-inception-v3/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/tf-inception-v3/#citation","text":"@article { DBLP:journals/corr/SzegedyVISW15 , author = {Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna} , title = {Rethinking the Inception Architecture for Computer Vision} , journal = {CoRR} , volume = {abs/1512.00567} , year = {2015} , url = {http://arxiv.org/abs/1512.00567} , archivePrefix = {arXiv} , eprint = {1512.00567} , timestamp = {Mon, 13 Aug 2018 16:49:07 +0200} , biburl = {https://dblp.org/rec/journals/corr/SzegedyVISW15.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/tf-mixnet/","text":"(Tensorflow) MixNet MixNet is a type of convolutional neural network discovered via AutoML that utilises MixConvs instead of regular depthwise convolutions . The weights from this model were ported from Tensorflow/TPU . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'tf_mixnet_l' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tf_mixnet_l . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tf_mixnet_l' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { tan2019mixconv , title = {MixConv: Mixed Depthwise Convolutional Kernels} , author = {Mingxing Tan and Quoc V. Le} , year = {2019} , eprint = {1907.09595} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"(Tensorflow) MixNet"},{"location":"models/tf-mixnet/#tensorflow-mixnet","text":"MixNet is a type of convolutional neural network discovered via AutoML that utilises MixConvs instead of regular depthwise convolutions . The weights from this model were ported from Tensorflow/TPU .","title":"(Tensorflow) MixNet"},{"location":"models/tf-mixnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'tf_mixnet_l' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tf_mixnet_l . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/tf-mixnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tf_mixnet_l' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/tf-mixnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/tf-mixnet/#citation","text":"@misc { tan2019mixconv , title = {MixConv: Mixed Depthwise Convolutional Kernels} , author = {Mingxing Tan and Quoc V. Le} , year = {2019} , eprint = {1907.09595} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/tf-mobilenet-v3/","text":"(Tensorflow) MobileNet v3 MobileNetV3 is a convolutional neural network that is designed for mobile phone CPUs. The network design includes the use of a hard swish activation and squeeze-and-excitation modules in the MBConv blocks . The weights from this model were ported from Tensorflow/Models . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'tf_mobilenetv3_large_075' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tf_mobilenetv3_large_075 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tf_mobilenetv3_large_075' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/abs-1905-02244 , author = {Andrew Howard and Mark Sandler and Grace Chu and Liang{-}Chieh Chen and Bo Chen and Mingxing Tan and Weijun Wang and Yukun Zhu and Ruoming Pang and Vijay Vasudevan and Quoc V. Le and Hartwig Adam} , title = {Searching for MobileNetV3} , journal = {CoRR} , volume = {abs/1905.02244} , year = {2019} , url = {http://arxiv.org/abs/1905.02244} , archivePrefix = {arXiv} , eprint = {1905.02244} , timestamp = {Tue, 12 Jan 2021 15:30:06 +0100} , biburl = {https://dblp.org/rec/journals/corr/abs-1905-02244.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"(Tensorflow) MobileNet v3"},{"location":"models/tf-mobilenet-v3/#tensorflow-mobilenet-v3","text":"MobileNetV3 is a convolutional neural network that is designed for mobile phone CPUs. The network design includes the use of a hard swish activation and squeeze-and-excitation modules in the MBConv blocks . The weights from this model were ported from Tensorflow/Models .","title":"(Tensorflow) MobileNet v3"},{"location":"models/tf-mobilenet-v3/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'tf_mobilenetv3_large_075' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tf_mobilenetv3_large_075 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/tf-mobilenet-v3/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tf_mobilenetv3_large_075' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/tf-mobilenet-v3/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/tf-mobilenet-v3/#citation","text":"@article { DBLP:journals/corr/abs-1905-02244 , author = {Andrew Howard and Mark Sandler and Grace Chu and Liang{-}Chieh Chen and Bo Chen and Mingxing Tan and Weijun Wang and Yukun Zhu and Ruoming Pang and Vijay Vasudevan and Quoc V. Le and Hartwig Adam} , title = {Searching for MobileNetV3} , journal = {CoRR} , volume = {abs/1905.02244} , year = {2019} , url = {http://arxiv.org/abs/1905.02244} , archivePrefix = {arXiv} , eprint = {1905.02244} , timestamp = {Tue, 12 Jan 2021 15:30:06 +0100} , biburl = {https://dblp.org/rec/journals/corr/abs-1905-02244.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/tresnet/","text":"TResNet A TResNet is a variant on a ResNet that aim to boost accuracy while maintaining GPU training and inference efficiency. They contain several design tricks including a SpaceToDepth stem, Anti-Alias downsampling , In-Place Activated BatchNorm, Blocks selection and squeeze-and-excitation layers . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'tresnet_l' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tresnet_l . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tresnet_l' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { ridnik2020tresnet , title = {TResNet: High Performance GPU-Dedicated Architecture} , author = {Tal Ridnik and Hussam Lawen and Asaf Noy and Emanuel Ben Baruch and Gilad Sharir and Itamar Friedman} , year = {2020} , eprint = {2003.13630} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"TResNet"},{"location":"models/tresnet/#tresnet","text":"A TResNet is a variant on a ResNet that aim to boost accuracy while maintaining GPU training and inference efficiency. They contain several design tricks including a SpaceToDepth stem, Anti-Alias downsampling , In-Place Activated BatchNorm, Blocks selection and squeeze-and-excitation layers .","title":"TResNet"},{"location":"models/tresnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'tresnet_l' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. tresnet_l . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/tresnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'tresnet_l' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/tresnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/tresnet/#citation","text":"@misc { ridnik2020tresnet , title = {TResNet: High Performance GPU-Dedicated Architecture} , author = {Tal Ridnik and Hussam Lawen and Asaf Noy and Emanuel Ben Baruch and Gilad Sharir and Itamar Friedman} , year = {2020} , eprint = {2003.13630} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/vision-transformer/","text":"Vision Transformer (ViT) The Vision Transformer is a model for image classification that employs a Transformer-like architecture over patches of the image. This includes the use of Multi-Head Attention , Scaled Dot-Product Attention and other architectural features seen in the Transformer architecture traditionally used for NLP. How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'vit_base_patch16_224' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. vit_base_patch16_224 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'vit_base_patch16_224' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @misc { dosovitskiy2020image , title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale} , author = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby} , year = {2020} , eprint = {2010.11929} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Vision Transformer (ViT)"},{"location":"models/vision-transformer/#vision-transformer-vit","text":"The Vision Transformer is a model for image classification that employs a Transformer-like architecture over patches of the image. This includes the use of Multi-Head Attention , Scaled Dot-Product Attention and other architectural features seen in the Transformer architecture traditionally used for NLP.","title":"Vision Transformer (ViT)"},{"location":"models/vision-transformer/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'vit_base_patch16_224' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. vit_base_patch16_224 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/vision-transformer/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'vit_base_patch16_224' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/vision-transformer/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/vision-transformer/#citation","text":"@misc { dosovitskiy2020image , title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale} , author = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby} , year = {2020} , eprint = {2010.11929} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"},{"location":"models/wide-resnet/","text":"Wide ResNet Wide Residual Networks are a variant on ResNets where we decrease depth and increase the width of residual networks. This is achieved through the use of wide residual blocks . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'wide_resnet101_2' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. wide_resnet101_2 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'wide_resnet101_2' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/ZagoruykoK16 , author = {Sergey Zagoruyko and Nikos Komodakis} , title = {Wide Residual Networks} , journal = {CoRR} , volume = {abs/1605.07146} , year = {2016} , url = {http://arxiv.org/abs/1605.07146} , archivePrefix = {arXiv} , eprint = {1605.07146} , timestamp = {Mon, 13 Aug 2018 16:46:42 +0200} , biburl = {https://dblp.org/rec/journals/corr/ZagoruykoK16.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Wide ResNet"},{"location":"models/wide-resnet/#wide-resnet","text":"Wide Residual Networks are a variant on ResNets where we decrease depth and increase the width of residual networks. This is achieved through the use of wide residual blocks .","title":"Wide ResNet"},{"location":"models/wide-resnet/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'wide_resnet101_2' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. wide_resnet101_2 . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/wide-resnet/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'wide_resnet101_2' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/wide-resnet/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/wide-resnet/#citation","text":"@article { DBLP:journals/corr/ZagoruykoK16 , author = {Sergey Zagoruyko and Nikos Komodakis} , title = {Wide Residual Networks} , journal = {CoRR} , volume = {abs/1605.07146} , year = {2016} , url = {http://arxiv.org/abs/1605.07146} , archivePrefix = {arXiv} , eprint = {1605.07146} , timestamp = {Mon, 13 Aug 2018 16:46:42 +0200} , biburl = {https://dblp.org/rec/journals/corr/ZagoruykoK16.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Citation"},{"location":"models/xception/","text":"Xception Xception is a convolutional neural network architecture that relies solely on depthwise separable convolution layers . The weights from this model were ported from Tensorflow/Models . How do I use this model on an image? To load a pretrained model: import timm model = timm . create_model ( 'xception' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. xception . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use. How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'xception' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset. How do I train this model? You can follow the timm recipe scripts for training a new model afresh. Citation @article { DBLP:journals/corr/ZagoruykoK16 , @misc {chollet2017xception, title = {Xception: Deep Learning with Depthwise Separable Convolutions} , author = {Fran\u00e7ois Chollet} , year = {2017} , eprint = {1610.02357} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Xception"},{"location":"models/xception/#xception","text":"Xception is a convolutional neural network architecture that relies solely on depthwise separable convolution layers . The weights from this model were ported from Tensorflow/Models .","title":"Xception"},{"location":"models/xception/#how-do-i-use-this-model-on-an-image","text":"To load a pretrained model: import timm model = timm . create_model ( 'xception' , pretrained = True ) model . eval () To load and preprocess the image: import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config ({}, model = model ) transform = create_transform ( ** config ) url , filename = ( \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\" , \"dog.jpg\" ) urllib . request . urlretrieve ( url , filename ) img = Image . open ( filename ) . convert ( 'RGB' ) tensor = transform ( img ) . unsqueeze ( 0 ) # transform and add batch dimension To get the model predictions: import torch with torch . no_grad (): out = model ( tensor ) probabilities = torch . nn . functional . softmax ( out [ 0 ], dim = 0 ) print ( probabilities . shape ) # prints: torch.Size([1000]) To get the top-5 predictions class names: # Get imagenet class mappings url , filename = ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" , \"imagenet_classes.txt\" ) urllib . request . urlretrieve ( url , filename ) with open ( \"imagenet_classes.txt\" , \"r\" ) as f : categories = [ s . strip () for s in f . readlines ()] # Print top categories per image top5_prob , top5_catid = torch . topk ( probabilities , 5 ) for i in range ( top5_prob . size ( 0 )): print ( categories [ top5_catid [ i ]], top5_prob [ i ] . item ()) # prints class names and probabilities like: # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)] Replace the model name with the variant you want to use, e.g. xception . You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the timm feature extraction examples , just change the name of the model you want to use.","title":"How do I use this model on an image?"},{"location":"models/xception/#how-do-i-finetune-this-model","text":"You can finetune any of the pre-trained models just by changing the classifier (the last layer). model = timm . create_model ( 'xception' , pretrained = True , num_classes = NUM_FINETUNE_CLASSES ) To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.","title":"How do I finetune this model?"},{"location":"models/xception/#how-do-i-train-this-model","text":"You can follow the timm recipe scripts for training a new model afresh.","title":"How do I train this model?"},{"location":"models/xception/#citation","text":"@article { DBLP:journals/corr/ZagoruykoK16 , @misc {chollet2017xception, title = {Xception: Deep Learning with Depthwise Separable Convolutions} , author = {Fran\u00e7ois Chollet} , year = {2017} , eprint = {1610.02357} , archivePrefix = {arXiv} , primaryClass = {cs.CV} }","title":"Citation"}]}