{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":""},{"location":"#welcome","title":"Welcome","text":"<p>Welcome to the <code>timm</code> documentation, a lean set of docs that covers the basics of <code>timm</code>.</p> <p>For a more comprehensive set of docs (currently under development), please visit timmdocs by Aman Arora.</p>"},{"location":"#install","title":"Install","text":"<p>The library can be installed with pip:</p> <pre><code>pip install timm\n</code></pre> <p>I update the PyPi (pip) packages when I'm confident there are no significant model regressions from previous releases. If you want to pip install the bleeding edge from GitHub, use: <pre><code>pip install git+https://github.com/rwightman/pytorch-image-models.git\n</code></pre></p>  <p>Conda Environment</p> <p>All development and testing has been done in Conda Python 3 environments on Linux x86-64 systems, specifically 3.7, 3.8, 3.9, 3.10</p> <p>Little to no care has been taken to be Python 2.x friendly and will not support it. If you run into any challenges running on Windows, or other OS, I'm definitely open to looking into those issues so long as it's in a reproducible (read Conda) environment.</p> <p>PyTorch versions 1.9, 1.10, 1.11 have been tested with the latest versions of this code.</p> <p>I've tried to keep the dependencies minimal, the setup is as per the PyTorch default install instructions for Conda: <pre><code>conda create -n torch-env\nconda activate torch-env\nconda install pytorch torchvision cudatoolkit=11.3 -c pytorch\nconda install pyyaml\n</code></pre></p>"},{"location":"#load-a-pretrained-model","title":"Load a Pretrained Model","text":"<p>Pretrained models can be loaded using <code>timm.create_model</code></p> <pre><code>import timm\n\nm = timm.create_model('mobilenetv3_large_100', pretrained=True)\nm.eval()\n</code></pre>"},{"location":"#list-models-with-pretrained-weights","title":"List Models with Pretrained Weights","text":"<pre><code>import timm\nfrom pprint import pprint\nmodel_names = timm.list_models(pretrained=True)\npprint(model_names)\n&gt;&gt;&gt; ['adv_inception_v3',\n 'cspdarknet53',\n 'cspresnext50',\n 'densenet121',\n 'densenet161',\n 'densenet169',\n 'densenet201',\n 'densenetblur121d',\n 'dla34',\n 'dla46_c',\n...\n]\n</code></pre>"},{"location":"#list-model-architectures-by-wildcard","title":"List Model Architectures by Wildcard","text":"<pre><code>import timm\nfrom pprint import pprint\nmodel_names = timm.list_models('*resne*t*')\npprint(model_names)\n&gt;&gt;&gt; ['cspresnet50',\n 'cspresnet50d',\n 'cspresnet50w',\n 'cspresnext50',\n...\n]\n</code></pre>"},{"location":"archived_changes/","title":"Archived Changes","text":""},{"location":"archived_changes/#nov-22-2021","title":"Nov 22, 2021","text":"<ul> <li>A number of updated weights anew new model defs<ul> <li><code>eca_halonext26ts</code> - 79.5 @ 256</li> <li><code>resnet50_gn</code> (new) - 80.1 @ 224, 81.3 @ 288</li> <li><code>resnet50</code> - 80.7 @ 224, 80.9 @ 288 (trained at 176, not replacing current a1 weights as default since these don't scale as well to higher res, weights)</li> <li><code>resnext50_32x4d</code> - 81.1 @ 224, 82.0 @ 288</li> <li><code>sebotnet33ts_256</code> (new) - 81.2 @ 224</li> <li><code>lamhalobotnet50ts_256</code> - 81.5 @ 256</li> <li><code>halonet50ts</code> - 81.7 @ 256</li> <li><code>halo2botnet50ts_256</code> - 82.0 @ 256</li> <li><code>resnet101</code> - 82.0 @ 224, 82.8 @ 288</li> <li><code>resnetv2_101</code> (new) - 82.1 @ 224, 83.0 @ 288</li> <li><code>resnet152</code> - 82.8 @ 224, 83.5 @ 288</li> <li><code>regnetz_d8</code> (new) - 83.5 @ 256, 84.0 @ 320</li> <li><code>regnetz_e8</code> (new) - 84.5 @ 256, 85.0 @ 320</li> </ul> </li> <li><code>vit_base_patch8_224</code> (85.8 top-1) &amp; <code>in21k</code> variant weights added thanks Martins Bruveris</li> <li>Groundwork in for FX feature extraction thanks to Alexander Soare<ul> <li>models updated for tracing compatibility (almost full support with some distlled transformer exceptions)</li> </ul> </li> </ul>"},{"location":"archived_changes/#oct-19-2021","title":"Oct 19, 2021","text":"<ul> <li>ResNet strikes back (https://arxiv.org/abs/2110.00476) weights added, plus any extra training components used. Model weights and some more details here (https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-rsb-weights)</li> <li>BCE loss and Repeated Augmentation support for RSB paper</li> <li>4 series of ResNet based attention model experiments being added (implemented across byobnet.py/byoanet.py). These include all sorts of attention, from channel attn like SE, ECA to 2D QKV self-attention layers such as Halo, Bottlneck, Lambda. Details here (https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights)</li> <li>Working implementations of the following 2D self-attention modules (likely to be differences from paper or eventual official impl):<ul> <li>Halo (https://arxiv.org/abs/2103.12731)</li> <li>Bottleneck Transformer (https://arxiv.org/abs/2101.11605)</li> <li>LambdaNetworks (https://arxiv.org/abs/2102.08602)</li> </ul> </li> <li>A RegNetZ series of models with some attention experiments (being added to). These do not follow the paper (https://arxiv.org/abs/2103.06877) in any way other than block architecture, details of official models are not available. See more here (https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights)</li> <li>ConvMixer (https://openreview.net/forum?id=TVHS5Y4dNvM), CrossVit (https://arxiv.org/abs/2103.14899), and BeiT (https://arxiv.org/abs/2106.08254) architectures + weights added</li> <li>freeze/unfreeze helpers by Alexander Soare</li> </ul>"},{"location":"archived_changes/#aug-18-2021","title":"Aug 18, 2021","text":"<ul> <li>Optimizer bonanza!<ul> <li>Add LAMB and LARS optimizers, incl trust ratio clipping options. Tweaked to work properly in PyTorch XLA (tested on TPUs w/ <code>timm bits</code> branch)</li> <li>Add MADGRAD from FB research w/ a few tweaks (decoupled decay option, step handling that works with PyTorch XLA)</li> <li>Some cleanup on all optimizers and factory. No more <code>.data</code>, a bit more consistency, unit tests for all!</li> <li>SGDP and AdamP still won't work with PyTorch XLA but others should (have yet to test Adabelief, Adafactor, Adahessian myself).</li> </ul> </li> <li>EfficientNet-V2 XL TF ported weights added, but they don't validate well in PyTorch (L is better). The pre-processing for the V2 TF training is a bit diff and the fine-tuned 21k -&gt; 1k weights are very sensitive and less robust than the 1k weights.</li> <li>Added PyTorch trained EfficientNet-V2 'Tiny' w/ GlobalContext attn weights. Only .1-.2 top-1 better than the SE so more of a curiosity for those interested.</li> </ul>"},{"location":"archived_changes/#july-12-2021","title":"July 12, 2021","text":"<ul> <li>Add XCiT models from official facebook impl. Contributed by Alexander Soare</li> </ul>"},{"location":"archived_changes/#july-5-9-2021","title":"July 5-9, 2021","text":"<ul> <li>Add <code>efficientnetv2_rw_t</code> weights, a custom 'tiny' 13.6M param variant that is a bit better than (non NoisyStudent) B3 models. Both faster and better accuracy (at same or lower res)<ul> <li>top-1 82.34 @ 288x288 and 82.54 @ 320x320</li> </ul> </li> <li>Add SAM pretrained in1k weight for ViT B/16 (<code>vit_base_patch16_sam_224</code>) and B/32 (<code>vit_base_patch32_sam_224</code>)  models.</li> <li>Add 'Aggregating Nested Transformer' (NesT) w/ weights converted from official Flax impl. Contributed by Alexander Soare.<ul> <li><code>jx_nest_base</code> - 83.534, <code>jx_nest_small</code> - 83.120, <code>jx_nest_tiny</code> - 81.426</li> </ul> </li> </ul>"},{"location":"archived_changes/#june-23-2021","title":"June 23, 2021","text":"<ul> <li>Reproduce gMLP model training, <code>gmlp_s16_224</code> trained to 79.6 top-1, matching paper. Hparams for this and other recent MLP training here</li> </ul>"},{"location":"archived_changes/#june-20-2021","title":"June 20, 2021","text":"<ul> <li>Release Vision Transformer 'AugReg' weights from How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers<ul> <li>.npz weight loading support added, can load any of the 50K+ weights from the AugReg series</li> <li>See example notebook from official impl for navigating the augreg weights</li> <li>Replaced all default weights w/ best AugReg variant (if possible). All AugReg 21k classifiers work.<ul> <li>Highlights: <code>vit_large_patch16_384</code> (87.1 top-1), <code>vit_large_r50_s32_384</code> (86.2 top-1), <code>vit_base_patch16_384</code> (86.0 top-1)</li> </ul> </li> <li><code>vit_deit_*</code> renamed to just <code>deit_*</code></li> <li>Remove my old small model, replace with DeiT compatible small w/ AugReg weights</li> </ul> </li> <li>Add 1st training of my <code>gmixer_24_224</code> MLP /w GLU, 78.1 top-1 w/ 25M params.</li> <li>Add weights from official ResMLP release (https://github.com/facebookresearch/deit)</li> <li>Add <code>eca_nfnet_l2</code> weights from my 'lightweight' series. 84.7 top-1 at 384x384.</li> <li>Add distilled BiT 50x1 student and 152x2 Teacher weights from  Knowledge distillation: A good teacher is patient and consistent</li> <li>NFNets and ResNetV2-BiT models work w/ Pytorch XLA now<ul> <li>weight standardization uses F.batch_norm instead of std_mean (std_mean wasn't lowered)</li> <li>eps values adjusted, will be slight differences but should be quite close</li> </ul> </li> <li>Improve test coverage and classifier interface of non-conv (vision transformer and mlp) models</li> <li>Cleanup a few classifier / flatten details for models w/ conv classifiers or early global pool</li> <li>Please report any regressions, this PR touched quite a few models.</li> </ul>"},{"location":"archived_changes/#june-8-2021","title":"June 8, 2021","text":"<ul> <li>Add first ResMLP weights, trained in PyTorch XLA on TPU-VM w/ my XLA branch. 24 block variant, 79.2 top-1.</li> <li>Add ResNet51-Q model w/ pretrained weights at 82.36 top-1.<ul> <li>NFNet inspired block layout with quad layer stem and no maxpool</li> <li>Same param count (35.7M) and throughput as ResNetRS-50 but +1.5 top-1 @ 224x224 and +2.5 top-1 at 288x288</li> </ul> </li> </ul>"},{"location":"archived_changes/#may-25-2021","title":"May 25, 2021","text":"<ul> <li>Add LeViT, Visformer, Convit (PR by Aman Arora), Twins (PR by paper authors) transformer models</li> <li>Cleanup input_size/img_size override handling and testing for all vision transformer models</li> <li>Add <code>efficientnetv2_rw_m</code> model and weights (started training before official code). 84.8 top-1, 53M params.</li> </ul>"},{"location":"archived_changes/#may-14-2021","title":"May 14, 2021","text":"<ul> <li>Add EfficientNet-V2 official model defs w/ ported weights from official Tensorflow/Keras impl.<ul> <li>1k trained variants: <code>tf_efficientnetv2_s/m/l</code></li> <li>21k trained variants: <code>tf_efficientnetv2_s/m/l_in21k</code></li> <li>21k pretrained -&gt; 1k fine-tuned: <code>tf_efficientnetv2_s/m/l_in21ft1k</code></li> <li>v2 models w/ v1 scaling: <code>tf_efficientnetv2_b0</code> through <code>b3</code></li> <li>Rename my prev V2 guess <code>efficientnet_v2s</code> -&gt; <code>efficientnetv2_rw_s</code></li> <li>Some blank <code>efficientnetv2_*</code> models in-place for future native PyTorch training</li> </ul> </li> </ul>"},{"location":"archived_changes/#may-5-2021","title":"May 5, 2021","text":"<ul> <li>Add MLP-Mixer models and port pretrained weights from Google JAX impl</li> <li>Add CaiT models and pretrained weights from FB</li> <li>Add ResNet-RS models and weights from TF. Thanks Aman Arora</li> <li>Add CoaT models and weights. Thanks Mohammed Rizin</li> <li>Add new ImageNet-21k weights &amp; finetuned weights for TResNet, MobileNet-V3, ViT models. Thanks mrT</li> <li>Add GhostNet models and weights. Thanks Kai Han</li> <li>Update ByoaNet attention modles<ul> <li>Improve SA module inits</li> <li>Hack together experimental stand-alone Swin based attn module and <code>swinnet</code></li> <li>Consistent '26t' model defs for experiments.</li> </ul> </li> <li>Add improved Efficientnet-V2S (prelim model def) weights. 83.8 top-1.</li> <li>WandB logging support</li> </ul>"},{"location":"archived_changes/#april-13-2021","title":"April 13, 2021","text":"<ul> <li>Add Swin Transformer models and weights from https://github.com/microsoft/Swin-Transformer</li> </ul>"},{"location":"archived_changes/#april-12-2021","title":"April 12, 2021","text":"<ul> <li>Add ECA-NFNet-L1 (slimmed down F1 w/ SiLU, 41M params) trained with this code. 84% top-1 @ 320x320. Trained at 256x256.</li> <li>Add EfficientNet-V2S model (unverified model definition) weights. 83.3 top-1 @ 288x288. Only trained single res 224. Working on progressive training.</li> <li>Add ByoaNet model definition (Bring-your-own-attention) w/ SelfAttention block and corresponding SA/SA-like modules and model defs<ul> <li>Lambda Networks - https://arxiv.org/abs/2102.08602</li> <li>Bottleneck Transformers - https://arxiv.org/abs/2101.11605</li> <li>Halo Nets - https://arxiv.org/abs/2103.12731</li> </ul> </li> <li>Adabelief optimizer contributed by Juntang Zhuang</li> </ul>"},{"location":"archived_changes/#april-1-2021","title":"April 1, 2021","text":"<ul> <li>Add snazzy <code>benchmark.py</code> script for bulk <code>timm</code> model benchmarking of train and/or inference</li> <li>Add Pooling-based Vision Transformer (PiT) models (from https://github.com/naver-ai/pit)<ul> <li>Merged distilled variant into main for torchscript compatibility</li> <li>Some <code>timm</code> cleanup/style tweaks and weights have hub download support</li> </ul> </li> <li>Cleanup Vision Transformer (ViT) models<ul> <li>Merge distilled (DeiT) model into main so that torchscript can work</li> <li>Support updated weight init (defaults to old still) that closer matches original JAX impl (possibly better training from scratch)</li> <li>Separate hybrid model defs into different file and add several new model defs to fiddle with, support patch_size != 1 for hybrids</li> <li>Fix fine-tuning num_class changes (PiT and ViT) and pos_embed resizing (Vit) with distilled variants</li> <li>nn.Sequential for block stack (does not break downstream compat)</li> </ul> </li> <li>TnT (Transformer-in-Transformer) models contributed by author (from https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/TNT)</li> <li>Add RegNetY-160 weights from DeiT teacher model</li> <li>Add new NFNet-L0 w/ SE attn (rename <code>nfnet_l0b</code>-&gt;<code>nfnet_l0</code>) weights 82.75 top-1 @ 288x288</li> <li>Some fixes/improvements for TFDS dataset wrapper</li> </ul>"},{"location":"archived_changes/#march-7-2021","title":"March 7, 2021","text":"<ul> <li>First 0.4.x PyPi release w/ NFNets (&amp; related), ByoB (GPU-Efficient, RepVGG, etc).</li> <li>Change feature extraction for pre-activation nets (NFNets, ResNetV2) to return features before activation.</li> </ul>"},{"location":"archived_changes/#feb-18-2021","title":"Feb 18, 2021","text":"<ul> <li>Add pretrained weights and model variants for NFNet-F* models from DeepMind Haiku impl.<ul> <li>Models are prefixed with <code>dm_</code>. They require SAME padding conv, skipinit enabled, and activation gains applied in act fn.</li> <li>These models are big, expect to run out of GPU memory. With the GELU activiation + other options, they are roughly \u00bd the inference speed of my SiLU PyTorch optimized <code>s</code> variants.</li> <li>Original model results are based on pre-processing that is not the same as all other models so you'll see different results in the results csv (once updated).</li> <li>Matching the original pre-processing as closely as possible I get these results:<ul> <li><code>dm_nfnet_f6</code> - 86.352</li> <li><code>dm_nfnet_f5</code> - 86.100</li> <li><code>dm_nfnet_f4</code> - 85.834</li> <li><code>dm_nfnet_f3</code> - 85.676</li> <li><code>dm_nfnet_f2</code> - 85.178</li> <li><code>dm_nfnet_f1</code> - 84.696</li> <li><code>dm_nfnet_f0</code> - 83.464</li> </ul> </li> </ul> </li> </ul>"},{"location":"archived_changes/#feb-16-2021","title":"Feb 16, 2021","text":"<ul> <li>Add Adaptive Gradient Clipping (AGC) as per https://arxiv.org/abs/2102.06171. Integrated w/ PyTorch gradient clipping via mode arg that defaults to prev 'norm' mode. For backward arg compat, clip-grad arg must be specified to enable when using train.py.<ul> <li>AGC w/ default clipping factor <code>--clip-grad .01 --clip-mode agc</code></li> <li>PyTorch global norm of 1.0 (old behaviour, always norm), <code>--clip-grad 1.0</code></li> <li>PyTorch value clipping of 10, <code>--clip-grad 10. --clip-mode value</code></li> <li>AGC performance is definitely sensitive to the clipping factor. More experimentation needed to determine good values for smaller batch sizes and optimizers besides those in paper. So far I've found .001-.005 is necessary for stable RMSProp training w/ NFNet/NF-ResNet.</li> </ul> </li> </ul>"},{"location":"archived_changes/#feb-12-2021","title":"Feb 12, 2021","text":"<ul> <li>Update Normalization-Free nets to include new NFNet-F (https://arxiv.org/abs/2102.06171) model defs</li> </ul>"},{"location":"archived_changes/#feb-10-2021","title":"Feb 10, 2021","text":"<ul> <li>More model archs, incl a flexible ByobNet backbone ('Bring-your-own-blocks')<ul> <li>GPU-Efficient-Networks (https://github.com/idstcv/GPU-Efficient-Networks), impl in <code>byobnet.py</code></li> <li>RepVGG (https://github.com/DingXiaoH/RepVGG), impl in <code>byobnet.py</code></li> <li>classic VGG (from torchvision, impl in <code>vgg</code>)</li> </ul> </li> <li>Refinements to normalizer layer arg handling and normalizer+act layer handling in some models</li> <li>Default AMP mode changed to native PyTorch AMP instead of APEX. Issues not being fixed with APEX. Native works with <code>--channels-last</code> and <code>--torchscript</code> model training, APEX does not.</li> <li>Fix a few bugs introduced since last pypi release</li> </ul>"},{"location":"archived_changes/#feb-8-2021","title":"Feb 8, 2021","text":"<ul> <li>Add several ResNet weights with ECA attention. 26t &amp; 50t trained @ 256, test @ 320. 269d train @ 256, fine-tune @320, test @ 352.<ul> <li><code>ecaresnet26t</code> - 79.88 top-1 @ 320x320, 79.08 @ 256x256</li> <li><code>ecaresnet50t</code> - 82.35 top-1 @ 320x320, 81.52 @ 256x256</li> <li><code>ecaresnet269d</code> - 84.93 top-1 @ 352x352, 84.87 @ 320x320</li> </ul> </li> <li>Remove separate tiered (<code>t</code>) vs tiered_narrow (<code>tn</code>) ResNet model defs, all <code>tn</code> changed to <code>t</code> and <code>t</code> models removed (<code>seresnext26t_32x4d</code> only model w/ weights that was removed).</li> <li>Support model default_cfgs with separate train vs test resolution <code>test_input_size</code> and remove extra <code>_320</code> suffix ResNet model defs that were just for test.</li> </ul>"},{"location":"archived_changes/#jan-30-2021","title":"Jan 30, 2021","text":"<ul> <li>Add initial \"Normalization Free\" NF-RegNet-B* and NF-ResNet model definitions based on paper</li> </ul>"},{"location":"archived_changes/#jan-25-2021","title":"Jan 25, 2021","text":"<ul> <li>Add ResNetV2 Big Transfer (BiT) models w/ ImageNet-1k and 21k weights from https://github.com/google-research/big_transfer</li> <li>Add official R50+ViT-B/16 hybrid models + weights from https://github.com/google-research/vision_transformer</li> <li>ImageNet-21k ViT weights are added w/ model defs and representation layer (pre logits) support<ul> <li>NOTE: ImageNet-21k classifier heads were zero'd in original weights, they are only useful for transfer learning</li> </ul> </li> <li>Add model defs and weights for DeiT Vision Transformer models from https://github.com/facebookresearch/deit</li> <li>Refactor dataset classes into ImageDataset/IterableImageDataset + dataset specific parser classes</li> <li>Add Tensorflow-Datasets (TFDS) wrapper to allow use of TFDS image classification sets with train script<ul> <li>Ex: <code>train.py /data/tfds --dataset tfds/oxford_iiit_pet --val-split test --model resnet50 -b 256 --amp --num-classes 37 --opt adamw --lr 3e-4 --weight-decay .001 --pretrained -j 2</code></li> </ul> </li> <li>Add improved .tar dataset parser that reads images from .tar, folder of .tar files, or .tar within .tar<ul> <li>Run validation on full ImageNet-21k directly from tar w/ BiT model: <code>validate.py /data/fall11_whole.tar --model resnetv2_50x1_bitm_in21k --amp</code></li> </ul> </li> <li>Models in this update should be stable w/ possible exception of ViT/BiT, possibility of some regressions with train/val scripts and dataset handling</li> </ul>"},{"location":"archived_changes/#jan-3-2021","title":"Jan 3, 2021","text":"<ul> <li>Add SE-ResNet-152D weights<ul> <li>256x256 val, 0.94 crop top-1 - 83.75</li> <li>320x320 val, 1.0 crop - 84.36</li> </ul> </li> <li>Update results files</li> </ul>"},{"location":"archived_changes/#dec-18-2020","title":"Dec 18, 2020","text":"<ul> <li>Add ResNet-101D, ResNet-152D, and ResNet-200D weights trained @ 256x256<ul> <li>256x256 val, 0.94 crop (top-1) - 101D (82.33), 152D (83.08), 200D (83.25)</li> <li>288x288 val, 1.0 crop - 101D (82.64), 152D (83.48), 200D (83.76)</li> <li>320x320 val, 1.0 crop - 101D (83.00), 152D (83.66), 200D (84.01)</li> </ul> </li> </ul>"},{"location":"archived_changes/#dec-7-2020","title":"Dec 7, 2020","text":"<ul> <li>Simplify EMA module (ModelEmaV2), compatible with fully torchscripted models</li> <li>Misc fixes for SiLU ONNX export, default_cfg missing from Feature extraction models, Linear layer w/ AMP + torchscript</li> <li>PyPi release @ 0.3.2 (needed by EfficientDet)</li> </ul>"},{"location":"archived_changes/#oct-30-2020","title":"Oct 30, 2020","text":"<ul> <li>Test with PyTorch 1.7 and fix a small top-n metric view vs reshape issue.</li> <li>Convert newly added 224x224 Vision Transformer weights from official JAX repo. 81.8 top-1 for B/16, 83.1 L/16.</li> <li>Support PyTorch 1.7 optimized, native SiLU (aka Swish) activation. Add mapping to 'silu' name, custom swish will eventually be deprecated.</li> <li>Fix regression for loading pretrained classifier via direct model entrypoint functions. Didn't impact create_model() factory usage.</li> <li>PyPi release @ 0.3.0 version!</li> </ul>"},{"location":"archived_changes/#oct-26-2020","title":"Oct 26, 2020","text":"<ul> <li>Update Vision Transformer models to be compatible with official code release at https://github.com/google-research/vision_transformer</li> <li>Add Vision Transformer weights (ImageNet-21k pretrain) for 384x384 base and large models converted from official jax impl<ul> <li>ViT-B/16 - 84.2</li> <li>ViT-B/32 - 81.7</li> <li>ViT-L/16 - 85.2</li> <li>ViT-L/32 - 81.5</li> </ul> </li> </ul>"},{"location":"archived_changes/#oct-21-2020","title":"Oct 21, 2020","text":"<ul> <li>Weights added for Vision Transformer (ViT) models. 77.86 top-1 for 'small' and 79.35 for 'base'. Thanks to Christof for training the base model w/ lots of GPUs.</li> </ul>"},{"location":"archived_changes/#oct-13-2020","title":"Oct 13, 2020","text":"<ul> <li>Initial impl of Vision Transformer models. Both patch and hybrid (CNN backbone) variants. Currently trying to train...</li> <li>Adafactor and AdaHessian (FP32 only, no AMP) optimizers</li> <li>EdgeTPU-M (<code>efficientnet_em</code>) model trained in PyTorch, 79.3 top-1</li> <li>Pip release, doc updates pending a few more changes...</li> </ul>"},{"location":"archived_changes/#sept-18-2020","title":"Sept 18, 2020","text":"<ul> <li>New ResNet 'D' weights. 72.7 (top-1) ResNet-18-D, 77.1 ResNet-34-D, 80.5 ResNet-50-D</li> <li>Added a few untrained defs for other ResNet models (66D, 101D, 152D, 200/200D)</li> </ul>"},{"location":"archived_changes/#sept-3-2020","title":"Sept 3, 2020","text":"<ul> <li>New weights<ul> <li>Wide-ResNet50 - 81.5 top-1 (vs 78.5 torchvision)</li> <li>SEResNeXt50-32x4d - 81.3 top-1 (vs 79.1 cadene)</li> </ul> </li> <li>Support for native Torch AMP and channels_last memory format added to train/validate scripts (<code>--channels-last</code>, <code>--native-amp</code> vs <code>--apex-amp</code>)</li> <li>Models tested with channels_last on latest NGC 20.08 container. AdaptiveAvgPool in attn layers changed to mean((2,3)) to work around bug with NHWC kernel.</li> </ul>"},{"location":"archived_changes/#aug-12-2020","title":"Aug 12, 2020","text":"<ul> <li>New/updated weights from training experiments<ul> <li>EfficientNet-B3 - 82.1 top-1 (vs 81.6 for official with AA and 81.9 for AdvProp)</li> <li>RegNetY-3.2GF - 82.0 top-1 (78.9 from official ver)</li> <li>CSPResNet50 - 79.6 top-1 (76.6 from official ver)</li> </ul> </li> <li>Add CutMix integrated w/ Mixup. See pull request for some usage examples</li> <li>Some fixes for using pretrained weights with <code>in_chans</code> != 3 on several models.</li> </ul>"},{"location":"archived_changes/#aug-5-2020","title":"Aug 5, 2020","text":"<p>Universal feature extraction, new models, new weights, new test sets. * All models support the <code>features_only=True</code> argument for <code>create_model</code> call to return a network that extracts feature maps from the deepest layer at each stride. * New models   * CSPResNet, CSPResNeXt, CSPDarkNet, DarkNet   * ReXNet   * (Modified Aligned) Xception41/65/71 (a proper port of TF models) * New trained weights   * SEResNet50 - 80.3 top-1   * CSPDarkNet53 - 80.1 top-1   * CSPResNeXt50 - 80.0 top-1   * DPN68b - 79.2 top-1   * EfficientNet-Lite0 (non-TF ver) - 75.5 (submitted by @hal-314) * Add 'real' labels for ImageNet and ImageNet-Renditions test set, see <code>results/README.md</code> * Test set ranking/top-n diff script by @KushajveerSingh * Train script and loader/transform tweaks to punch through more aug arguments * README and documentation overhaul. See initial (WIP) documentation at https://rwightman.github.io/pytorch-image-models/ * adamp and sgdp optimizers added by @hellbell</p>"},{"location":"archived_changes/#june-11-2020","title":"June 11, 2020","text":"<p>Bunch of changes: * DenseNet models updated with memory efficient addition from torchvision (fixed a bug), blur pooling and deep stem additions * VoVNet V1 and V2 models added, 39 V2 variant (ese_vovnet_39b) trained to 79.3 top-1 * Activation factory added along with new activations:    * select act at model creation time for more flexibility in using activations compatible with scripting or tracing (ONNX export)    * hard_mish (experimental) added with memory-efficient grad, along with ME hard_swish    * context mgr for setting exportable/scriptable/no_jit states * Norm + Activation combo layers added with initial trial support in DenseNet and VoVNet along with impl of EvoNorm and InplaceAbn wrapper that fit the interface * Torchscript works for all but two of the model types as long as using Pytorch 1.5+, tests added for this * Some import cleanup and classifier reset changes, all models will have classifier reset to nn.Identity on reset_classifer(0) call * Prep for 0.1.28 pip release</p>"},{"location":"archived_changes/#may-12-2020","title":"May 12, 2020","text":"<ul> <li>Add ResNeSt models (code adapted from https://github.com/zhanghang1989/ResNeSt, paper https://arxiv.org/abs/2004.08955))</li> </ul>"},{"location":"archived_changes/#may-3-2020","title":"May 3, 2020","text":"<ul> <li>Pruned EfficientNet B1, B2, and B3 (https://arxiv.org/abs/2002.08258) contributed by Yonathan Aflalo</li> </ul>"},{"location":"archived_changes/#may-1-2020","title":"May 1, 2020","text":"<ul> <li>Merged a number of execellent contributions in the ResNet model family over the past month<ul> <li>BlurPool2D and resnetblur models initiated by Chris Ha, I trained resnetblur50 to 79.3.</li> <li>TResNet models and SpaceToDepth, AntiAliasDownsampleLayer layers by mrT23</li> <li>ecaresnet (50d, 101d, light) models and two pruned variants using pruning as per (https://arxiv.org/abs/2002.08258) by Yonathan Aflalo</li> </ul> </li> <li>200 pretrained models in total now with updated results csv in results folder</li> </ul>"},{"location":"archived_changes/#april-5-2020","title":"April 5, 2020","text":"<ul> <li>Add some newly trained MobileNet-V2 models trained with latest h-params, rand augment. They compare quite favourably to EfficientNet-Lite<ul> <li>3.5M param MobileNet-V2 100 @ 73%</li> <li>4.5M param MobileNet-V2 110d @ 75%</li> <li>6.1M param MobileNet-V2 140 @ 76.5%</li> <li>5.8M param MobileNet-V2 120d @ 77.3%</li> </ul> </li> </ul>"},{"location":"archived_changes/#march-18-2020","title":"March 18, 2020","text":"<ul> <li>Add EfficientNet-Lite models w/ weights ported from Tensorflow TPU</li> <li>Add RandAugment trained ResNeXt-50 32x4d weights with 79.8 top-1. Trained by Andrew Lavin (see Training section for hparams)</li> </ul>"},{"location":"archived_changes/#april-5-2020_1","title":"April 5, 2020","text":"<ul> <li>Add some newly trained MobileNet-V2 models trained with latest h-params, rand augment. They compare quite favourably to EfficientNet-Lite<ul> <li>3.5M param MobileNet-V2 100 @ 73%</li> <li>4.5M param MobileNet-V2 110d @ 75%</li> <li>6.1M param MobileNet-V2 140 @ 76.5%</li> <li>5.8M param MobileNet-V2 120d @ 77.3%</li> </ul> </li> </ul>"},{"location":"archived_changes/#march-18-2020_1","title":"March 18, 2020","text":"<ul> <li>Add EfficientNet-Lite models w/ weights ported from Tensorflow TPU</li> <li>Add RandAugment trained ResNeXt-50 32x4d weights with 79.8 top-1. Trained by Andrew Lavin (see Training section for hparams)</li> </ul>"},{"location":"archived_changes/#feb-29-2020","title":"Feb 29, 2020","text":"<ul> <li>New MobileNet-V3 Large weights trained from stratch with this code to 75.77% top-1</li> <li>IMPORTANT CHANGE - default weight init changed for all MobilenetV3 / EfficientNet / related models<ul> <li>overall results similar to a bit better training from scratch on a few smaller models tried</li> <li>performance early in training seems consistently improved but less difference by end</li> <li>set <code>fix_group_fanout=False</code> in <code>_init_weight_goog</code> fn if you need to reproducte past behaviour</li> </ul> </li> <li>Experimental LR noise feature added applies a random perturbation to LR each epoch in specified range of training</li> </ul>"},{"location":"archived_changes/#feb-18-2020","title":"Feb 18, 2020","text":"<ul> <li>Big refactor of model layers and addition of several attention mechanisms. Several additions motivated by 'Compounding the Performance Improvements...' (https://arxiv.org/abs/2001.06268):<ul> <li>Move layer/module impl into <code>layers</code> subfolder/module of <code>models</code> and organize in a more granular fashion</li> <li>ResNet downsample paths now properly support dilation (output stride != 32) for avg_pool ('D' variant) and 3x3 (SENets) networks</li> <li>Add Selective Kernel Nets on top of ResNet base, pretrained weights<ul> <li>skresnet18 - 73% top-1</li> <li>skresnet34 - 76.9% top-1 </li> <li>skresnext50_32x4d (equiv to SKNet50) - 80.2% top-1</li> </ul> </li> <li>ECA and CECA (circular padding) attention layer contributed by Chris Ha</li> <li>CBAM attention experiment (not the best results so far, may remove)</li> <li>Attention factory to allow dynamically selecting one of SE, ECA, CBAM in the <code>.se</code> position for all ResNets</li> <li>Add DropBlock and DropPath (formerly DropConnect for EfficientNet/MobileNetv3) support to all ResNet variants</li> </ul> </li> <li>Full dataset results updated that incl NoisyStudent weights and 2 of the 3 SK weights</li> </ul>"},{"location":"archived_changes/#feb-12-2020","title":"Feb 12, 2020","text":"<ul> <li>Add EfficientNet-L2 and B0-B7 NoisyStudent weights ported from Tensorflow TPU</li> </ul>"},{"location":"archived_changes/#feb-6-2020","title":"Feb 6, 2020","text":"<ul> <li>Add RandAugment trained EfficientNet-ES (EdgeTPU-Small) weights with 78.1 top-1. Trained by Andrew Lavin (see Training section for hparams)</li> </ul>"},{"location":"archived_changes/#feb-12-2020_1","title":"Feb \u00bd, 2020","text":"<ul> <li>Port new EfficientNet-B8 (RandAugment) weights, these are different than the B8 AdvProp, different input normalization.</li> <li>Update results csv files on all models for ImageNet validation and three other test sets</li> <li>Push PyPi package update</li> </ul>"},{"location":"archived_changes/#jan-31-2020","title":"Jan 31, 2020","text":"<ul> <li>Update ResNet50 weights with a new 79.038 result from further JSD / AugMix experiments. Full command line for reproduction in training section below.</li> </ul>"},{"location":"archived_changes/#jan-1112-2020","title":"Jan 11/12, 2020","text":"<ul> <li>Master may be a bit unstable wrt to training, these changes have been tested but not all combos</li> <li>Implementations of AugMix added to existing RA and AA. Including numerous supporting pieces like JSD loss (Jensen-Shannon divergence + CE), and AugMixDataset</li> <li>SplitBatchNorm adaptation layer added for implementing Auxiliary BN as per AdvProp paper</li> <li>ResNet-50 AugMix trained model w/ 79% top-1 added</li> <li><code>seresnext26tn_32x4d</code> - 77.99 top-1, 93.75 top-5 added to tiered experiment, higher img/s than 't' and 'd'</li> </ul>"},{"location":"archived_changes/#jan-3-2020","title":"Jan 3, 2020","text":"<ul> <li>Add RandAugment trained EfficientNet-B0 weight with 77.7 top-1. Trained by Michael Klachko with this code and recent hparams (see Training section)</li> <li>Add <code>avg_checkpoints.py</code> script for post training weight averaging and update all scripts with header docstrings and shebangs.</li> </ul>"},{"location":"archived_changes/#dec-30-2019","title":"Dec 30, 2019","text":"<ul> <li>Merge Dushyant Mehta's PR for SelecSLS (Selective Short and Long Range Skip Connections) networks. Good GPU memory consumption and throughput. Original: https://github.com/mehtadushy/SelecSLS-Pytorch</li> </ul>"},{"location":"archived_changes/#dec-28-2019","title":"Dec 28, 2019","text":"<ul> <li>Add new model weights and training hparams (see Training Hparams section)<ul> <li><code>efficientnet_b3</code> - 81.5 top-1, 95.7 top-5 at default res/crop, 81.9, 95.8 at 320x320 1.0 crop-pct<ul> <li>trained with RandAugment, ended up with an interesting but less than perfect result (see training section)</li> </ul> </li> <li><code>seresnext26d_32x4d</code>- 77.6 top-1, 93.6 top-5<ul> <li>deep stem (32, 32, 64), avgpool downsample</li> <li>stem/dowsample from bag-of-tricks paper</li> </ul> </li> <li><code>seresnext26t_32x4d</code>- 78.0 top-1, 93.7 top-5<ul> <li>deep tiered stem (24, 48, 64), avgpool downsample (a modified 'D' variant)</li> <li>stem sizing mods from Jeremy Howard and fastai devs discussing ResNet architecture experiments</li> </ul> </li> </ul> </li> </ul>"},{"location":"archived_changes/#dec-23-2019","title":"Dec 23, 2019","text":"<ul> <li>Add RandAugment trained MixNet-XL weights with 80.48 top-1.</li> <li><code>--dist-bn</code> argument added to train.py, will distribute BN stats between nodes after each train epoch, before eval</li> </ul>"},{"location":"archived_changes/#dec-4-2019","title":"Dec 4, 2019","text":"<ul> <li>Added weights from the first training from scratch of an EfficientNet (B2) with my new RandAugment implementation. Much better than my previous B2 and very close to the official AdvProp ones (80.4 top-1, 95.08 top-5).</li> </ul>"},{"location":"archived_changes/#nov-29-2019","title":"Nov 29, 2019","text":"<ul> <li>Brought EfficientNet and MobileNetV3 up to date with my https://github.com/rwightman/gen-efficientnet-pytorch code. Torchscript and ONNX export compat excluded.<ul> <li>AdvProp weights added</li> <li>Official TF MobileNetv3 weights added</li> </ul> </li> <li>EfficientNet and MobileNetV3 hook based 'feature extraction' classes added. Will serve as basis for using models as backbones in obj detection/segmentation tasks. Lots more to be done here...</li> <li>HRNet classification models and weights added from https://github.com/HRNet/HRNet-Image-Classification</li> <li>Consistency in global pooling, <code>reset_classifer</code>, and <code>forward_features</code> across models<ul> <li><code>forward_features</code> always returns unpooled feature maps now</li> </ul> </li> <li>Reasonable chance I broke something... let me know</li> </ul>"},{"location":"archived_changes/#nov-22-2019","title":"Nov 22, 2019","text":"<ul> <li>Add ImageNet training RandAugment implementation alongside AutoAugment. PyTorch Transform compatible format, using PIL. Currently training two EfficientNet models from scratch with promising results... will update.</li> <li><code>drop-connect</code> cmd line arg finally added to <code>train.py</code>, no need to hack model fns. Works for efficientnet/mobilenetv3 based models, ignored otherwise.</li> </ul>"},{"location":"changes/","title":"Recent Changes","text":""},{"location":"changes/#jan-5-2023","title":"Jan 5, 2023","text":"<ul> <li>ConvNeXt-V2 models and weights added to existing <code>convnext.py</code><ul> <li>Paper: ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders</li> <li>Reference impl: https://github.com/facebookresearch/ConvNeXt-V2 (NOTE: weights currently CC-BY-NC)</li> </ul> </li> </ul>"},{"location":"changes/#dec-23-2022","title":"Dec 23, 2022 \ud83c\udf84\u2603","text":"<ul> <li>Add FlexiViT models and weights from https://github.com/google-research/big_vision (check out paper at https://arxiv.org/abs/2212.08013)<ul> <li>NOTE currently resizing is static on model creation, on-the-fly dynamic / train patch size sampling is a WIP</li> </ul> </li> <li>Many more models updated to multi-weight and downloadable via HF hub now (convnext, efficientnet, mobilenet, vision_transformer*, beit)</li> <li>More model pretrained tag and adjustments, some model names changed (working on deprecation translations, consider main branch DEV branch right now, use 0.6.x for stable use)</li> <li>More ImageNet-12k (subset of 22k) pretrain models popping up:<ul> <li><code>efficientnet_b5.in12k_ft_in1k</code> - 85.9 @ 448x448</li> <li><code>vit_medium_patch16_gap_384.in12k_ft_in1k</code> - 85.5 @ 384x384</li> <li><code>vit_medium_patch16_gap_256.in12k_ft_in1k</code> - 84.5 @ 256x256</li> <li><code>convnext_nano.in12k_ft_in1k</code> - 82.9 @ 288x288</li> </ul> </li> </ul>"},{"location":"changes/#dec-8-2022","title":"Dec 8, 2022","text":"<ul> <li>Add 'EVA l' to <code>vision_transformer.py</code>, MAE style ViT-L/14 MIM pretrain w/ EVA-CLIP targets, FT on ImageNet-1k (w/ ImageNet-22k intermediate for some)<ul> <li>original source: https://github.com/baaivision/EVA</li> </ul> </li> </ul>    model top1 param_count gmac macts hub     eva_large_patch14_336.in22k_ft_in22k_in1k 89.2 304.5 191.1 270.2 link   eva_large_patch14_336.in22k_ft_in1k 88.7 304.5 191.1 270.2 link   eva_large_patch14_196.in22k_ft_in22k_in1k 88.6 304.1 61.6 63.5 link   eva_large_patch14_196.in22k_ft_in1k 87.9 304.1 61.6 63.5 link"},{"location":"changes/#dec-6-2022","title":"Dec 6, 2022","text":"<ul> <li>Add 'EVA g', BEiT style ViT-g/14 model weights w/ both MIM pretrain and CLIP pretrain to <code>beit.py</code>. <ul> <li>original source: https://github.com/baaivision/EVA</li> <li>paper: https://arxiv.org/abs/2211.07636</li> </ul> </li> </ul>    model top1 param_count gmac macts hub     eva_giant_patch14_560.m30m_ft_in22k_in1k 89.8 1014.4 1906.8 2577.2 link   eva_giant_patch14_336.m30m_ft_in22k_in1k 89.6 1013 620.6 550.7 link   eva_giant_patch14_336.clip_ft_in1k 89.4 1013 620.6 550.7 link   eva_giant_patch14_224.clip_ft_in1k 89.1 1012.6 267.2 192.6 link"},{"location":"changes/#dec-5-2022","title":"Dec 5, 2022","text":"<ul> <li>Pre-release (<code>0.8.0dev0</code>) of multi-weight support (<code>model_arch.pretrained_tag</code>). Install with <code>pip install --pre timm</code><ul> <li>vision_transformer, maxvit, convnext are the first three model impl w/ support</li> <li>model names are changing with this (previous _21k, etc. fn will merge), still sorting out deprecation handling</li> <li>bugs are likely, but I need feedback so please try it out</li> <li>if stability is needed, please use 0.6.x pypi releases or clone from 0.6.x branch</li> </ul> </li> <li>Support for PyTorch 2.0 compile is added in train/validate/inference/benchmark, use <code>--torchcompile</code> argument</li> <li>Inference script allows more control over output, select k for top-class index + prob json, csv or parquet output</li> <li>Add a full set of fine-tuned CLIP image tower weights from both LAION-2B and original OpenAI CLIP models</li> </ul>    model top1 param_count gmac macts hub     vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k 88.6 632.5 391 407.5 link   vit_large_patch14_clip_336.openai_ft_in12k_in1k 88.3 304.5 191.1 270.2 link   vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k 88.2 632 167.4 139.4 link   vit_large_patch14_clip_336.laion2b_ft_in12k_in1k 88.2 304.5 191.1 270.2 link   vit_large_patch14_clip_224.openai_ft_in12k_in1k 88.2 304.2 81.1 88.8 link   vit_large_patch14_clip_224.laion2b_ft_in12k_in1k 87.9 304.2 81.1 88.8 link   vit_large_patch14_clip_224.openai_ft_in1k 87.9 304.2 81.1 88.8 link   vit_large_patch14_clip_336.laion2b_ft_in1k 87.9 304.5 191.1 270.2 link   vit_huge_patch14_clip_224.laion2b_ft_in1k 87.6 632 167.4 139.4 link   vit_large_patch14_clip_224.laion2b_ft_in1k 87.3 304.2 81.1 88.8 link   vit_base_patch16_clip_384.laion2b_ft_in12k_in1k 87.2 86.9 55.5 101.6 link   vit_base_patch16_clip_384.openai_ft_in12k_in1k 87 86.9 55.5 101.6 link   vit_base_patch16_clip_384.laion2b_ft_in1k 86.6 86.9 55.5 101.6 link   vit_base_patch16_clip_384.openai_ft_in1k 86.2 86.9 55.5 101.6 link   vit_base_patch16_clip_224.laion2b_ft_in12k_in1k 86.2 86.6 17.6 23.9 link   vit_base_patch16_clip_224.openai_ft_in12k_in1k 85.9 86.6 17.6 23.9 link   vit_base_patch32_clip_448.laion2b_ft_in12k_in1k 85.8 88.3 17.9 23.9 link   vit_base_patch16_clip_224.laion2b_ft_in1k 85.5 86.6 17.6 23.9 link   vit_base_patch32_clip_384.laion2b_ft_in12k_in1k 85.4 88.3 13.1 16.5 link   vit_base_patch16_clip_224.openai_ft_in1k 85.3 86.6 17.6 23.9 link   vit_base_patch32_clip_384.openai_ft_in12k_in1k 85.2 88.3 13.1 16.5 link   vit_base_patch32_clip_224.laion2b_ft_in12k_in1k 83.3 88.2 4.4 5 link   vit_base_patch32_clip_224.laion2b_ft_in1k 82.6 88.2 4.4 5 link   vit_base_patch32_clip_224.openai_ft_in1k 81.9 88.2 4.4 5 link    <ul> <li>Port of MaxViT Tensorflow Weights from official impl at https://github.com/google-research/maxvit<ul> <li>There was larger than expected drops for the upscaled 384/512 in21k fine-tune weights, possible detail missing, but the 21k FT did seem sensitive to small preprocessing</li> </ul> </li> </ul>    model top1 param_count gmac macts hub     maxvit_xlarge_tf_512.in21k_ft_in1k 88.5 475.8 534.1 1413.2 link   maxvit_xlarge_tf_384.in21k_ft_in1k 88.3 475.3 292.8 668.8 link   maxvit_base_tf_512.in21k_ft_in1k 88.2 119.9 138 704 link   maxvit_large_tf_512.in21k_ft_in1k 88 212.3 244.8 942.2 link   maxvit_large_tf_384.in21k_ft_in1k 88 212 132.6 445.8 link   maxvit_base_tf_384.in21k_ft_in1k 87.9 119.6 73.8 332.9 link   maxvit_base_tf_512.in1k 86.6 119.9 138 704 link   maxvit_large_tf_512.in1k 86.5 212.3 244.8 942.2 link   maxvit_base_tf_384.in1k 86.3 119.6 73.8 332.9 link   maxvit_large_tf_384.in1k 86.2 212 132.6 445.8 link   maxvit_small_tf_512.in1k 86.1 69.1 67.3 383.8 link   maxvit_tiny_tf_512.in1k 85.7 31 33.5 257.6 link   maxvit_small_tf_384.in1k 85.5 69 35.9 183.6 link   maxvit_tiny_tf_384.in1k 85.1 31 17.5 123.4 link   maxvit_large_tf_224.in1k 84.9 211.8 43.7 127.4 link   maxvit_base_tf_224.in1k 84.9 119.5 24 95 link   maxvit_small_tf_224.in1k 84.4 68.9 11.7 53.2 link   maxvit_tiny_tf_224.in1k 83.4 30.9 5.6 35.8 link"},{"location":"changes/#oct-15-2022","title":"Oct 15, 2022","text":"<ul> <li>Train and validation script enhancements</li> <li>Non-GPU (ie CPU) device support</li> <li>SLURM compatibility for train script</li> <li>HF datasets support (via ReaderHfds)</li> <li>TFDS/WDS dataloading improvements (sample padding/wrap for distributed use fixed wrt sample count estimate)</li> <li>in_chans !=3 support for scripts / loader</li> <li>Adan optimizer</li> <li>Can enable per-step LR scheduling via args</li> <li>Dataset 'parsers' renamed to 'readers', more descriptive of purpose</li> <li>AMP args changed, APEX via <code>--amp-impl apex</code>, bfloat16 supportedf via <code>--amp-dtype bfloat16</code></li> <li>main branch switched to 0.7.x version, 0.6x forked for stable release of weight only adds</li> <li>master -&gt; main branch rename</li> </ul>"},{"location":"changes/#oct-10-2022","title":"Oct 10, 2022","text":"<ul> <li>More weights in <code>maxxvit</code> series, incl first ConvNeXt block based <code>coatnext</code> and <code>maxxvit</code> experiments:<ul> <li><code>coatnext_nano_rw_224</code> - 82.0 @ 224 (G) -- (uses ConvNeXt conv block, no BatchNorm)</li> <li><code>maxxvit_rmlp_nano_rw_256</code> - 83.0 @ 256, 83.7 @ 320  (G) (uses ConvNeXt conv block, no BN)</li> <li><code>maxvit_rmlp_small_rw_224</code> - 84.5 @ 224, 85.1 @ 320 (G)</li> <li><code>maxxvit_rmlp_small_rw_256</code> - 84.6 @ 256, 84.9 @ 288 (G) -- could be trained better, hparams need tuning (uses ConvNeXt block, no BN)</li> <li><code>coatnet_rmlp_2_rw_224</code> - 84.6 @ 224, 85 @ 320  (T)</li> <li>NOTE: official MaxVit weights (in1k) have been released at https://github.com/google-research/maxvit -- some extra work is needed to port and adapt since my impl was created independently of theirs and has a few small differences + the whole TF same padding fun.</li> </ul> </li> </ul>"},{"location":"changes/#sept-23-2022","title":"Sept 23, 2022","text":"<ul> <li>LAION-2B CLIP image towers supported as pretrained backbones for fine-tune or features (no classifier)<ul> <li>vit_base_patch32_224_clip_laion2b</li> <li>vit_large_patch14_224_clip_laion2b</li> <li>vit_huge_patch14_224_clip_laion2b</li> <li>vit_giant_patch14_224_clip_laion2b</li> </ul> </li> </ul>"},{"location":"changes/#sept-7-2022","title":"Sept 7, 2022","text":"<ul> <li>Hugging Face <code>timm</code> docs home now exists, look for more here in the future</li> <li>Add BEiT-v2 weights for base and large 224x224 models from https://github.com/microsoft/unilm/tree/master/beit2</li> <li>Add more weights in <code>maxxvit</code> series incl a <code>pico</code> (7.5M params, 1.9 GMACs), two <code>tiny</code> variants:<ul> <li><code>maxvit_rmlp_pico_rw_256</code> - 80.5 @ 256, 81.3 @ 320  (T)</li> <li><code>maxvit_tiny_rw_224</code> - 83.5 @ 224 (G)</li> <li><code>maxvit_rmlp_tiny_rw_256</code> - 84.2 @ 256, 84.8 @ 320 (T)</li> </ul> </li> </ul>"},{"location":"changes/#aug-29-2022","title":"Aug 29, 2022","text":"<ul> <li>MaxVit window size scales with img_size by default. Add new RelPosMlp MaxViT weight that leverages this:<ul> <li><code>maxvit_rmlp_nano_rw_256</code> - 83.0 @ 256, 83.6 @ 320  (T)</li> </ul> </li> </ul>"},{"location":"changes/#aug-26-2022","title":"Aug 26, 2022","text":"<ul> <li>CoAtNet (https://arxiv.org/abs/2106.04803) and MaxVit (https://arxiv.org/abs/2204.01697) <code>timm</code> original models<ul> <li>both found in <code>maxxvit.py</code> model def, contains numerous experiments outside scope of original papers</li> <li>an unfinished Tensorflow version from MaxVit authors can be found https://github.com/google-research/maxvit</li> </ul> </li> <li>Initial CoAtNet and MaxVit timm pretrained weights (working on more):<ul> <li><code>coatnet_nano_rw_224</code> - 81.7 @ 224  (T)</li> <li><code>coatnet_rmlp_nano_rw_224</code> - 82.0 @ 224, 82.8 @ 320 (T)</li> <li><code>coatnet_0_rw_224</code> - 82.4  (T)  -- NOTE timm '0' coatnets have 2 more 3rd stage blocks</li> <li><code>coatnet_bn_0_rw_224</code> - 82.4  (T)</li> <li><code>maxvit_nano_rw_256</code> - 82.9 @ 256  (T)</li> <li><code>coatnet_rmlp_1_rw_224</code> - 83.4 @ 224, 84 @ 320  (T)</li> <li><code>coatnet_1_rw_224</code> - 83.6 @ 224 (G) </li> <li>(T) = TPU trained with <code>bits_and_tpu</code> branch training code, (G) = GPU trained</li> </ul> </li> <li>GCVit (weights adapted from https://github.com/NVlabs/GCVit, code 100% <code>timm</code> re-write for license purposes)</li> <li>MViT-V2 (multi-scale vit, adapted from https://github.com/facebookresearch/mvit)</li> <li>EfficientFormer (adapted from https://github.com/snap-research/EfficientFormer)</li> <li>PyramidVisionTransformer-V2 (adapted from https://github.com/whai362/PVT)</li> <li>'Fast Norm' support for LayerNorm and GroupNorm that avoids float32 upcast w/ AMP (uses APEX LN if available for further boost)</li> </ul>"},{"location":"changes/#aug-15-2022","title":"Aug 15, 2022","text":"<ul> <li>ConvNeXt atto weights added<ul> <li><code>convnext_atto</code> - 75.7 @ 224, 77.0 @ 288</li> <li><code>convnext_atto_ols</code> - 75.9  @ 224, 77.2 @ 288</li> </ul> </li> </ul>"},{"location":"changes/#aug-5-2022","title":"Aug 5, 2022","text":"<ul> <li>More custom ConvNeXt smaller model defs with weights <ul> <li><code>convnext_femto</code> - 77.5 @ 224, 78.7 @ 288</li> <li><code>convnext_femto_ols</code> - 77.9  @ 224, 78.9 @ 288</li> <li><code>convnext_pico</code> - 79.5 @ 224, 80.4 @ 288</li> <li><code>convnext_pico_ols</code> - 79.5 @ 224, 80.5 @ 288</li> <li><code>convnext_nano_ols</code> - 80.9 @ 224, 81.6 @ 288</li> </ul> </li> <li>Updated EdgeNeXt to improve ONNX export, add new base variant and weights from original (https://github.com/mmaaz60/EdgeNeXt)</li> </ul>"},{"location":"changes/#july-28-2022","title":"July 28, 2022","text":"<ul> <li>Add freshly minted DeiT-III Medium (width=512, depth=12, num_heads=8) model weights. Thanks Hugo Touvron!</li> </ul>"},{"location":"changes/#july-27-2022","title":"July 27, 2022","text":"<ul> <li>All runtime benchmark and validation result csv files are up-to-date!</li> <li>A few more weights &amp; model defs added:<ul> <li><code>darknetaa53</code> -  79.8 @ 256, 80.5 @ 288</li> <li><code>convnext_nano</code> - 80.8 @ 224, 81.5 @ 288</li> <li><code>cs3sedarknet_l</code> - 81.2 @ 256, 81.8 @ 288</li> <li><code>cs3darknet_x</code> - 81.8 @ 256, 82.2 @ 288</li> <li><code>cs3sedarknet_x</code> - 82.2 @ 256, 82.7 @ 288</li> <li><code>cs3edgenet_x</code> - 82.2 @ 256, 82.7 @ 288</li> <li><code>cs3se_edgenet_x</code> - 82.8 @ 256, 83.5 @ 320</li> </ul> </li> <li><code>cs3*</code> weights above all trained on TPU w/ <code>bits_and_tpu</code> branch. Thanks to TRC program!</li> <li>Add output_stride=8 and 16 support to ConvNeXt (dilation)</li> <li>deit3 models not being able to resize pos_emb fixed</li> <li>Version 0.6.7 PyPi release (/w above bug fixes and new weighs since 0.6.5)</li> </ul>"},{"location":"changes/#july-8-2022","title":"July 8, 2022","text":"<p>More models, more fixes * Official research models (w/ weights) added:   * EdgeNeXt from (https://github.com/mmaaz60/EdgeNeXt)   * MobileViT-V2 from (https://github.com/apple/ml-cvnets)   * DeiT III (Revenge of the ViT) from (https://github.com/facebookresearch/deit) * My own models:   * Small <code>ResNet</code> defs added by request with 1 block repeats for both basic and bottleneck (resnet10 and resnet14)   * <code>CspNet</code> refactored with dataclass config, simplified CrossStage3 (<code>cs3</code>) option. These are closer to YOLO-v5+ backbone defs.   * More relative position vit fiddling. Two <code>srelpos</code> (shared relative position) models trained, and a medium w/ class token.   * Add an alternate downsample mode to EdgeNeXt and train a <code>small</code> model. Better than original small, but not their new USI trained weights. * My own model weight results (all ImageNet-1k training)   * <code>resnet10t</code> - 66.5 @ 176, 68.3 @ 224   * <code>resnet14t</code> - 71.3 @ 176, 72.3 @ 224   * <code>resnetaa50</code> - 80.6 @ 224 , 81.6 @ 288   * <code>darknet53</code> -  80.0 @ 256, 80.5 @ 288   * <code>cs3darknet_m</code> - 77.0 @ 256, 77.6 @ 288   * <code>cs3darknet_focus_m</code> - 76.7 @ 256, 77.3 @ 288   * <code>cs3darknet_l</code> - 80.4 @ 256, 80.9 @ 288   * <code>cs3darknet_focus_l</code> - 80.3 @ 256, 80.9 @ 288   * <code>vit_srelpos_small_patch16_224</code> - 81.1 @ 224, 82.1 @ 320   * <code>vit_srelpos_medium_patch16_224</code> - 82.3 @ 224, 83.1 @ 320   * <code>vit_relpos_small_patch16_cls_224</code> - 82.6 @ 224, 83.6 @ 320   * <code>edgnext_small_rw</code> - 79.6 @ 224, 80.4 @ 320 * <code>cs3</code>, <code>darknet</code>, and <code>vit_*relpos</code> weights above all trained on TPU thanks to TRC program! Rest trained on overheating GPUs. * Hugging Face Hub support fixes verified, demo notebook TBA * Pretrained weights / configs can be loaded externally (ie from local disk) w/ support for head adaptation. * Add support to change image extensions scanned by <code>timm</code> datasets/parsers. See (https://github.com/rwightman/pytorch-image-models/pull/1274#issuecomment-1178303103) * Default ConvNeXt LayerNorm impl to use <code>F.layer_norm(x.permute(0, 2, 3, 1), ...).permute(0, 3, 1, 2)</code> via <code>LayerNorm2d</code> in all cases.    * a bit slower than previous custom impl on some hardware (ie Ampere w/ CL), but overall fewer regressions across wider HW / PyTorch version ranges.    * previous impl exists as <code>LayerNormExp2d</code> in <code>models/layers/norm.py</code> * Numerous bug fixes * Currently testing for imminent PyPi 0.6.x release * LeViT pretraining of larger models still a WIP, they don't train well / easily without distillation. Time to add distill support (finally)? * ImageNet-22k weight training + finetune ongoing, work on multi-weight support (slowly) chugging along (there are a LOT of weights, sigh) ...</p>"},{"location":"changes/#may-13-2022","title":"May 13, 2022","text":"<ul> <li>Official Swin-V2 models and weights added from (https://github.com/microsoft/Swin-Transformer). Cleaned up to support torchscript.</li> <li>Some refactoring for existing <code>timm</code> Swin-V2-CR impl, will likely do a bit more to bring parts closer to official and decide whether to merge some aspects.</li> <li>More Vision Transformer relative position / residual post-norm experiments (all trained on TPU thanks to TRC program)<ul> <li><code>vit_relpos_small_patch16_224</code> - 81.5 @ 224, 82.5 @ 320 -- rel pos, layer scale, no class token, avg pool</li> <li><code>vit_relpos_medium_patch16_rpn_224</code> - 82.3 @ 224, 83.1 @ 320 -- rel pos + res-post-norm, no class token, avg pool</li> <li><code>vit_relpos_medium_patch16_224</code> - 82.5 @ 224, 83.3 @ 320 -- rel pos, layer scale, no class token, avg pool</li> <li><code>vit_relpos_base_patch16_gapcls_224</code> - 82.8 @ 224, 83.9 @ 320 -- rel pos, layer scale, class token, avg pool (by mistake)</li> </ul> </li> <li>Bring 512 dim, 8-head 'medium' ViT model variant back to life (after using in a pre DeiT 'small' model for first ViT impl back in 2020)</li> <li>Add ViT relative position support for switching btw existing impl and some additions in official Swin-V2 impl for future trials</li> <li>Sequencer2D impl (https://arxiv.org/abs/2205.01972), added via PR from author (https://github.com/okojoalg)</li> </ul>"},{"location":"changes/#may-2-2022","title":"May 2, 2022","text":"<ul> <li>Vision Transformer experiments adding Relative Position (Swin-V2 log-coord) (<code>vision_transformer_relpos.py</code>) and Residual Post-Norm branches (from Swin-V2) (<code>vision_transformer*.py</code>)<ul> <li><code>vit_relpos_base_patch32_plus_rpn_256</code> - 79.5 @ 256, 80.6 @ 320 -- rel pos + extended width + res-post-norm, no class token, avg pool</li> <li><code>vit_relpos_base_patch16_224</code> - 82.5 @ 224, 83.6 @ 320 -- rel pos, layer scale, no class token, avg pool</li> <li><code>vit_base_patch16_rpn_224</code> - 82.3 @ 224 -- rel pos + res-post-norm, no class token, avg pool</li> </ul> </li> <li>Vision Transformer refactor to remove representation layer that was only used in initial vit and rarely used since with newer pretrain (ie <code>How to Train Your ViT</code>)</li> <li><code>vit_*</code> models support removal of class token, use of global average pool, use of fc_norm (ala beit, mae).</li> </ul>"},{"location":"changes/#april-22-2022","title":"April 22, 2022","text":"<ul> <li><code>timm</code> models are now officially supported in fast.ai! Just in time for the new Practical Deep Learning course. <code>timmdocs</code> documentation link updated to timm.fast.ai.</li> <li>Two more model weights added in the TPU trained series. Some In22k pretrain still in progress.<ul> <li><code>seresnext101d_32x8d</code> - 83.69 @ 224, 84.35 @ 288</li> <li><code>seresnextaa101d_32x8d</code> (anti-aliased w/ AvgPool2d) - 83.85 @ 224, 84.57 @ 288</li> </ul> </li> </ul>"},{"location":"changes/#march-23-2022","title":"March 23, 2022","text":"<ul> <li>Add <code>ParallelBlock</code> and <code>LayerScale</code> option to base vit models to support model configs in Three things everyone should know about ViT</li> <li><code>convnext_tiny_hnf</code> (head norm first) weights trained with (close to) A2 recipe, 82.2% top-1, could do better with more epochs.</li> </ul>"},{"location":"changes/#march-21-2022","title":"March 21, 2022","text":"<ul> <li>Merge <code>norm_norm_norm</code>. IMPORTANT this update for a coming 0.6.x release will likely de-stabilize the master branch for a while. Branch <code>0.5.x</code> or a previous 0.5.x release can be used if stability is required.</li> <li>Significant weights update (all TPU trained) as described in this release<ul> <li><code>regnety_040</code> - 82.3 @ 224, 82.96 @ 288</li> <li><code>regnety_064</code> - 83.0 @ 224, 83.65 @ 288</li> <li><code>regnety_080</code> - 83.17 @ 224, 83.86 @ 288</li> <li><code>regnetv_040</code> - 82.44 @ 224, 83.18 @ 288   (timm pre-act)</li> <li><code>regnetv_064</code> - 83.1 @ 224, 83.71 @ 288   (timm pre-act)</li> <li><code>regnetz_040</code> - 83.67 @ 256, 84.25 @ 320</li> <li><code>regnetz_040h</code> - 83.77 @ 256, 84.5 @ 320 (w/ extra fc in head)</li> <li><code>resnetv2_50d_gn</code> - 80.8 @ 224, 81.96 @ 288 (pre-act GroupNorm)</li> <li><code>resnetv2_50d_evos</code> 80.77 @ 224, 82.04 @ 288 (pre-act EvoNormS)</li> <li><code>regnetz_c16_evos</code>  - 81.9 @ 256, 82.64 @ 320 (EvoNormS)</li> <li><code>regnetz_d8_evos</code>  - 83.42 @ 256, 84.04 @ 320 (EvoNormS)</li> <li><code>xception41p</code> - 82 @ 299   (timm pre-act)</li> <li><code>xception65</code> -  83.17 @ 299</li> <li><code>xception65p</code> -  83.14 @ 299   (timm pre-act)</li> <li><code>resnext101_64x4d</code> - 82.46 @ 224, 83.16 @ 288</li> <li><code>seresnext101_32x8d</code> - 83.57 @ 224, 84.270 @ 288</li> <li><code>resnetrs200</code> - 83.85 @ 256, 84.44 @ 320</li> </ul> </li> <li>HuggingFace hub support fixed w/ initial groundwork for allowing alternative 'config sources' for pretrained model definitions and weights (generic local file / remote url support soon)</li> <li>SwinTransformer-V2 implementation added. Submitted by Christoph Reich. Training experiments and model changes by myself are ongoing so expect compat breaks.</li> <li>Swin-S3 (AutoFormerV2) models / weights added from https://github.com/microsoft/Cream/tree/main/AutoFormerV2</li> <li>MobileViT models w/ weights adapted from https://github.com/apple/ml-cvnets</li> <li>PoolFormer models w/ weights adapted from https://github.com/sail-sg/poolformer</li> <li>VOLO models w/ weights adapted from https://github.com/sail-sg/volo</li> <li>Significant work experimenting with non-BatchNorm norm layers such as EvoNorm, FilterResponseNorm, GroupNorm, etc</li> <li>Enhance support for alternate norm + act ('NormAct') layers added to a number of models, esp EfficientNet/MobileNetV3, RegNet, and aligned Xception</li> <li>Grouped conv support added to EfficientNet family</li> <li>Add 'group matching' API to all models to allow grouping model parameters for application of 'layer-wise' LR decay, lr scale added to LR scheduler</li> <li>Gradient checkpointing support added to many models</li> <li><code>forward_head(x, pre_logits=False)</code> fn added to all models to allow separate calls of <code>forward_features</code> + <code>forward_head</code></li> <li>All vision transformer and vision MLP models update to return non-pooled / non-token selected features from <code>foward_features</code>, for consistency with CNN models, token selection or pooling now applied in <code>forward_head</code></li> </ul>"},{"location":"changes/#feb-2-2022","title":"Feb 2, 2022","text":"<ul> <li>Chris Hughes posted an exhaustive run through of <code>timm</code> on his blog yesterday. Well worth a read. Getting Started with PyTorch Image Models (timm): A Practitioner\u2019s Guide</li> <li>I'm currently prepping to merge the <code>norm_norm_norm</code> branch back to master (ver 0.6.x) in next week or so.<ul> <li>The changes are more extensive than usual and may destabilize and break some model API use (aiming for full backwards compat). So, beware <code>pip install git+https://github.com/rwightman/pytorch-image-models</code> installs!</li> <li><code>0.5.x</code> releases and a <code>0.5.x</code> branch will remain stable with a cherry pick or two until dust clears. Recommend sticking to pypi install for a bit if you want stable.</li> </ul> </li> </ul>"},{"location":"changes/#jan-14-2022","title":"Jan 14, 2022","text":"<ul> <li>Version 0.5.4 w/ release to be pushed to pypi. It's been a while since last pypi update and riskier changes will be merged to main branch soon....</li> <li>Add ConvNeXT models /w weights from official impl (https://github.com/facebookresearch/ConvNeXt), a few perf tweaks, compatible with timm features</li> <li>Tried training a few small (~1.8-3M param) / mobile optimized models, a few are good so far, more on the way...<ul> <li><code>mnasnet_small</code> - 65.6 top-1</li> <li><code>mobilenetv2_050</code> - 65.9</li> <li><code>lcnet_100/075/050</code> - 72.1 / 68.8 / 63.1</li> <li><code>semnasnet_075</code> - 73</li> <li><code>fbnetv3_b/d/g</code> - 79.1 / 79.7 / 82.0</li> </ul> </li> <li>TinyNet models added by rsomani95</li> <li>LCNet added via MobileNetV3 architecture</li> </ul>"},{"location":"feature_extraction/","title":"Feature Extraction","text":"<p>All of the models in <code>timm</code> have consistent mechanisms for obtaining various types of features from the model for tasks besides classification.</p>"},{"location":"feature_extraction/#penultimate-layer-features-pre-classifier-features","title":"Penultimate Layer Features (Pre-Classifier Features)","text":"<p>The features from the penultimate model layer can be obtained in several ways without requiring model surgery (although feel free to do surgery). One must first decide if they want pooled or un-pooled features.</p>"},{"location":"feature_extraction/#unpooled","title":"Unpooled","text":"<p>There are three ways to obtain unpooled features.</p> <p>Without modifying the network, one can call <code>model.forward_features(input)</code> on any model instead of the usual <code>model(input)</code>. This will bypass the head classifier and global pooling for networks.</p> <p>If one wants to explicitly modify the network to return unpooled features, they can either create the model without a classifier and pooling, or remove it later. Both paths remove the parameters associated with the classifier from the network.</p>"},{"location":"feature_extraction/#forward_features","title":"forward_features()","text":"<p><pre><code>import torch\nimport timm\nm = timm.create_model('xception41', pretrained=True)\no = m(torch.randn(2, 3, 299, 299))\nprint(f'Original shape: {o.shape}')\no = m.forward_features(torch.randn(2, 3, 299, 299))\nprint(f'Unpooled shape: {o.shape}')\n</code></pre> Output: <pre><code>Original shape: torch.Size([2, 1000])\nUnpooled shape: torch.Size([2, 2048, 10, 10])\n</code></pre></p>"},{"location":"feature_extraction/#create-with-no-classifier-and-pooling","title":"Create with no classifier and pooling","text":"<p><pre><code>import torch\nimport timm\nm = timm.create_model('resnet50', pretrained=True, num_classes=0, global_pool='')\no = m(torch.randn(2, 3, 224, 224))\nprint(f'Unpooled shape: {o.shape}')\n</code></pre> Output: <pre><code>Unpooled shape: torch.Size([2, 2048, 7, 7])\n</code></pre></p>"},{"location":"feature_extraction/#remove-it-later","title":"Remove it later","text":"<p><pre><code>import torch\nimport timm\nm = timm.create_model('densenet121', pretrained=True)\no = m(torch.randn(2, 3, 224, 224))\nprint(f'Original shape: {o.shape}')\nm.reset_classifier(0, '')\no = m(torch.randn(2, 3, 224, 224))\nprint(f'Unpooled shape: {o.shape}')\n</code></pre> Output: <pre><code>Original shape: torch.Size([2, 1000])\nUnpooled shape: torch.Size([2, 1024, 7, 7])\n</code></pre></p>"},{"location":"feature_extraction/#pooled","title":"Pooled","text":"<p>To modify the network to return pooled features, one can use <code>forward_features()</code> and pool/flatten the result themselves, or modify the network like above but keep pooling intact. </p>"},{"location":"feature_extraction/#create-with-no-classifier","title":"Create with no classifier","text":"<p><pre><code>import torch\nimport timm\nm = timm.create_model('resnet50', pretrained=True, num_classes=0)\no = m(torch.randn(2, 3, 224, 224))\nprint(f'Pooled shape: {o.shape}')\n</code></pre> Output: <pre><code>Pooled shape: torch.Size([2, 2048])\n</code></pre></p>"},{"location":"feature_extraction/#remove-it-later_1","title":"Remove it later","text":"<p><pre><code>import torch\nimport timm\nm = timm.create_model('ese_vovnet19b_dw', pretrained=True)\no = m(torch.randn(2, 3, 224, 224))\nprint(f'Original shape: {o.shape}')\nm.reset_classifier(0)\no = m(torch.randn(2, 3, 224, 224))\nprint(f'Pooled shape: {o.shape}')\n</code></pre> Output: <pre><code>Original shape: torch.Size([2, 1000])\nPooled shape: torch.Size([2, 1024])\n</code></pre></p>"},{"location":"feature_extraction/#multi-scale-feature-maps-feature-pyramid","title":"Multi-scale Feature Maps (Feature Pyramid)","text":"<p>Object detection, segmentation, keypoint, and a variety of dense pixel tasks require access to feature maps from the backbone network at multiple scales. This is often done by modifying the original classification network. Since each network varies quite a bit in structure, it's not uncommon to see only a few backbones supported in any given obj detection or segmentation library.</p> <p><code>timm</code> allows a consistent interface for creating any of the included models as feature backbones that output feature maps for selected levels. </p> <p>A feature backbone can be created by adding the argument <code>features_only=True</code> to any <code>create_model</code> call. By default 5 strides will be output from most models (not all have that many), with the first starting at 2 (some start at 1 or 4).</p>"},{"location":"feature_extraction/#create-a-feature-map-extraction-model","title":"Create a feature map extraction model","text":"<p><pre><code>import torch\nimport timm\nm = timm.create_model('resnest26d', features_only=True, pretrained=True)\no = m(torch.randn(2, 3, 224, 224))\nfor x in o:\n  print(x.shape)\n</code></pre> Output: <pre><code>torch.Size([2, 64, 112, 112])\ntorch.Size([2, 256, 56, 56])\ntorch.Size([2, 512, 28, 28])\ntorch.Size([2, 1024, 14, 14])\ntorch.Size([2, 2048, 7, 7])\n</code></pre></p>"},{"location":"feature_extraction/#query-the-feature-information","title":"Query the feature information","text":"<p>After a feature backbone has been created, it can be queried to provide channel or resolution reduction information to the downstream heads without requiring static config or hardcoded constants. The <code>.feature_info</code> attribute is a class encapsulating the information about the feature extraction points.</p> <p><pre><code>import torch\nimport timm\nm = timm.create_model('regnety_032', features_only=True, pretrained=True)\nprint(f'Feature channels: {m.feature_info.channels()}')\no = m(torch.randn(2, 3, 224, 224))\nfor x in o:\n  print(x.shape)\n</code></pre> Output: <pre><code>Feature channels: [32, 72, 216, 576, 1512]\ntorch.Size([2, 32, 112, 112])\ntorch.Size([2, 72, 56, 56])\ntorch.Size([2, 216, 28, 28])\ntorch.Size([2, 576, 14, 14])\ntorch.Size([2, 1512, 7, 7])\n</code></pre></p>"},{"location":"feature_extraction/#select-specific-feature-levels-or-limit-the-stride","title":"Select specific feature levels or limit the stride","text":"<p>There are two additional creation arguments impacting the output features. </p> <ul> <li><code>out_indices</code> selects which indices to output</li> <li><code>output_stride</code> limits the feature output stride of the network (also works in classification mode BTW)</li> </ul> <p><code>out_indices</code> is supported by all models, but not all models have the same index to feature stride mapping. Look at the code or check feature_info to compare. The out indices generally correspond to the <code>C(i+1)th</code> feature level (a <code>2^(i+1)</code> reduction). For most models, index 0 is the stride 2 features, and index 4 is stride 32.</p> <p><code>output_stride</code> is achieved by converting layers to use dilated convolutions. Doing so is not always straightforward, some networks only support <code>output_stride=32</code>.</p> <p><pre><code>import torch\nimport timm\nm = timm.create_model('ecaresnet101d', features_only=True, output_stride=8, out_indices=(2, 4), pretrained=True)\nprint(f'Feature channels: {m.feature_info.channels()}')\nprint(f'Feature reduction: {m.feature_info.reduction()}')\no = m(torch.randn(2, 3, 320, 320))\nfor x in o:\n  print(x.shape)\n</code></pre> Output: <pre><code>Feature channels: [512, 2048]\nFeature reduction: [8, 8]\ntorch.Size([2, 512, 40, 40])\ntorch.Size([2, 2048, 40, 40])\n</code></pre></p>"},{"location":"models/","title":"Model Summaries","text":"<p>The model architectures included come from a wide variety of sources. Sources, including papers, original impl (\"reference code\") that I rewrote / adapted, and PyTorch impl that I leveraged directly (\"code\") are listed below.</p> <p>Most included models have pretrained weights. The weights are either:</p> <ol> <li>from their original sources</li> <li>ported by myself from their original impl in a different framework (e.g. Tensorflow models)</li> <li>trained from scratch using the included training script</li> </ol> <p>The validation results for the pretrained weights are here</p> <p>A more exciting view (with pretty pictures) of the models within <code>timm</code> can be found at paperswithcode.</p>"},{"location":"models/#big-transfer-resnetv2-bit-resnetv2py","title":"Big Transfer ResNetV2 (BiT) [resnetv2.py]","text":"<ul> <li>Paper: <code>Big Transfer (BiT): General Visual Representation Learning</code> - https://arxiv.org/abs/1912.11370</li> <li>Reference code: https://github.com/google-research/big_transfer</li> </ul>"},{"location":"models/#cross-stage-partial-networks-cspnetpy","title":"Cross-Stage Partial Networks [cspnet.py]","text":"<ul> <li>Paper: <code>CSPNet: A New Backbone that can Enhance Learning Capability of CNN</code> - https://arxiv.org/abs/1911.11929</li> <li>Reference impl: https://github.com/WongKinYiu/CrossStagePartialNetworks</li> </ul>"},{"location":"models/#densenet-densenetpy","title":"DenseNet [densenet.py]","text":"<ul> <li>Paper: <code>Densely Connected Convolutional Networks</code> - https://arxiv.org/abs/1608.06993</li> <li>Code: https://github.com/pytorch/vision/tree/master/torchvision/models</li> </ul>"},{"location":"models/#dla-dlapy","title":"DLA [dla.py]","text":"<ul> <li>Paper: https://arxiv.org/abs/1707.06484</li> <li>Code: https://github.com/ucbdrive/dla</li> </ul>"},{"location":"models/#dual-path-networks-dpnpy","title":"Dual-Path Networks [dpn.py]","text":"<ul> <li>Paper: <code>Dual Path Networks</code> - https://arxiv.org/abs/1707.01629</li> <li>My PyTorch code: https://github.com/rwightman/pytorch-dpn-pretrained</li> <li>Reference code: https://github.com/cypw/DPNs</li> </ul>"},{"location":"models/#gpu-efficient-networks-byobnetpy","title":"GPU-Efficient Networks [byobnet.py]","text":"<ul> <li>Paper: <code>Neural Architecture Design for GPU-Efficient Networks</code> - https://arxiv.org/abs/2006.14090</li> <li>Reference code: https://github.com/idstcv/GPU-Efficient-Networks</li> </ul>"},{"location":"models/#hrnet-hrnetpy","title":"HRNet [hrnet.py]","text":"<ul> <li>Paper: <code>Deep High-Resolution Representation Learning for Visual Recognition</code> - https://arxiv.org/abs/1908.07919</li> <li>Code: https://github.com/HRNet/HRNet-Image-Classification</li> </ul>"},{"location":"models/#inception-v3-inception_v3py","title":"Inception-V3 [inception_v3.py]","text":"<ul> <li>Paper: <code>Rethinking the Inception Architecture for Computer Vision</code> - https://arxiv.org/abs/1512.00567</li> <li>Code: https://github.com/pytorch/vision/tree/master/torchvision/models</li> </ul>"},{"location":"models/#inception-v4-inception_v4py","title":"Inception-V4 [inception_v4.py]","text":"<ul> <li>Paper: <code>Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</code> - https://arxiv.org/abs/1602.07261</li> <li>Code: https://github.com/Cadene/pretrained-models.pytorch</li> <li>Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets</li> </ul>"},{"location":"models/#inception-resnet-v2-inception_resnet_v2py","title":"Inception-ResNet-V2 [inception_resnet_v2.py]","text":"<ul> <li>Paper: <code>Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</code> - https://arxiv.org/abs/1602.07261</li> <li>Code: https://github.com/Cadene/pretrained-models.pytorch</li> <li>Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets</li> </ul>"},{"location":"models/#nasnet-a-nasnetpy","title":"NASNet-A [nasnet.py]","text":"<ul> <li>Papers: <code>Learning Transferable Architectures for Scalable Image Recognition</code> - https://arxiv.org/abs/1707.07012</li> <li>Code: https://github.com/Cadene/pretrained-models.pytorch</li> <li>Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet</li> </ul>"},{"location":"models/#pnasnet-5-pnasnetpy","title":"PNasNet-5 [pnasnet.py]","text":"<ul> <li>Papers: <code>Progressive Neural Architecture Search</code> - https://arxiv.org/abs/1712.00559</li> <li>Code: https://github.com/Cadene/pretrained-models.pytorch</li> <li>Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet</li> </ul>"},{"location":"models/#efficientnet-efficientnetpy","title":"EfficientNet [efficientnet.py]","text":"<ul> <li>Papers:<ul> <li>EfficientNet NoisyStudent (B0-B7, L2) - https://arxiv.org/abs/1911.04252</li> <li>EfficientNet AdvProp (B0-B8) - https://arxiv.org/abs/1911.09665</li> <li>EfficientNet (B0-B7) - https://arxiv.org/abs/1905.11946</li> <li>EfficientNet-EdgeTPU (S, M, L) - https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html</li> <li>MixNet - https://arxiv.org/abs/1907.09595</li> <li>MNASNet B1, A1 (Squeeze-Excite), and Small - https://arxiv.org/abs/1807.11626</li> <li>MobileNet-V2 - https://arxiv.org/abs/1801.04381</li> <li>FBNet-C - https://arxiv.org/abs/1812.03443</li> <li>Single-Path NAS - https://arxiv.org/abs/1904.02877</li> </ul> </li> <li>My PyTorch code: https://github.com/rwightman/gen-efficientnet-pytorch</li> <li>Reference code: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet</li> </ul>"},{"location":"models/#mobilenet-v3-mobilenetv3py","title":"MobileNet-V3 [mobilenetv3.py]","text":"<ul> <li>Paper: <code>Searching for MobileNetV3</code> - https://arxiv.org/abs/1905.02244</li> <li>Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet</li> </ul>"},{"location":"models/#regnet-regnetpy","title":"RegNet [regnet.py]","text":"<ul> <li>Paper: <code>Designing Network Design Spaces</code> - https://arxiv.org/abs/2003.13678</li> <li>Reference code: https://github.com/facebookresearch/pycls/blob/master/pycls/models/regnet.py</li> </ul>"},{"location":"models/#repvgg-byobnetpy","title":"RepVGG [byobnet.py]","text":"<ul> <li>Paper: <code>Making VGG-style ConvNets Great Again</code> - https://arxiv.org/abs/2101.03697</li> <li>Reference code: https://github.com/DingXiaoH/RepVGG</li> </ul>"},{"location":"models/#resnet-resnext-resnetpy","title":"ResNet, ResNeXt [resnet.py]","text":"<ul> <li>ResNet (V1B)<ul> <li>Paper: <code>Deep Residual Learning for Image Recognition</code> - https://arxiv.org/abs/1512.03385</li> <li>Code: https://github.com/pytorch/vision/tree/master/torchvision/models</li> </ul> </li> <li>ResNeXt<ul> <li>Paper: <code>Aggregated Residual Transformations for Deep Neural Networks</code> - https://arxiv.org/abs/1611.05431</li> <li>Code: https://github.com/pytorch/vision/tree/master/torchvision/models</li> </ul> </li> <li>'Bag of Tricks' / Gluon C, D, E, S ResNet variants<ul> <li>Paper: <code>Bag of Tricks for Image Classification with CNNs</code> - https://arxiv.org/abs/1812.01187</li> <li>Code: https://github.com/dmlc/gluon-cv/blob/master/gluoncv/model_zoo/resnetv1b.py</li> </ul> </li> <li>Instagram pretrained / ImageNet tuned ResNeXt101<ul> <li>Paper: <code>Exploring the Limits of Weakly Supervised Pretraining</code> - https://arxiv.org/abs/1805.00932</li> <li>Weights: https://pytorch.org/hub/facebookresearch_WSL-Images_resnext (NOTE: CC BY-NC 4.0 License, NOT commercial friendly)</li> </ul> </li> <li>Semi-supervised (SSL) / Semi-weakly Supervised (SWSL) ResNet and ResNeXts<ul> <li>Paper: <code>Billion-scale semi-supervised learning for image classification</code> - https://arxiv.org/abs/1905.00546</li> <li>Weights: https://github.com/facebookresearch/semi-supervised-ImageNet1K-models (NOTE: CC BY-NC 4.0 License, NOT commercial friendly)</li> </ul> </li> <li>Squeeze-and-Excitation Networks<ul> <li>Paper: <code>Squeeze-and-Excitation Networks</code> - https://arxiv.org/abs/1709.01507</li> <li>Code: Added to ResNet base, this is current version going forward, old <code>senet.py</code> is being deprecated</li> </ul> </li> <li>ECAResNet (ECA-Net)<ul> <li>Paper: <code>ECA-Net: Efficient Channel Attention for Deep CNN</code> - https://arxiv.org/abs/1910.03151v4</li> <li>Code: Added to ResNet base, ECA module contributed by @VRandme, reference https://github.com/BangguWu/ECANet</li> </ul> </li> </ul>"},{"location":"models/#res2net-res2netpy","title":"Res2Net [res2net.py]","text":"<ul> <li>Paper: <code>Res2Net: A New Multi-scale Backbone Architecture</code> - https://arxiv.org/abs/1904.01169</li> <li>Code: https://github.com/gasvn/Res2Net</li> </ul>"},{"location":"models/#resnest-resnestpy","title":"ResNeSt [resnest.py]","text":"<ul> <li>Paper: <code>ResNeSt: Split-Attention Networks</code> - https://arxiv.org/abs/2004.08955</li> <li>Code: https://github.com/zhanghang1989/ResNeSt</li> </ul>"},{"location":"models/#rexnet-rexnetpy","title":"ReXNet [rexnet.py]","text":"<ul> <li>Paper: <code>ReXNet: Diminishing Representational Bottleneck on CNN</code> - https://arxiv.org/abs/2007.00992</li> <li>Code: https://github.com/clovaai/rexnet</li> </ul>"},{"location":"models/#selective-kernel-networks-sknetpy","title":"Selective-Kernel Networks [sknet.py]","text":"<ul> <li>Paper: <code>Selective-Kernel Networks</code> - https://arxiv.org/abs/1903.06586</li> <li>Code: https://github.com/implus/SKNet, https://github.com/clovaai/assembled-cnn</li> </ul>"},{"location":"models/#selecsls-selecslspy","title":"SelecSLS [selecsls.py]","text":"<ul> <li>Paper: <code>XNect: Real-time Multi-Person 3D Motion Capture with a Single RGB Camera</code> - https://arxiv.org/abs/1907.00837</li> <li>Code: https://github.com/mehtadushy/SelecSLS-Pytorch</li> </ul>"},{"location":"models/#squeeze-and-excitation-networks-senetpy","title":"Squeeze-and-Excitation Networks [senet.py]","text":"<p>NOTE: I am deprecating this version of the networks, the new ones are part of <code>resnet.py</code></p> <ul> <li>Paper: <code>Squeeze-and-Excitation Networks</code> - https://arxiv.org/abs/1709.01507</li> <li>Code: https://github.com/Cadene/pretrained-models.pytorch </li> </ul>"},{"location":"models/#tresnet-tresnetpy","title":"TResNet [tresnet.py]","text":"<ul> <li>Paper: <code>TResNet: High Performance GPU-Dedicated Architecture</code> - https://arxiv.org/abs/2003.13630</li> <li>Code: https://github.com/mrT23/TResNet</li> </ul>"},{"location":"models/#vgg-vggpy","title":"VGG [vgg.py]","text":"<ul> <li>Paper: <code>Very Deep Convolutional Networks For Large-Scale Image Recognition</code> - https://arxiv.org/pdf/1409.1556.pdf</li> <li>Reference code: https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py</li> </ul>"},{"location":"models/#vision-transformer-vision_transformerpy","title":"Vision Transformer [vision_transformer.py]","text":"<ul> <li>Paper: <code>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</code> - https://arxiv.org/abs/2010.11929</li> <li>Reference code and pretrained weights: https://github.com/google-research/vision_transformer</li> </ul>"},{"location":"models/#vovnet-v2-and-v1-vovnetpy","title":"VovNet V2 and V1 [vovnet.py]","text":"<ul> <li>Paper: <code>CenterMask : Real-Time Anchor-Free Instance Segmentation</code> - https://arxiv.org/abs/1911.06667</li> <li>Reference code: https://github.com/youngwanLEE/vovnet-detectron2</li> </ul>"},{"location":"models/#xception-xceptionpy","title":"Xception [xception.py]","text":"<ul> <li>Paper: <code>Xception: Deep Learning with Depthwise Separable Convolutions</code> - https://arxiv.org/abs/1610.02357</li> <li>Code: https://github.com/Cadene/pretrained-models.pytorch</li> </ul>"},{"location":"models/#xception-modified-aligned-gluon-gluon_xceptionpy","title":"Xception (Modified Aligned, Gluon) [gluon_xception.py]","text":"<ul> <li>Paper: <code>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</code> - https://arxiv.org/abs/1802.02611</li> <li>Reference code: https://github.com/dmlc/gluon-cv/tree/master/gluoncv/model_zoo, https://github.com/jfzhang95/pytorch-deeplab-xception/</li> </ul>"},{"location":"models/#xception-modified-aligned-tf-aligned_xceptionpy","title":"Xception (Modified Aligned, TF) [aligned_xception.py]","text":"<ul> <li>Paper: <code>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</code> - https://arxiv.org/abs/1802.02611</li> <li>Reference code: https://github.com/tensorflow/models/tree/master/research/deeplab</li> </ul>"},{"location":"results/","title":"Results","text":"<p>CSV files containing an ImageNet-1K and out-of-distribution (OOD) test set validation results for all models with pretrained weights is located in the repository results folder.</p>"},{"location":"results/#self-trained-weights","title":"Self-trained Weights","text":"<p>The table below includes ImageNet-1k validation results of model weights that I've trained myself. It is not updated as frequently as the csv results outputs linked above.</p>    Model Acc@1 (Err) Acc@5 (Err) Param # (M) Interpolation Image Size     efficientnet_b3a 82.242 (17.758) 96.114 (3.886) 12.23 bicubic 320 (1.0 crop)   efficientnet_b3 82.076 (17.924) 96.020 (3.980) 12.23 bicubic 300   regnet_32 82.002 (17.998) 95.906 (4.094) 19.44 bicubic 224   skresnext50d_32x4d 81.278 (18.722) 95.366 (4.634) 27.5 bicubic 288 (1.0 crop)   seresnext50d_32x4d 81.266 (18.734) 95.620 (4.380) 27.6 bicubic 224   efficientnet_b2a 80.608 (19.392) 95.310 (4.690) 9.11 bicubic 288 (1.0 crop)   resnet50d 80.530 (19.470) 95.160 (4.840) 25.6 bicubic 224   mixnet_xl 80.478 (19.522) 94.932 (5.068) 11.90 bicubic 224   efficientnet_b2 80.402 (19.598) 95.076 (4.924) 9.11 bicubic 260   seresnet50 80.274 (19.726) 95.070 (4.930) 28.1 bicubic 224   skresnext50d_32x4d 80.156 (19.844) 94.642 (5.358) 27.5 bicubic 224   cspdarknet53 80.058 (19.942) 95.084 (4.916) 27.6 bicubic 256   cspresnext50 80.040 (19.960) 94.944 (5.056) 20.6 bicubic 224   resnext50_32x4d 79.762 (20.238) 94.600 (5.400) 25 bicubic 224   resnext50d_32x4d 79.674 (20.326) 94.868 (5.132) 25.1 bicubic 224   cspresnet50 79.574 (20.426) 94.712 (5.288) 21.6 bicubic 256   ese_vovnet39b 79.320 (20.680) 94.710 (5.290) 24.6 bicubic 224   resnetblur50 79.290 (20.710) 94.632 (5.368) 25.6 bicubic 224   dpn68b 79.216 (20.784) 94.414 (5.586) 12.6 bicubic 224   resnet50 79.038 (20.962) 94.390 (5.610) 25.6 bicubic 224   mixnet_l 78.976 (21.024 94.184 (5.816) 7.33 bicubic 224   efficientnet_b1 78.692 (21.308) 94.086 (5.914) 7.79 bicubic 240   efficientnet_es 78.066 (21.934) 93.926 (6.074) 5.44 bicubic 224   seresnext26t_32x4d 77.998 (22.002) 93.708 (6.292) 16.8 bicubic 224   seresnext26tn_32x4d 77.986 (22.014) 93.746 (6.254) 16.8 bicubic 224   efficientnet_b0 77.698 (22.302) 93.532 (6.468) 5.29 bicubic 224   seresnext26d_32x4d 77.602 (22.398) 93.608 (6.392) 16.8 bicubic 224   mobilenetv2_120d 77.294 (22.706 93.502 (6.498) 5.8 bicubic 224   mixnet_m 77.256 (22.744) 93.418 (6.582) 5.01 bicubic 224   resnet34d 77.116 (22.884) 93.382 (6.618) 21.8 bicubic 224   seresnext26_32x4d 77.104 (22.896) 93.316 (6.684) 16.8 bicubic 224   skresnet34 76.912 (23.088) 93.322 (6.678) 22.2 bicubic 224   ese_vovnet19b_dw 76.798 (23.202) 93.268 (6.732) 6.5 bicubic 224   resnet26d 76.68 (23.32) 93.166 (6.834) 16 bicubic 224   densenetblur121d 76.576 (23.424) 93.190 (6.810) 8.0 bicubic 224   mobilenetv2_140 76.524 (23.476) 92.990 (7.010) 6.1 bicubic 224   mixnet_s 75.988 (24.012) 92.794 (7.206) 4.13 bicubic 224   mobilenetv3_large_100 75.766 (24.234) 92.542 (7.458) 5.5 bicubic 224   mobilenetv3_rw 75.634 (24.366) 92.708 (7.292) 5.5 bicubic 224   mnasnet_a1 75.448 (24.552) 92.604 (7.396) 3.89 bicubic 224   resnet26 75.292 (24.708) 92.57 (7.43) 16 bicubic 224   fbnetc_100 75.124 (24.876) 92.386 (7.614) 5.6 bilinear 224   resnet34 75.110 (24.890) 92.284 (7.716) 22 bilinear 224   mobilenetv2_110d 75.052 (24.948) 92.180 (7.820) 4.5 bicubic 224   seresnet34 74.808 (25.192) 92.124 (7.876) 22 bilinear 224   mnasnet_b1 74.658 (25.342) 92.114 (7.886) 4.38 bicubic 224   spnasnet_100 74.084 (25.916) 91.818 (8.182) 4.42 bilinear 224   skresnet18 73.038 (26.962) 91.168 (8.832) 11.9 bicubic 224   mobilenetv2_100 72.978 (27.022) 91.016 (8.984) 3.5 bicubic 224   resnet18d 72.260 (27.740) 90.696 (9.304) 11.7 bicubic 224   seresnet18 71.742 (28.258) 90.334 (9.666) 11.8 bicubic 224"},{"location":"results/#ported-and-other-weights","title":"Ported and Other Weights","text":"<p>For weights ported from other deep learning frameworks (Tensorflow, MXNet GluonCV) or copied from other PyTorch sources, please see the full results tables for ImageNet and various OOD test sets at in the results tables.</p> <p>Model code .py files contain links to original sources of models and weights.</p>"},{"location":"scripts/","title":"Scripts","text":"<p>A train, validation, inference, and checkpoint cleaning script included in the github root folder. Scripts are not currently packaged in the pip release.</p> <p>The training and validation scripts evolved from early versions of the PyTorch Imagenet Examples. I have added significant functionality over time, including CUDA specific performance enhancements based on NVIDIA's APEX Examples.</p>"},{"location":"scripts/#training-script","title":"Training Script","text":"<p>The variety of training args is large and not all combinations of options (or even options) have been fully tested. For the training dataset folder, specify the folder to the base that contains a <code>train</code> and <code>validation</code> folder.</p> <p>To train an SE-ResNet34 on ImageNet, locally distributed, 4 GPUs, one process per GPU w/ cosine schedule, random-erasing prob of 50% and per-pixel random value:</p> <p><code>./distributed_train.sh 4 /data/imagenet --model seresnet34 --sched cosine --epochs 150 --warmup-epochs 5 --lr 0.4 --reprob 0.5 --remode pixel --batch-size 256 --amp -j 4</code></p> <p>NOTE: It is recommended to use PyTorch 1.9+ w/ PyTorch native AMP and DDP instead of APEX AMP. <code>--amp</code> defaults to native AMP as of timm ver 0.4.3.  <code>--apex-amp</code> will force use of APEX components if they are installed.</p>"},{"location":"scripts/#validation-inference-scripts","title":"Validation / Inference Scripts","text":"<p>Validation and inference scripts are similar in usage. One outputs metrics on a validation set and the other outputs topk class ids in a csv. Specify the folder containing validation images, not the base as in training script. </p> <p>To validate with the model's pretrained weights (if they exist):</p> <p><code>python validate.py /imagenet/validation/ --model seresnext26_32x4d --pretrained</code></p> <p>To run inference from a checkpoint:</p> <p><code>python inference.py /imagenet/validation/ --model mobilenetv3_large_100 --checkpoint ./output/train/model_best.pth.tar</code></p>"},{"location":"training_hparam_examples/","title":"Training Examples","text":""},{"location":"training_hparam_examples/#efficientnet-b2-with-randaugment-804-top-1-951-top-5","title":"EfficientNet-B2 with RandAugment - 80.4 top-1, 95.1 top-5","text":"<p>These params are for dual Titan RTX cards with NVIDIA Apex installed:</p> <p><code>./distributed_train.sh 2 /imagenet/ --model efficientnet_b2 -b 128 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .97 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.3 --drop-path 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .016</code></p>"},{"location":"training_hparam_examples/#mixnet-xl-with-randaugment-805-top-1-949-top-5","title":"MixNet-XL with RandAugment - 80.5 top-1, 94.9 top-5","text":"<p>This params are for dual Titan RTX cards with NVIDIA Apex installed:</p> <p><code>./distributed_train.sh 2 /imagenet/ --model mixnet_xl -b 128 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .969 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.3 --drop-path 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.3 --amp --lr .016 --dist-bn reduce</code></p>"},{"location":"training_hparam_examples/#se-resnext-26-d-and-se-resnext-26-t","title":"SE-ResNeXt-26-D and SE-ResNeXt-26-T","text":"<p>These hparams (or similar) work well for a wide range of ResNet architecture, generally a good idea to increase the epoch # as the model size increases... ie approx 180-200 for ResNe(X)t50, and 220+ for larger. Increase batch size and LR proportionally for better GPUs or with AMP enabled. These params were for 2 1080Ti cards:</p> <p><code>./distributed_train.sh 2 /imagenet/ --model seresnext26t_32x4d --lr 0.1 --warmup-epochs 5 --epochs 160 --weight-decay 1e-4 --sched cosine --reprob 0.4 --remode pixel -b 112</code></p>"},{"location":"training_hparam_examples/#efficientnet-b3-with-randaugment-815-top-1-957-top-5","title":"EfficientNet-B3 with RandAugment - 81.5 top-1, 95.7 top-5","text":"<p>The training of this model started with the same command line as EfficientNet-B2 w/ RA above. After almost three weeks of training the process crashed. The results weren't looking amazing so I resumed the training several times with tweaks to a few params (increase RE prob, decrease rand-aug, increase ema-decay). Nothing looked great. I ended up averaging the best checkpoints from all restarts. The result is mediocre at default res/crop but oddly performs much better with a full image test crop of 1.0. </p>"},{"location":"training_hparam_examples/#efficientnet-b0-with-randaugment-777-top-1-953-top-5","title":"EfficientNet-B0 with RandAugment - 77.7 top-1, 95.3 top-5","text":"<p>Michael Klachko achieved these results with the command line for B2 adapted for larger batch size, with the recommended B0 dropout rate of 0.2.</p> <p><code>./distributed_train.sh 2 /imagenet/ --model efficientnet_b0 -b 384 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .97 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-path 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .048</code></p>"},{"location":"training_hparam_examples/#resnet50-with-jsd-loss-and-randaugment-clean-2x-ra-augs-7904-top-1-9439-top-5","title":"ResNet50 with JSD loss and RandAugment (clean + 2x RA augs) - 79.04 top-1, 94.39 top-5","text":"<p>Trained on two older 1080Ti cards, this took a while. Only slightly, non statistically better ImageNet validation result than my first good AugMix training of 78.99. However, these weights are more robust on tests with ImageNetV2, ImageNet-Sketch, etc. Unlike my first AugMix runs, I've enabled SplitBatchNorm, disabled random erasing on the clean split, and cranked up random erasing prob on the 2 augmented paths.</p> <p><code>./distributed_train.sh 2 /imagenet -b 64 --model resnet50 --sched cosine --epochs 200 --lr 0.05 --amp --remode pixel --reprob 0.6 --aug-splits 3 --aa rand-m9-mstd0.5-inc1 --resplit --split-bn --jsd --dist-bn reduce</code></p>"},{"location":"training_hparam_examples/#efficientnet-es-edgetpu-small-with-randaugment-78066-top-1-93926-top-5","title":"EfficientNet-ES (EdgeTPU-Small) with RandAugment - 78.066 top-1, 93.926 top-5","text":"<p>Trained by Andrew Lavin with 8 V100 cards. Model EMA was not used, final checkpoint is the average of 8 best checkpoints during training.</p> <p><code>./distributed_train.sh 8 /imagenet --model efficientnet_es -b 128 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .97 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-path 0.2  --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .064</code></p>"},{"location":"training_hparam_examples/#mobilenetv3-large-100-75766-top-1-92542-top-5","title":"MobileNetV3-Large-100 - 75.766 top-1, 92,542 top-5","text":"<p><code>./distributed_train.sh 2 /imagenet/ --model mobilenetv3_large_100 -b 512 --sched step --epochs 600 --decay-epochs 2.4 --decay-rate .973 --opt rmsproptf --opt-eps .001 -j 7 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-path 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .064 --lr-noise 0.42 0.9</code></p>"},{"location":"training_hparam_examples/#resnext-50-32x4d-w-randaugment-79762-top-1-9460-top-5","title":"ResNeXt-50 32x4d w/ RandAugment - 79.762 top-1, 94.60 top-5","text":"<p>These params will also work well for SE-ResNeXt-50 and SK-ResNeXt-50 and likely 101. I used them for the SK-ResNeXt-50 32x4d that I trained with 2 GPU using a slightly higher LR per effective batch size (lr=0.18, b=192 per GPU). The cmd line below are tuned for 8 GPU training.</p> <p><code>./distributed_train.sh 8 /imagenet --model resnext50_32x4d --lr 0.6 --warmup-epochs 5 --epochs 240 --weight-decay 1e-4 --sched cosine --reprob 0.4 --recount 3 --remode pixel --aa rand-m7-mstd0.5-inc1 -b 192 -j 6 --amp --dist-bn reduce</code></p>"},{"location":"models/adversarial-inception-v3/","title":"Adversarial Inception v3","text":"<p>Inception v3 is a convolutional neural network architecture from the Inception family that makes several improvements including using Label Smoothing, Factorized 7 x 7 convolutions, and the use of an auxiliary classifer to propagate label information lower down the network (along with the use of batch normalization for layers in the sidehead). The key building block is an Inception Module.</p> <p>This particular model was trained for study of adversarial examples (adversarial training).</p> <p>The weights from this model were ported from Tensorflow/Models.</p>"},{"location":"models/adversarial-inception-v3/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('adv_inception_v3', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>adv_inception_v3</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/adversarial-inception-v3/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('adv_inception_v3', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/adversarial-inception-v3/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/adversarial-inception-v3/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-1804-00097,\n  author    = {Alexey Kurakin and\n               Ian J. Goodfellow and\n               Samy Bengio and\n               Yinpeng Dong and\n               Fangzhou Liao and\n               Ming Liang and\n               Tianyu Pang and\n               Jun Zhu and\n               Xiaolin Hu and\n               Cihang Xie and\n               Jianyu Wang and\n               Zhishuai Zhang and\n               Zhou Ren and\n               Alan L. Yuille and\n               Sangxia Huang and\n               Yao Zhao and\n               Yuzhe Zhao and\n               Zhonglin Han and\n               Junjiajia Long and\n               Yerkebulan Berdibekov and\n               Takuya Akiba and\n               Seiya Tokui and\n               Motoki Abe},\n  title     = {Adversarial Attacks and Defences Competition},\n  journal   = {CoRR},\n  volume    = {abs/1804.00097},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1804.00097},\n  archivePrefix = {arXiv},\n  eprint    = {1804.00097},\n  timestamp = {Thu, 31 Oct 2019 16:31:22 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-00097.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/advprop/","title":"AdvProp (EfficientNet)","text":"<p>AdvProp is an adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to the method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples.</p> <p>The weights from this model were ported from Tensorflow/TPU.</p>"},{"location":"models/advprop/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('tf_efficientnet_b0_ap', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>tf_efficientnet_b0_ap</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/advprop/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('tf_efficientnet_b0_ap', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/advprop/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/advprop/#citation","title":"Citation","text":"<pre><code>@misc{xie2020adversarial,\n      title={Adversarial Examples Improve Image Recognition}, \n      author={Cihang Xie and Mingxing Tan and Boqing Gong and Jiang Wang and Alan Yuille and Quoc V. Le},\n      year={2020},\n      eprint={1911.09665},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/big-transfer/","title":"Big Transfer (BiT)","text":"<p>Big Transfer (BiT) is a type of pretraining recipe that pre-trains  on a large supervised source dataset, and fine-tunes the weights on the target task. Models are trained on the JFT-300M dataset. The finetuned models contained in this collection are finetuned on ImageNet.</p>"},{"location":"models/big-transfer/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('resnetv2_101x1_bitm', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>resnetv2_101x1_bitm</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/big-transfer/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('resnetv2_101x1_bitm', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/big-transfer/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/big-transfer/#citation","title":"Citation","text":"<pre><code>@misc{kolesnikov2020big,\n      title={Big Transfer (BiT): General Visual Representation Learning}, \n      author={Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Joan Puigcerver and Jessica Yung and Sylvain Gelly and Neil Houlsby},\n      year={2020},\n      eprint={1912.11370},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/csp-darknet/","title":"CSP-DarkNet","text":"<p>CSPDarknet53 is a convolutional neural network and backbone for object detection that uses DarkNet-53. It employs a CSPNet strategy to partition the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network. </p> <p>This CNN is used as the backbone for YOLOv4.</p>"},{"location":"models/csp-darknet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('cspdarknet53', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>cspdarknet53</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/csp-darknet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('cspdarknet53', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/csp-darknet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/csp-darknet/#citation","title":"Citation","text":"<pre><code>@misc{bochkovskiy2020yolov4,\n      title={YOLOv4: Optimal Speed and Accuracy of Object Detection}, \n      author={Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},\n      year={2020},\n      eprint={2004.10934},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/csp-resnet/","title":"CSP-ResNet","text":"<p>CSPResNet is a convolutional neural network where we apply the Cross Stage Partial Network (CSPNet) approach to ResNet. The CSPNet partitions the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network.</p>"},{"location":"models/csp-resnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('cspresnet50', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>cspresnet50</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/csp-resnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('cspresnet50', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/csp-resnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/csp-resnet/#citation","title":"Citation","text":"<pre><code>@misc{wang2019cspnet,\n      title={CSPNet: A New Backbone that can Enhance Learning Capability of CNN}, \n      author={Chien-Yao Wang and Hong-Yuan Mark Liao and I-Hau Yeh and Yueh-Hua Wu and Ping-Yang Chen and Jun-Wei Hsieh},\n      year={2019},\n      eprint={1911.11929},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/csp-resnext/","title":"CSP-ResNeXt","text":"<p>CSPResNeXt is a convolutional neural network where we apply the Cross Stage Partial Network (CSPNet) approach to ResNeXt. The CSPNet partitions the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network.</p>"},{"location":"models/csp-resnext/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('cspresnext50', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>cspresnext50</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/csp-resnext/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('cspresnext50', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/csp-resnext/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/csp-resnext/#citation","title":"Citation","text":"<pre><code>@misc{wang2019cspnet,\n      title={CSPNet: A New Backbone that can Enhance Learning Capability of CNN}, \n      author={Chien-Yao Wang and Hong-Yuan Mark Liao and I-Hau Yeh and Yueh-Hua Wu and Ping-Yang Chen and Jun-Wei Hsieh},\n      year={2019},\n      eprint={1911.11929},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/densenet/","title":"DenseNet","text":"<p>DenseNet is a type of convolutional neural network that utilises dense connections between layers, through Dense Blocks, where we connect all layers (with matching feature-map sizes) directly with each other. To preserve the feed-forward nature, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers.</p> <p>The DenseNet Blur variant in this collection by Ross Wightman employs Blur Pooling</p>"},{"location":"models/densenet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('densenet121', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>densenet121</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/densenet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('densenet121', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/densenet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/densenet/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/HuangLW16a,\n  author    = {Gao Huang and\n               Zhuang Liu and\n               Kilian Q. Weinberger},\n  title     = {Densely Connected Convolutional Networks},\n  journal   = {CoRR},\n  volume    = {abs/1608.06993},\n  year      = {2016},\n  url       = {http://arxiv.org/abs/1608.06993},\n  archivePrefix = {arXiv},\n  eprint    = {1608.06993},\n  timestamp = {Mon, 10 Sep 2018 15:49:32 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/HuangLW16a.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> <pre><code>@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/rwightman/pytorch-image-models}}\n}\n</code></pre>"},{"location":"models/dla/","title":"Deep Layer Aggregation","text":"<p>Extending  \u201cshallow\u201d skip connections, Dense Layer Aggregation (DLA) incorporates more depth and sharing. The authors introduce two structures for deep layer aggregation (DLA): iterative deep aggregation (IDA) and hierarchical deep aggregation (HDA). These structures are expressed through an architectural framework, independent of the choice of backbone, for compatibility with current and future networks. </p> <p>IDA focuses on fusing resolutions and scales while HDA focuses on merging features from all modules and channels. IDA follows the base hierarchy to refine resolution and aggregate scale stage-bystage. HDA assembles its own hierarchy of tree-structured connections that cross and merge stages to aggregate different levels of representation. </p>"},{"location":"models/dla/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('dla102', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>dla102</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/dla/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('dla102', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/dla/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/dla/#citation","title":"Citation","text":"<pre><code>@misc{yu2019deep,\n      title={Deep Layer Aggregation}, \n      author={Fisher Yu and Dequan Wang and Evan Shelhamer and Trevor Darrell},\n      year={2019},\n      eprint={1707.06484},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/dpn/","title":"Dual Path Network (DPN)","text":"<p>A Dual Path Network (DPN) is a convolutional neural network which presents a new topology of connection paths internally. The intuition is that ResNets enables feature re-usage while DenseNet enables new feature exploration, and both are important for learning good representations. To enjoy the benefits from both path topologies, Dual Path Networks share common features while maintaining the flexibility to explore new features through dual path architectures. </p> <p>The principal building block is an DPN Block.</p>"},{"location":"models/dpn/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('dpn107', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>dpn107</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/dpn/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('dpn107', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/dpn/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/dpn/#citation","title":"Citation","text":"<pre><code>@misc{chen2017dual,\n      title={Dual Path Networks}, \n      author={Yunpeng Chen and Jianan Li and Huaxin Xiao and Xiaojie Jin and Shuicheng Yan and Jiashi Feng},\n      year={2017},\n      eprint={1707.01629},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/ecaresnet/","title":"ECA-ResNet","text":"<p>An ECA ResNet is a variant on a ResNet that utilises an Efficient Channel Attention module. Efficient Channel Attention is an architectural unit based on squeeze-and-excitation blocks that reduces model complexity without dimensionality reduction. </p>"},{"location":"models/ecaresnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('ecaresnet101d', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>ecaresnet101d</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/ecaresnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('ecaresnet101d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/ecaresnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/ecaresnet/#citation","title":"Citation","text":"<pre><code>@misc{wang2020ecanet,\n      title={ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks}, \n      author={Qilong Wang and Banggu Wu and Pengfei Zhu and Peihua Li and Wangmeng Zuo and Qinghua Hu},\n      year={2020},\n      eprint={1910.03151},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/efficientnet-pruned/","title":"EfficientNet (Knapsack Pruned)","text":"<p>EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient. Unlike conventional practice that arbitrary scales  these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use 2^N times more computational resources, then we can simply increase the network depth by \\alpha ^ N\\alpha ^ N,  width by \\beta ^ N\\beta ^ N, and image size by \\gamma ^ N\\gamma ^ N, where \\alpha, \\beta, \\gamma\\alpha, \\beta, \\gamma are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\phi\\phi to uniformly scales network width, depth, and resolution in a  principled way.</p> <p>The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image.</p> <p>The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2, in addition to squeeze-and-excitation blocks.</p> <p>This collection consists of pruned EfficientNet models.</p>"},{"location":"models/efficientnet-pruned/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('efficientnet_b1_pruned', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>efficientnet_b1_pruned</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/efficientnet-pruned/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('efficientnet_b1_pruned', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/efficientnet-pruned/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/efficientnet-pruned/#citation","title":"Citation","text":"<pre><code>@misc{tan2020efficientnet,\n      title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}, \n      author={Mingxing Tan and Quoc V. Le},\n      year={2020},\n      eprint={1905.11946},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n</code></pre> <pre><code>@misc{aflalo2020knapsack,\n      title={Knapsack Pruning with Inner Distillation},\n      author={Yonathan Aflalo and Asaf Noy and Ming Lin and Itamar Friedman and Lihi Zelnik},\n      year={2020},\n      eprint={2002.08258},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n</code></pre>"},{"location":"models/efficientnet/","title":"EfficientNet","text":"<p>EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient. Unlike conventional practice that arbitrary scales  these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use 2^N times more computational resources, then we can simply increase the network depth by \\alpha ^ N\\alpha ^ N,  width by \\beta ^ N\\beta ^ N, and image size by \\gamma ^ N\\gamma ^ N, where \\alpha, \\beta, \\gamma\\alpha, \\beta, \\gamma are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\phi\\phi to uniformly scales network width, depth, and resolution in a  principled way.</p> <p>The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image.</p> <p>The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2, in addition to squeeze-and-excitation blocks.</p>"},{"location":"models/efficientnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('efficientnet_b0', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>efficientnet_b0</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/efficientnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/efficientnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/efficientnet/#citation","title":"Citation","text":"<pre><code>@misc{tan2020efficientnet,\n      title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}, \n      author={Mingxing Tan and Quoc V. Le},\n      year={2020},\n      eprint={1905.11946},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n</code></pre>"},{"location":"models/ensemble-adversarial/","title":"# Ensemble Adversarial Inception ResNet v2","text":"<p>Inception-ResNet-v2 is a convolutional neural architecture that builds on the Inception family of architectures but incorporates residual connections (replacing the filter concatenation stage of the Inception architecture).</p> <p>This particular model was trained for study of adversarial examples (adversarial training).</p> <p>The weights from this model were ported from Tensorflow/Models.</p>"},{"location":"models/ensemble-adversarial/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('ens_adv_inception_resnet_v2', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>ens_adv_inception_resnet_v2</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/ensemble-adversarial/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('ens_adv_inception_resnet_v2', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/ensemble-adversarial/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/ensemble-adversarial/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-1804-00097,\n  author    = {Alexey Kurakin and\n               Ian J. Goodfellow and\n               Samy Bengio and\n               Yinpeng Dong and\n               Fangzhou Liao and\n               Ming Liang and\n               Tianyu Pang and\n               Jun Zhu and\n               Xiaolin Hu and\n               Cihang Xie and\n               Jianyu Wang and\n               Zhishuai Zhang and\n               Zhou Ren and\n               Alan L. Yuille and\n               Sangxia Huang and\n               Yao Zhao and\n               Yuzhe Zhao and\n               Zhonglin Han and\n               Junjiajia Long and\n               Yerkebulan Berdibekov and\n               Takuya Akiba and\n               Seiya Tokui and\n               Motoki Abe},\n  title     = {Adversarial Attacks and Defences Competition},\n  journal   = {CoRR},\n  volume    = {abs/1804.00097},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1804.00097},\n  archivePrefix = {arXiv},\n  eprint    = {1804.00097},\n  timestamp = {Thu, 31 Oct 2019 16:31:22 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-00097.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/ese-vovnet/","title":"ESE-VoVNet","text":"<p>VoVNet is a convolutional neural network that seeks to make DenseNet more efficient by concatenating all features only once in the last feature map, which makes input size constant and enables enlarging new output channel. </p> <p>Read about one-shot aggregation here.</p>"},{"location":"models/ese-vovnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('ese_vovnet19b_dw', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>ese_vovnet19b_dw</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/ese-vovnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('ese_vovnet19b_dw', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/ese-vovnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/ese-vovnet/#citation","title":"Citation","text":"<pre><code>@misc{lee2019energy,\n      title={An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection}, \n      author={Youngwan Lee and Joong-won Hwang and Sangrok Lee and Yuseok Bae and Jongyoul Park},\n      year={2019},\n      eprint={1904.09730},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/fbnet/","title":"FBNet","text":"<p>FBNet is a type of convolutional neural architectures discovered through DNAS neural architecture search. It utilises a basic type of image model block inspired by MobileNetv2 that utilises depthwise convolutions and an inverted residual structure (see components).</p> <p>The principal building block is the FBNet Block.</p>"},{"location":"models/fbnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('fbnetc_100', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>fbnetc_100</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/fbnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('fbnetc_100', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/fbnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/fbnet/#citation","title":"Citation","text":"<pre><code>@misc{wu2019fbnet,\n      title={FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search}, \n      author={Bichen Wu and Xiaoliang Dai and Peizhao Zhang and Yanghan Wang and Fei Sun and Yiming Wu and Yuandong Tian and Peter Vajda and Yangqing Jia and Kurt Keutzer},\n      year={2019},\n      eprint={1812.03443},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/gloun-inception-v3/","title":"(Gluon) Inception v3","text":"<p>Inception v3 is a convolutional neural network architecture from the Inception family that makes several improvements including using Label Smoothing, Factorized 7 x 7 convolutions, and the use of an auxiliary classifer to propagate label information lower down the network (along with the use of batch normalization for layers in the sidehead). The key building block is an Inception Module.</p> <p>The weights from this model were ported from Gluon.</p>"},{"location":"models/gloun-inception-v3/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('gluon_inception_v3', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>gluon_inception_v3</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/gloun-inception-v3/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('gluon_inception_v3', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/gloun-inception-v3/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/gloun-inception-v3/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/SzegedyVISW15,\n  author    = {Christian Szegedy and\n               Vincent Vanhoucke and\n               Sergey Ioffe and\n               Jonathon Shlens and\n               Zbigniew Wojna},\n  title     = {Rethinking the Inception Architecture for Computer Vision},\n  journal   = {CoRR},\n  volume    = {abs/1512.00567},\n  year      = {2015},\n  url       = {http://arxiv.org/abs/1512.00567},\n  archivePrefix = {arXiv},\n  eprint    = {1512.00567},\n  timestamp = {Mon, 13 Aug 2018 16:49:07 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/SzegedyVISW15.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/gloun-resnet/","title":"(Gluon) ResNet","text":"<p>Residual Networks, or ResNets, learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack residual blocks ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. </p> <p>The weights from this model were ported from Gluon.</p>"},{"location":"models/gloun-resnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('gluon_resnet101_v1b', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>gluon_resnet101_v1b</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/gloun-resnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('gluon_resnet101_v1b', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/gloun-resnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/gloun-resnet/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/HeZRS15,\n  author    = {Kaiming He and\n               Xiangyu Zhang and\n               Shaoqing Ren and\n               Jian Sun},\n  title     = {Deep Residual Learning for Image Recognition},\n  journal   = {CoRR},\n  volume    = {abs/1512.03385},\n  year      = {2015},\n  url       = {http://arxiv.org/abs/1512.03385},\n  archivePrefix = {arXiv},\n  eprint    = {1512.03385},\n  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/HeZRS15.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/gloun-resnext/","title":"(Gluon) ResNeXt","text":"<p>A ResNeXt repeats a building block that aggregates a set of transformations with the same topology. Compared to a ResNet, it exposes a new dimension,  cardinality (the size of the set of transformations) C, as an essential factor in addition to the dimensions of depth and width. </p> <p>The weights from this model were ported from Gluon.</p>"},{"location":"models/gloun-resnext/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('gluon_resnext101_32x4d', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>gluon_resnext101_32x4d</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/gloun-resnext/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('gluon_resnext101_32x4d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/gloun-resnext/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/gloun-resnext/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/XieGDTH16,\n  author    = {Saining Xie and\n               Ross B. Girshick and\n               Piotr Doll{\\'{a}}r and\n               Zhuowen Tu and\n               Kaiming He},\n  title     = {Aggregated Residual Transformations for Deep Neural Networks},\n  journal   = {CoRR},\n  volume    = {abs/1611.05431},\n  year      = {2016},\n  url       = {http://arxiv.org/abs/1611.05431},\n  archivePrefix = {arXiv},\n  eprint    = {1611.05431},\n  timestamp = {Mon, 13 Aug 2018 16:45:58 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/XieGDTH16.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/gloun-senet/","title":"(Gluon) SENet","text":"<p>A SENet is a convolutional neural network architecture that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration.</p> <p>The weights from this model were ported from Gluon.</p>"},{"location":"models/gloun-senet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('gluon_senet154', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>gluon_senet154</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/gloun-senet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('gluon_senet154', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/gloun-senet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/gloun-senet/#citation","title":"Citation","text":"<pre><code>@misc{hu2019squeezeandexcitation,\n      title={Squeeze-and-Excitation Networks}, \n      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},\n      year={2019},\n      eprint={1709.01507},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/gloun-seresnext/","title":"(Gluon) SE-ResNeXt","text":"<p>SE ResNeXt is a variant of a ResNext that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration.</p> <p>The weights from this model were ported from Gluon.</p>"},{"location":"models/gloun-seresnext/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('gluon_seresnext101_32x4d', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>gluon_seresnext101_32x4d</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/gloun-seresnext/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('gluon_seresnext101_32x4d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/gloun-seresnext/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/gloun-seresnext/#citation","title":"Citation","text":"<pre><code>@misc{hu2019squeezeandexcitation,\n      title={Squeeze-and-Excitation Networks}, \n      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},\n      year={2019},\n      eprint={1709.01507},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/gloun-xception/","title":"(Gluon) Xception","text":"<p>Xception is a convolutional neural network architecture that relies solely on depthwise separable convolution layers.</p> <p>The weights from this model were ported from Gluon.</p>"},{"location":"models/gloun-xception/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('gluon_xception65', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>gluon_xception65</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/gloun-xception/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('gluon_xception65', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/gloun-xception/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/gloun-xception/#citation","title":"Citation","text":"<pre><code>@misc{chollet2017xception,\n      title={Xception: Deep Learning with Depthwise Separable Convolutions}, \n      author={Fran\u00e7ois Chollet},\n      year={2017},\n      eprint={1610.02357},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/hrnet/","title":"HRNet","text":"<p>HRNet, or High-Resolution Net, is a general purpose convolutional neural network for tasks like semantic segmentation, object detection and image classification. It is able to maintain high resolution representations through the whole process. We start from a high-resolution convolution stream, gradually add high-to-low resolution convolution streams one by one, and connect the multi-resolution streams in parallel. The resulting network consists of several (4 in the paper) stages and the nnth stage contains nn streams corresponding to nn resolutions. The authors conduct repeated multi-resolution fusions by exchanging the information across the parallel streams over and over.</p>"},{"location":"models/hrnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('hrnet_w18', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>hrnet_w18</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/hrnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('hrnet_w18', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/hrnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/hrnet/#citation","title":"Citation","text":"<pre><code>@misc{sun2019highresolution,\n      title={High-Resolution Representations for Labeling Pixels and Regions}, \n      author={Ke Sun and Yang Zhao and Borui Jiang and Tianheng Cheng and Bin Xiao and Dong Liu and Yadong Mu and Xinggang Wang and Wenyu Liu and Jingdong Wang},\n      year={2019},\n      eprint={1904.04514},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/ig-resnext/","title":"Instagram ResNeXt WSL","text":"<p>A ResNeXt repeats a building block that aggregates a set of transformations with the same topology. Compared to a ResNet, it exposes a new dimension,  cardinality (the size of the set of transformations) C, as an essential factor in addition to the dimensions of depth and width. </p> <p>This model was trained on billions of Instagram images using thousands of distinct hashtags as labels exhibit excellent transfer learning performance. </p> <p>Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.</p>"},{"location":"models/ig-resnext/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('ig_resnext101_32x16d', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>ig_resnext101_32x16d</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/ig-resnext/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('ig_resnext101_32x16d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/ig-resnext/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/ig-resnext/#citation","title":"Citation","text":"<pre><code>@misc{mahajan2018exploring,\n      title={Exploring the Limits of Weakly Supervised Pretraining}, \n      author={Dhruv Mahajan and Ross Girshick and Vignesh Ramanathan and Kaiming He and Manohar Paluri and Yixuan Li and Ashwin Bharambe and Laurens van der Maaten},\n      year={2018},\n      eprint={1805.00932},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/inception-resnet-v2/","title":"Inception ResNet v2","text":"<p>Inception-ResNet-v2 is a convolutional neural architecture that builds on the Inception family of architectures but incorporates residual connections (replacing the filter concatenation stage of the Inception architecture).</p>"},{"location":"models/inception-resnet-v2/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('inception_resnet_v2', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>inception_resnet_v2</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/inception-resnet-v2/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('inception_resnet_v2', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/inception-resnet-v2/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/inception-resnet-v2/#citation","title":"Citation","text":"<pre><code>@misc{szegedy2016inceptionv4,\n      title={Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}, \n      author={Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alex Alemi},\n      year={2016},\n      eprint={1602.07261},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/inception-v3/","title":"Inception v3","text":"<p>Inception v3 is a convolutional neural network architecture from the Inception family that makes several improvements including using Label Smoothing, Factorized 7 x 7 convolutions, and the use of an auxiliary classifer to propagate label information lower down the network (along with the use of batch normalization for layers in the sidehead). The key building block is an Inception Module.</p>"},{"location":"models/inception-v3/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('inception_v3', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>inception_v3</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/inception-v3/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('inception_v3', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/inception-v3/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/inception-v3/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/SzegedyVISW15,\n  author    = {Christian Szegedy and\n               Vincent Vanhoucke and\n               Sergey Ioffe and\n               Jonathon Shlens and\n               Zbigniew Wojna},\n  title     = {Rethinking the Inception Architecture for Computer Vision},\n  journal   = {CoRR},\n  volume    = {abs/1512.00567},\n  year      = {2015},\n  url       = {http://arxiv.org/abs/1512.00567},\n  archivePrefix = {arXiv},\n  eprint    = {1512.00567},\n  timestamp = {Mon, 13 Aug 2018 16:49:07 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/SzegedyVISW15.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/inception-v4/","title":"Inception v4","text":"<p>Inception-v4 is a convolutional neural network architecture that builds on previous iterations of the Inception family by simplifying the architecture and using more inception modules than Inception-v3.</p>"},{"location":"models/inception-v4/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('inception_v4', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>inception_v4</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/inception-v4/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('inception_v4', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/inception-v4/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/inception-v4/#citation","title":"Citation","text":"<pre><code>@misc{szegedy2016inceptionv4,\n      title={Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}, \n      author={Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alex Alemi},\n      year={2016},\n      eprint={1602.07261},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/legacy-se-resnet/","title":"(Legacy) SE-ResNet","text":"<p>SE ResNet is a variant of a ResNet that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration.</p>"},{"location":"models/legacy-se-resnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('legacy_seresnet101', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>legacy_seresnet101</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/legacy-se-resnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('legacy_seresnet101', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/legacy-se-resnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/legacy-se-resnet/#citation","title":"Citation","text":"<pre><code>@misc{hu2019squeezeandexcitation,\n      title={Squeeze-and-Excitation Networks}, \n      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},\n      year={2019},\n      eprint={1709.01507},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/legacy-se-resnext/","title":"(Legacy) SE-ResNeXt","text":"<p>SE ResNeXt is a variant of a ResNeXt that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration.</p>"},{"location":"models/legacy-se-resnext/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('legacy_seresnext101_32x4d', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>legacy_seresnext101_32x4d</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/legacy-se-resnext/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('legacy_seresnext101_32x4d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/legacy-se-resnext/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/legacy-se-resnext/#citation","title":"Citation","text":"<pre><code>@misc{hu2019squeezeandexcitation,\n      title={Squeeze-and-Excitation Networks}, \n      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},\n      year={2019},\n      eprint={1709.01507},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/legacy-senet/","title":"(Legacy) SENet","text":"<p>A SENet is a convolutional neural network architecture that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration.</p> <p>The weights from this model were ported from Gluon.</p>"},{"location":"models/legacy-senet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('legacy_senet154', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>legacy_senet154</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/legacy-senet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('legacy_senet154', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/legacy-senet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/legacy-senet/#citation","title":"Citation","text":"<pre><code>@misc{hu2019squeezeandexcitation,\n      title={Squeeze-and-Excitation Networks}, \n      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},\n      year={2019},\n      eprint={1709.01507},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/mixnet/","title":"MixNet","text":"<p>MixNet is a type of convolutional neural network discovered via AutoML that utilises MixConvs instead of regular depthwise convolutions.</p>"},{"location":"models/mixnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('mixnet_l', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>mixnet_l</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/mixnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('mixnet_l', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/mixnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/mixnet/#citation","title":"Citation","text":"<pre><code>@misc{tan2019mixconv,\n      title={MixConv: Mixed Depthwise Convolutional Kernels}, \n      author={Mingxing Tan and Quoc V. Le},\n      year={2019},\n      eprint={1907.09595},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/mnasnet/","title":"MnasNet","text":"<p>MnasNet is a type of convolutional neural network optimized for mobile devices that is discovered through mobile neural architecture search, which explicitly incorporates model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. The main building block is an inverted residual block (from MobileNetV2).</p>"},{"location":"models/mnasnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('mnasnet_100', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>mnasnet_100</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/mnasnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('mnasnet_100', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/mnasnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/mnasnet/#citation","title":"Citation","text":"<pre><code>@misc{tan2019mnasnet,\n      title={MnasNet: Platform-Aware Neural Architecture Search for Mobile}, \n      author={Mingxing Tan and Bo Chen and Ruoming Pang and Vijay Vasudevan and Mark Sandler and Andrew Howard and Quoc V. Le},\n      year={2019},\n      eprint={1807.11626},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/mobilenet-v2/","title":"MobileNet v2","text":"<p>MobileNetV2 is a convolutional neural network architecture that seeks to perform well on mobile devices. It is based on an inverted residual structure where the residual connections are between the bottleneck layers.  The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. As a whole, the architecture of MobileNetV2 contains the initial fully convolution layer with 32 filters, followed by 19 residual bottleneck layers.</p>"},{"location":"models/mobilenet-v2/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('mobilenetv2_100', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>mobilenetv2_100</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/mobilenet-v2/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('mobilenetv2_100', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/mobilenet-v2/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/mobilenet-v2/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-1801-04381,\n  author    = {Mark Sandler and\n               Andrew G. Howard and\n               Menglong Zhu and\n               Andrey Zhmoginov and\n               Liang{-}Chieh Chen},\n  title     = {Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification,\n               Detection and Segmentation},\n  journal   = {CoRR},\n  volume    = {abs/1801.04381},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1801.04381},\n  archivePrefix = {arXiv},\n  eprint    = {1801.04381},\n  timestamp = {Tue, 12 Jan 2021 15:30:06 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1801-04381.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/mobilenet-v3/","title":"MobileNet v3","text":"<p>MobileNetV3 is a convolutional neural network that is designed for mobile phone CPUs. The network design includes the use of a hard swish activation and squeeze-and-excitation modules in the MBConv blocks.</p>"},{"location":"models/mobilenet-v3/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('mobilenetv3_large_100', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>mobilenetv3_large_100</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/mobilenet-v3/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('mobilenetv3_large_100', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/mobilenet-v3/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/mobilenet-v3/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-1905-02244,\n  author    = {Andrew Howard and\n               Mark Sandler and\n               Grace Chu and\n               Liang{-}Chieh Chen and\n               Bo Chen and\n               Mingxing Tan and\n               Weijun Wang and\n               Yukun Zhu and\n               Ruoming Pang and\n               Vijay Vasudevan and\n               Quoc V. Le and\n               Hartwig Adam},\n  title     = {Searching for MobileNetV3},\n  journal   = {CoRR},\n  volume    = {abs/1905.02244},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1905.02244},\n  archivePrefix = {arXiv},\n  eprint    = {1905.02244},\n  timestamp = {Tue, 12 Jan 2021 15:30:06 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-02244.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/nasnet/","title":"NASNet","text":"<p>NASNet is a type of convolutional neural network discovered through neural architecture search. The building blocks consist of normal and reduction cells.</p>"},{"location":"models/nasnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('nasnetalarge', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>nasnetalarge</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/nasnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('nasnetalarge', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/nasnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/nasnet/#citation","title":"Citation","text":"<pre><code>@misc{zoph2018learning,\n      title={Learning Transferable Architectures for Scalable Image Recognition}, \n      author={Barret Zoph and Vijay Vasudevan and Jonathon Shlens and Quoc V. Le},\n      year={2018},\n      eprint={1707.07012},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/noisy-student/","title":"Noisy Student (EfficientNet)","text":"<p>Noisy Student Training is a semi-supervised learning approach. It extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. It has three main steps: </p> <ol> <li>train a teacher model on labeled images</li> <li>use the teacher to generate pseudo labels on unlabeled images</li> <li>train a student model on the combination of labeled images and pseudo labeled images. </li> </ol> <p>The algorithm is iterated a few times by treating the student as a teacher to relabel the unlabeled data and training a new student.</p> <p>Noisy Student Training seeks to improve on self-training and distillation in two ways. First, it makes the student larger than, or at least equal to, the teacher so the student can better learn from a larger dataset. Second, it adds noise to the student so the noised student is forced to learn harder from the pseudo labels. To noise the student, it uses input noise such as RandAugment data augmentation, and model noise such as dropout and stochastic depth during training.</p>"},{"location":"models/noisy-student/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('tf_efficientnet_b0_ns', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>tf_efficientnet_b0_ns</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/noisy-student/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('tf_efficientnet_b0_ns', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/noisy-student/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/noisy-student/#citation","title":"Citation","text":"<pre><code>@misc{xie2020selftraining,\n      title={Self-training with Noisy Student improves ImageNet classification}, \n      author={Qizhe Xie and Minh-Thang Luong and Eduard Hovy and Quoc V. Le},\n      year={2020},\n      eprint={1911.04252},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n</code></pre>"},{"location":"models/pnasnet/","title":"PNASNet","text":"<p>Progressive Neural Architecture Search, or PNAS, is a method for learning the structure of convolutional neural networks (CNNs). It uses a sequential model-based optimization (SMBO) strategy, where we search the space of cell structures, starting with simple (shallow) models and progressing to complex ones, pruning out unpromising structures as we go. </p>"},{"location":"models/pnasnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('pnasnet5large', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>pnasnet5large</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/pnasnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('pnasnet5large', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/pnasnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/pnasnet/#citation","title":"Citation","text":"<pre><code>@misc{liu2018progressive,\n      title={Progressive Neural Architecture Search}, \n      author={Chenxi Liu and Barret Zoph and Maxim Neumann and Jonathon Shlens and Wei Hua and Li-Jia Li and Li Fei-Fei and Alan Yuille and Jonathan Huang and Kevin Murphy},\n      year={2018},\n      eprint={1712.00559},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/regnetx/","title":"RegNetX","text":"<p>RegNetX is a convolutional network design space with simple, regular models with parameters: depth d, initial width w\\_{0} &gt; 0w\\_{0} &gt; 0, and slope w\\_{a} &gt; 0w\\_{a} &gt; 0, and generates a different block width u\\_{j}u\\_{j} for each block j &lt; dj &lt; d. The key restriction for the RegNet types of model is that there is a linear parameterisation of block widths (the design space only contains models with this linear structure):</p>   u\\_{j} = w\\_{0} + w\\_{a}\\cdot{j}   u\\_{j} = w\\_{0} + w\\_{a}\\cdot{j}   <p>For RegNetX we have additional restrictions: we set b = 1b = 1 (the bottleneck ratio), 12 \\leq d \\leq 2812 \\leq d \\leq 28, and w\\_{m} \\geq 2w\\_{m} \\geq 2 (the width multiplier).</p>"},{"location":"models/regnetx/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('regnetx_002', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>regnetx_002</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/regnetx/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('regnetx_002', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/regnetx/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/regnetx/#citation","title":"Citation","text":"<pre><code>@misc{radosavovic2020designing,\n      title={Designing Network Design Spaces}, \n      author={Ilija Radosavovic and Raj Prateek Kosaraju and Ross Girshick and Kaiming He and Piotr Doll\u00e1r},\n      year={2020},\n      eprint={2003.13678},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/regnety/","title":"RegNetY","text":"<p>RegNetY is a convolutional network design space with simple, regular models with parameters: depth d, initial width w\\_{0} &gt; 0w\\_{0} &gt; 0, and slope w\\_{a} &gt; 0w\\_{a} &gt; 0, and generates a different block width u\\_{j}u\\_{j} for each block j &lt; dj &lt; d. The key restriction for the RegNet types of model is that there is a linear parameterisation of block widths (the design space only contains models with this linear structure):</p>   u\\_{j} = w\\_{0} + w\\_{a}\\cdot{j}   u\\_{j} = w\\_{0} + w\\_{a}\\cdot{j}   <p>For RegNetX authors have additional restrictions: we set b = 1b = 1 (the bottleneck ratio), 12 \\leq d \\leq 2812 \\leq d \\leq 28, and w\\_{m} \\geq 2w\\_{m} \\geq 2 (the width multiplier).</p> <p>For RegNetY authors make one change, which is to include Squeeze-and-Excitation blocks.</p>"},{"location":"models/regnety/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('regnety_002', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>regnety_002</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/regnety/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('regnety_002', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/regnety/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/regnety/#citation","title":"Citation","text":"<pre><code>@misc{radosavovic2020designing,\n      title={Designing Network Design Spaces}, \n      author={Ilija Radosavovic and Raj Prateek Kosaraju and Ross Girshick and Kaiming He and Piotr Doll\u00e1r},\n      year={2020},\n      eprint={2003.13678},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/res2net/","title":"Res2Net","text":"<p>Res2Net is an image model that employs a variation on bottleneck residual blocks, Res2Net Blocks. The motivation is to be able to represent features at multiple scales. This is achieved through a novel building block for CNNs that constructs hierarchical residual-like connections within one single residual block. This represents multi-scale features at a granular level and increases the range of receptive fields for each network layer.</p>"},{"location":"models/res2net/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('res2net101_26w_4s', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>res2net101_26w_4s</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/res2net/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('res2net101_26w_4s', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/res2net/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/res2net/#citation","title":"Citation","text":"<pre><code>@article{Gao_2021,\n   title={Res2Net: A New Multi-Scale Backbone Architecture},\n   volume={43},\n   ISSN={1939-3539},\n   url={http://dx.doi.org/10.1109/TPAMI.2019.2938758},\n   DOI={10.1109/tpami.2019.2938758},\n   number={2},\n   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\n   author={Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip},\n   year={2021},\n   month={Feb},\n   pages={652\u2013662}\n}\n</code></pre>"},{"location":"models/res2next/","title":"Res2NeXt","text":"<p>Res2NeXt is an image model that employs a variation on ResNeXt bottleneck residual blocks. The motivation is to be able to represent features at multiple scales. This is achieved through a novel building block for CNNs that constructs hierarchical residual-like connections within one single residual block. This represents multi-scale features at a granular level and increases the range of receptive fields for each network layer.</p>"},{"location":"models/res2next/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('res2next50', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>res2next50</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/res2next/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('res2next50', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/res2next/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/res2next/#citation","title":"Citation","text":"<pre><code>@article{Gao_2021,\n   title={Res2Net: A New Multi-Scale Backbone Architecture},\n   volume={43},\n   ISSN={1939-3539},\n   url={http://dx.doi.org/10.1109/TPAMI.2019.2938758},\n   DOI={10.1109/tpami.2019.2938758},\n   number={2},\n   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\n   author={Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip},\n   year={2021},\n   month={Feb},\n   pages={652\u2013662}\n}\n</code></pre>"},{"location":"models/resnest/","title":"ResNeSt","text":"<p>A ResNeSt is a variant on a ResNet, which instead stacks Split-Attention blocks. The cardinal group representations are then concatenated along the channel dimension: V = \\text{Concat}{V^{1},V^{2},\\cdots{V}^{K}V^{1},V^{2},\\cdots{V}^{K}}. As in standard residual blocks, the final output YY of otheur Split-Attention block is produced using a shortcut connection: Y=V+XY=V+X, if the input and output feature-map share the same shape.  For blocks with a stride, an appropriate transformation \\mathcal{T}\\mathcal{T} is applied to the shortcut connection to align the output shapes:  Y=V+\\mathcal{T}(X)Y=V+\\mathcal{T}(X). For example, \\mathcal{T}\\mathcal{T} can be strided convolution or combined convolution-with-pooling.</p>"},{"location":"models/resnest/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('resnest101e', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>resnest101e</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/resnest/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('resnest101e', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/resnest/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/resnest/#citation","title":"Citation","text":"<pre><code>@misc{zhang2020resnest,\n      title={ResNeSt: Split-Attention Networks}, \n      author={Hang Zhang and Chongruo Wu and Zhongyue Zhang and Yi Zhu and Haibin Lin and Zhi Zhang and Yue Sun and Tong He and Jonas Mueller and R. Manmatha and Mu Li and Alexander Smola},\n      year={2020},\n      eprint={2004.08955},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/resnet-d/","title":"ResNet-D","text":"<p>ResNet-D is a modification on the ResNet architecture that utilises an average pooling tweak for downsampling. The motivation is that in the unmodified ResNet, the 1\u00d71 convolution for the downsampling block ignores \u00be of input feature maps, so this is modified so no information will be ignored</p>"},{"location":"models/resnet-d/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('resnet101d', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>resnet101d</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/resnet-d/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('resnet101d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/resnet-d/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/resnet-d/#citation","title":"Citation","text":"<pre><code>@misc{he2018bag,\n      title={Bag of Tricks for Image Classification with Convolutional Neural Networks}, \n      author={Tong He and Zhi Zhang and Hang Zhang and Zhongyue Zhang and Junyuan Xie and Mu Li},\n      year={2018},\n      eprint={1812.01187},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/resnet/","title":"ResNet","text":"<p>Residual Networks, or ResNets, learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack residual blocks ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. </p>"},{"location":"models/resnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('resnet18', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>resnet18</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/resnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('resnet18', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/resnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/resnet/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/HeZRS15,\n  author    = {Kaiming He and\n               Xiangyu Zhang and\n               Shaoqing Ren and\n               Jian Sun},\n  title     = {Deep Residual Learning for Image Recognition},\n  journal   = {CoRR},\n  volume    = {abs/1512.03385},\n  year      = {2015},\n  url       = {http://arxiv.org/abs/1512.03385},\n  archivePrefix = {arXiv},\n  eprint    = {1512.03385},\n  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/HeZRS15.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/resnext/","title":"ResNeXt","text":"<p>A ResNeXt repeats a building block that aggregates a set of transformations with the same topology. Compared to a ResNet, it exposes a new dimension,  cardinality (the size of the set of transformations) C, as an essential factor in addition to the dimensions of depth and width. </p>"},{"location":"models/resnext/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('resnext101_32x8d', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>resnext101_32x8d</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/resnext/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('resnext101_32x8d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/resnext/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/resnext/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/XieGDTH16,\n  author    = {Saining Xie and\n               Ross B. Girshick and\n               Piotr Doll{\\'{a}}r and\n               Zhuowen Tu and\n               Kaiming He},\n  title     = {Aggregated Residual Transformations for Deep Neural Networks},\n  journal   = {CoRR},\n  volume    = {abs/1611.05431},\n  year      = {2016},\n  url       = {http://arxiv.org/abs/1611.05431},\n  archivePrefix = {arXiv},\n  eprint    = {1611.05431},\n  timestamp = {Mon, 13 Aug 2018 16:45:58 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/XieGDTH16.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/rexnet/","title":"RexNet","text":"<p>Rank Expansion Networks (ReXNets) follow a set of new design principles for designing bottlenecks in image classification models. Authors refine each layer by 1) expanding the input channel size of the convolution layer and 2) replacing the ReLU6s.</p>"},{"location":"models/rexnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('rexnet_100', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>rexnet_100</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/rexnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('rexnet_100', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/rexnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/rexnet/#citation","title":"Citation","text":"<pre><code>@misc{han2020rexnet,\n      title={ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network}, \n      author={Dongyoon Han and Sangdoo Yun and Byeongho Heo and YoungJoon Yoo},\n      year={2020},\n      eprint={2007.00992},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/se-resnet/","title":"SE-ResNet","text":"<p>SE ResNet is a variant of a ResNet that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration.</p>"},{"location":"models/se-resnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('seresnet152d', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>seresnet152d</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/se-resnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('seresnet152d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/se-resnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/se-resnet/#citation","title":"Citation","text":"<pre><code>@misc{hu2019squeezeandexcitation,\n      title={Squeeze-and-Excitation Networks}, \n      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},\n      year={2019},\n      eprint={1709.01507},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/selecsls/","title":"SelecSLS","text":"<p>SelecSLS uses novel selective long and short range skip connections to improve the information flow allowing for a drastically faster network without compromising accuracy.</p>"},{"location":"models/selecsls/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('selecsls42b', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>selecsls42b</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/selecsls/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('selecsls42b', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/selecsls/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/selecsls/#citation","title":"Citation","text":"<pre><code>@article{Mehta_2020,\n   title={XNect},\n   volume={39},\n   ISSN={1557-7368},\n   url={http://dx.doi.org/10.1145/3386569.3392410},\n   DOI={10.1145/3386569.3392410},\n   number={4},\n   journal={ACM Transactions on Graphics},\n   publisher={Association for Computing Machinery (ACM)},\n   author={Mehta, Dushyant and Sotnychenko, Oleksandr and Mueller, Franziska and Xu, Weipeng and Elgharib, Mohamed and Fua, Pascal and Seidel, Hans-Peter and Rhodin, Helge and Pons-Moll, Gerard and Theobalt, Christian},\n   year={2020},\n   month={Jul}\n}\n</code></pre>"},{"location":"models/seresnext/","title":"SE-ResNeXt","text":"<p>SE ResNeXt is a variant of a ResNext that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration.</p>"},{"location":"models/seresnext/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('seresnext26d_32x4d', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>seresnext26d_32x4d</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/seresnext/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('seresnext26d_32x4d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/seresnext/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/seresnext/#citation","title":"Citation","text":"<pre><code>@misc{hu2019squeezeandexcitation,\n      title={Squeeze-and-Excitation Networks}, \n      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},\n      year={2019},\n      eprint={1709.01507},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/skresnet/","title":"SK-ResNet","text":"<p>SK ResNet is a variant of a ResNet that employs a Selective Kernel unit. In general, all the large kernel convolutions in the original bottleneck blocks in ResNet are replaced by the proposed SK convolutions, enabling the network to choose appropriate receptive field sizes in an adaptive manner.</p>"},{"location":"models/skresnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('skresnet18', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>skresnet18</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/skresnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('skresnet18', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/skresnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/skresnet/#citation","title":"Citation","text":"<pre><code>@misc{li2019selective,\n      title={Selective Kernel Networks}, \n      author={Xiang Li and Wenhai Wang and Xiaolin Hu and Jian Yang},\n      year={2019},\n      eprint={1903.06586},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/skresnext/","title":"SK-ResNeXt","text":"<p>SK ResNeXt is a variant of a ResNeXt that employs a Selective Kernel unit. In general, all the large kernel convolutions in the original bottleneck blocks in ResNext are replaced by the proposed SK convolutions, enabling the network to choose appropriate receptive field sizes in an adaptive manner.</p>"},{"location":"models/skresnext/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('skresnext50_32x4d', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>skresnext50_32x4d</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/skresnext/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('skresnext50_32x4d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/skresnext/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/skresnext/#citation","title":"Citation","text":"<pre><code>@misc{li2019selective,\n      title={Selective Kernel Networks}, \n      author={Xiang Li and Wenhai Wang and Xiaolin Hu and Jian Yang},\n      year={2019},\n      eprint={1903.06586},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/spnasnet/","title":"SPNASNet","text":"<p>Single-Path NAS is a novel differentiable NAS method for designing hardware-efficient ConvNets in less than 4 hours.</p>"},{"location":"models/spnasnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('spnasnet_100', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>spnasnet_100</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/spnasnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('spnasnet_100', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/spnasnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/spnasnet/#citation","title":"Citation","text":"<pre><code>@misc{stamoulis2019singlepath,\n      title={Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours}, \n      author={Dimitrios Stamoulis and Ruizhou Ding and Di Wang and Dimitrios Lymberopoulos and Bodhi Priyantha and Jie Liu and Diana Marculescu},\n      year={2019},\n      eprint={1904.02877},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n</code></pre>"},{"location":"models/ssl-resnet/","title":"SSL ResNet","text":"<p>Residual Networks, or ResNets, learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack residual blocks ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. </p> <p>The model in this collection utilises semi-supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. </p> <p>Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.</p>"},{"location":"models/ssl-resnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('ssl_resnet18', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>ssl_resnet18</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/ssl-resnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('ssl_resnet18', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/ssl-resnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/ssl-resnet/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-1905-00546,\n  author    = {I. Zeki Yalniz and\n               Herv{\\'{e}} J{\\'{e}}gou and\n               Kan Chen and\n               Manohar Paluri and\n               Dhruv Mahajan},\n  title     = {Billion-scale semi-supervised learning for image classification},\n  journal   = {CoRR},\n  volume    = {abs/1905.00546},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1905.00546},\n  archivePrefix = {arXiv},\n  eprint    = {1905.00546},\n  timestamp = {Mon, 28 Sep 2020 08:19:37 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-00546.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/ssl-resnext/","title":"SSL ResNeXT","text":"<p>A ResNeXt repeats a building block that aggregates a set of transformations with the same topology. Compared to a ResNet, it exposes a new dimension,  cardinality (the size of the set of transformations) C, as an essential factor in addition to the dimensions of depth and width. </p> <p>The model in this collection utilises semi-supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. </p> <p>Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.</p>"},{"location":"models/ssl-resnext/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('ssl_resnext101_32x16d', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>ssl_resnext101_32x16d</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/ssl-resnext/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('ssl_resnext101_32x16d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/ssl-resnext/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/ssl-resnext/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-1905-00546,\n  author    = {I. Zeki Yalniz and\n               Herv{\\'{e}} J{\\'{e}}gou and\n               Kan Chen and\n               Manohar Paluri and\n               Dhruv Mahajan},\n  title     = {Billion-scale semi-supervised learning for image classification},\n  journal   = {CoRR},\n  volume    = {abs/1905.00546},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1905.00546},\n  archivePrefix = {arXiv},\n  eprint    = {1905.00546},\n  timestamp = {Mon, 28 Sep 2020 08:19:37 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-00546.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/swsl-resnet/","title":"SWSL ResNet","text":"<p>Residual Networks, or ResNets, learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack residual blocks ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. </p> <p>The models in this collection utilise semi-weakly supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. </p> <p>Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.</p>"},{"location":"models/swsl-resnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('swsl_resnet18', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>swsl_resnet18</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/swsl-resnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('swsl_resnet18', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/swsl-resnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/swsl-resnet/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-1905-00546,\n  author    = {I. Zeki Yalniz and\n               Herv{\\'{e}} J{\\'{e}}gou and\n               Kan Chen and\n               Manohar Paluri and\n               Dhruv Mahajan},\n  title     = {Billion-scale semi-supervised learning for image classification},\n  journal   = {CoRR},\n  volume    = {abs/1905.00546},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1905.00546},\n  archivePrefix = {arXiv},\n  eprint    = {1905.00546},\n  timestamp = {Mon, 28 Sep 2020 08:19:37 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-00546.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/swsl-resnext/","title":"SWSL ResNeXt","text":"<p>A ResNeXt repeats a building block that aggregates a set of transformations with the same topology. Compared to a ResNet, it exposes a new dimension,  cardinality (the size of the set of transformations) C, as an essential factor in addition to the dimensions of depth and width. </p> <p>The models in this collection utilise semi-weakly supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. </p> <p>Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.</p>"},{"location":"models/swsl-resnext/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('swsl_resnext101_32x16d', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>swsl_resnext101_32x16d</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/swsl-resnext/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('swsl_resnext101_32x16d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/swsl-resnext/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/swsl-resnext/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-1905-00546,\n  author    = {I. Zeki Yalniz and\n               Herv{\\'{e}} J{\\'{e}}gou and\n               Kan Chen and\n               Manohar Paluri and\n               Dhruv Mahajan},\n  title     = {Billion-scale semi-supervised learning for image classification},\n  journal   = {CoRR},\n  volume    = {abs/1905.00546},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1905.00546},\n  archivePrefix = {arXiv},\n  eprint    = {1905.00546},\n  timestamp = {Mon, 28 Sep 2020 08:19:37 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-00546.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/tf-efficientnet-condconv/","title":"(Tensorflow) EfficientNet CondConv","text":"<p>EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient. Unlike conventional practice that arbitrary scales  these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use 2^N times more computational resources, then we can simply increase the network depth by \\alpha ^ N\\alpha ^ N,  width by \\beta ^ N\\beta ^ N, and image size by \\gamma ^ N\\gamma ^ N, where \\alpha, \\beta, \\gamma\\alpha, \\beta, \\gamma are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\phi\\phi to uniformly scales network width, depth, and resolution in a  principled way.</p> <p>The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image.</p> <p>The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2, in addition to squeeze-and-excitation blocks.</p> <p>This collection of models amends EfficientNet by adding CondConv convolutions.</p> <p>The weights from this model were ported from Tensorflow/TPU.</p>"},{"location":"models/tf-efficientnet-condconv/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('tf_efficientnet_cc_b0_4e', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>tf_efficientnet_cc_b0_4e</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/tf-efficientnet-condconv/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('tf_efficientnet_cc_b0_4e', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/tf-efficientnet-condconv/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/tf-efficientnet-condconv/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-1904-04971,\n  author    = {Brandon Yang and\n               Gabriel Bender and\n               Quoc V. Le and\n               Jiquan Ngiam},\n  title     = {Soft Conditional Computation},\n  journal   = {CoRR},\n  volume    = {abs/1904.04971},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1904.04971},\n  archivePrefix = {arXiv},\n  eprint    = {1904.04971},\n  timestamp = {Thu, 25 Apr 2019 13:55:01 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-04971.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/tf-efficientnet-lite/","title":"(Tensorflow) EfficientNet Lite","text":"<p>EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient. Unlike conventional practice that arbitrary scales  these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use 2^N times more computational resources, then we can simply increase the network depth by \\alpha ^ N\\alpha ^ N,  width by \\beta ^ N\\beta ^ N, and image size by \\gamma ^ N\\gamma ^ N, where \\alpha, \\beta, \\gamma\\alpha, \\beta, \\gamma are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\phi\\phi to uniformly scales network width, depth, and resolution in a  principled way.</p> <p>The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image.</p> <p>The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2.</p> <p>EfficientNet-Lite makes EfficientNet more suitable for mobile devices by introducing ReLU6 activation functions and removing squeeze-and-excitation blocks.</p> <p>The weights from this model were ported from Tensorflow/TPU.</p>"},{"location":"models/tf-efficientnet-lite/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('tf_efficientnet_lite0', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>tf_efficientnet_lite0</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/tf-efficientnet-lite/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('tf_efficientnet_lite0', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/tf-efficientnet-lite/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/tf-efficientnet-lite/#citation","title":"Citation","text":"<pre><code>@misc{tan2020efficientnet,\n      title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}, \n      author={Mingxing Tan and Quoc V. Le},\n      year={2020},\n      eprint={1905.11946},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n</code></pre>"},{"location":"models/tf-efficientnet/","title":"(Tensorflow) EfficientNet","text":"<p>EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient. Unlike conventional practice that arbitrary scales  these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use 2^N times more computational resources, then we can simply increase the network depth by \\alpha ^ N\\alpha ^ N,  width by \\beta ^ N\\beta ^ N, and image size by \\gamma ^ N\\gamma ^ N, where \\alpha, \\beta, \\gamma\\alpha, \\beta, \\gamma are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\phi\\phi to uniformly scales network width, depth, and resolution in a  principled way.</p> <p>The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image.</p> <p>The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2, in addition to squeeze-and-excitation blocks.</p> <p>The weights from this model were ported from Tensorflow/TPU.</p>"},{"location":"models/tf-efficientnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('tf_efficientnet_b0', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>tf_efficientnet_b0</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/tf-efficientnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('tf_efficientnet_b0', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/tf-efficientnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/tf-efficientnet/#citation","title":"Citation","text":"<pre><code>@misc{tan2020efficientnet,\n      title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}, \n      author={Mingxing Tan and Quoc V. Le},\n      year={2020},\n      eprint={1905.11946},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n</code></pre>"},{"location":"models/tf-inception-v3/","title":"(Tensorflow) Inception v3","text":"<p>Inception v3 is a convolutional neural network architecture from the Inception family that makes several improvements including using Label Smoothing, Factorized 7 x 7 convolutions, and the use of an auxiliary classifer to propagate label information lower down the network (along with the use of batch normalization for layers in the sidehead). The key building block is an Inception Module.</p> <p>The weights from this model were ported from Tensorflow/Models.</p>"},{"location":"models/tf-inception-v3/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('tf_inception_v3', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>tf_inception_v3</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/tf-inception-v3/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('tf_inception_v3', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/tf-inception-v3/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/tf-inception-v3/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/SzegedyVISW15,\n  author    = {Christian Szegedy and\n               Vincent Vanhoucke and\n               Sergey Ioffe and\n               Jonathon Shlens and\n               Zbigniew Wojna},\n  title     = {Rethinking the Inception Architecture for Computer Vision},\n  journal   = {CoRR},\n  volume    = {abs/1512.00567},\n  year      = {2015},\n  url       = {http://arxiv.org/abs/1512.00567},\n  archivePrefix = {arXiv},\n  eprint    = {1512.00567},\n  timestamp = {Mon, 13 Aug 2018 16:49:07 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/SzegedyVISW15.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/tf-mixnet/","title":"(Tensorflow) MixNet","text":"<p>MixNet is a type of convolutional neural network discovered via AutoML that utilises MixConvs instead of regular depthwise convolutions.</p> <p>The weights from this model were ported from Tensorflow/TPU.</p>"},{"location":"models/tf-mixnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('tf_mixnet_l', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>tf_mixnet_l</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/tf-mixnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('tf_mixnet_l', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/tf-mixnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/tf-mixnet/#citation","title":"Citation","text":"<pre><code>@misc{tan2019mixconv,\n      title={MixConv: Mixed Depthwise Convolutional Kernels}, \n      author={Mingxing Tan and Quoc V. Le},\n      year={2019},\n      eprint={1907.09595},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/tf-mobilenet-v3/","title":"(Tensorflow) MobileNet v3","text":"<p>MobileNetV3 is a convolutional neural network that is designed for mobile phone CPUs. The network design includes the use of a hard swish activation and squeeze-and-excitation modules in the MBConv blocks.</p> <p>The weights from this model were ported from Tensorflow/Models.</p>"},{"location":"models/tf-mobilenet-v3/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('tf_mobilenetv3_large_075', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>tf_mobilenetv3_large_075</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/tf-mobilenet-v3/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('tf_mobilenetv3_large_075', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/tf-mobilenet-v3/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/tf-mobilenet-v3/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-1905-02244,\n  author    = {Andrew Howard and\n               Mark Sandler and\n               Grace Chu and\n               Liang{-}Chieh Chen and\n               Bo Chen and\n               Mingxing Tan and\n               Weijun Wang and\n               Yukun Zhu and\n               Ruoming Pang and\n               Vijay Vasudevan and\n               Quoc V. Le and\n               Hartwig Adam},\n  title     = {Searching for MobileNetV3},\n  journal   = {CoRR},\n  volume    = {abs/1905.02244},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1905.02244},\n  archivePrefix = {arXiv},\n  eprint    = {1905.02244},\n  timestamp = {Tue, 12 Jan 2021 15:30:06 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-02244.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/tresnet/","title":"TResNet","text":"<p>A TResNet is a variant on a ResNet that aim to boost accuracy while maintaining GPU training and inference efficiency.  They contain several design tricks including a SpaceToDepth stem, Anti-Alias downsampling, In-Place Activated BatchNorm, Blocks selection and squeeze-and-excitation layers.</p>"},{"location":"models/tresnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('tresnet_l', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>tresnet_l</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/tresnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('tresnet_l', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/tresnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/tresnet/#citation","title":"Citation","text":"<pre><code>@misc{ridnik2020tresnet,\n      title={TResNet: High Performance GPU-Dedicated Architecture}, \n      author={Tal Ridnik and Hussam Lawen and Asaf Noy and Emanuel Ben Baruch and Gilad Sharir and Itamar Friedman},\n      year={2020},\n      eprint={2003.13630},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/vision-transformer/","title":"Vision Transformer (ViT)","text":"<p>The Vision Transformer is a model for image classification that employs a Transformer-like architecture over patches of the image. This includes the use of Multi-Head Attention, Scaled Dot-Product Attention and other architectural features seen in the Transformer architecture traditionally used for NLP.</p>"},{"location":"models/vision-transformer/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('vit_base_patch16_224', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>vit_base_patch16_224</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/vision-transformer/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/vision-transformer/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/vision-transformer/#citation","title":"Citation","text":"<pre><code>@misc{dosovitskiy2020image,\n      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, \n      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},\n      year={2020},\n      eprint={2010.11929},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"models/wide-resnet/","title":"Wide ResNet","text":"<p>Wide Residual Networks are a variant on ResNets where we decrease depth and increase the width of residual networks. This is achieved through the use of wide residual blocks.</p>"},{"location":"models/wide-resnet/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('wide_resnet101_2', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>wide_resnet101_2</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/wide-resnet/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('wide_resnet101_2', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/wide-resnet/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/wide-resnet/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/ZagoruykoK16,\n  author    = {Sergey Zagoruyko and\n               Nikos Komodakis},\n  title     = {Wide Residual Networks},\n  journal   = {CoRR},\n  volume    = {abs/1605.07146},\n  year      = {2016},\n  url       = {http://arxiv.org/abs/1605.07146},\n  archivePrefix = {arXiv},\n  eprint    = {1605.07146},\n  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/ZagoruykoK16.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"models/xception/","title":"Xception","text":"<p>Xception is a convolutional neural network architecture that relies solely on depthwise separable convolution layers.</p> <p>The weights from this model were ported from Tensorflow/Models.</p>"},{"location":"models/xception/#how-do-i-use-this-model-on-an-image","title":"How do I use this model on an image?","text":"<p>To load a pretrained model:</p> <pre><code>import timm\nmodel = timm.create_model('xception', pretrained=True)\nmodel.eval()\n</code></pre> <p>To load and preprocess the image: <pre><code>import urllib\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n</code></pre></p> <p>To get the model predictions: <pre><code>import torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n</code></pre></p> <p>To get the top-5 predictions class names: <pre><code># Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n</code></pre></p> <p>Replace the model name with the variant you want to use, e.g. <code>xception</code>. You can find the IDs in the model summaries at the top of this page.</p> <p>To extract image features with this model, follow the timm feature extraction examples, just change the name of the model you want to use.</p>"},{"location":"models/xception/#how-do-i-finetune-this-model","title":"How do I finetune this model?","text":"<p>You can finetune any of the pre-trained models just by changing the classifier (the last layer). <pre><code>model = timm.create_model('xception', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n</code></pre> To finetune on your own dataset, you have to write a training loop or adapt timm's training script to use your dataset.</p>"},{"location":"models/xception/#how-do-i-train-this-model","title":"How do I train this model?","text":"<p>You can follow the timm recipe scripts for training a new model afresh.</p>"},{"location":"models/xception/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/ZagoruykoK16,\n@misc{chollet2017xception,\n      title={Xception: Deep Learning with Depthwise Separable Convolutions}, \n      author={Fran\u00e7ois Chollet},\n      year={2017},\n      eprint={1610.02357},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"}]}